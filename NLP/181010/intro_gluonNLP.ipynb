{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# GluonNLP  소개 \n",
    "\n",
    "GluonNLP 기반으로 Vocab객체와 임베딩을 손쉽게 다루는 방법을 소개한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/81/94843b759ba2b71a9002b81cddc7f06b3c557f1eb748e8d9d160246f433c/mxnet-1.3.1b20181010-cp36-cp36m-macosx_10_11_x86_64.whl (12.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 12.7MB 126kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: graphviz<0.9.0,>=0.8.1 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from mxnet) (0.8.4)\n",
      "Requirement already satisfied, skipping upgrade: numpy<1.15.0,>=1.8.2 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from mxnet) (1.14.5)\n",
      "Requirement already satisfied, skipping upgrade: requests<2.19.0,>=2.18.4 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from mxnet) (2.18.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from requests<2.19.0,>=2.18.4->mxnet) (2018.4.16)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from requests<2.19.0,>=2.18.4->mxnet) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from requests<2.19.0,>=2.18.4->mxnet) (2.6)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from requests<2.19.0,>=2.18.4->mxnet) (1.22)\n",
      "Installing collected packages: mxnet\n",
      "Successfully installed mxnet-1.3.1b20181010\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --pre --upgrade mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gluonnlp\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/f0/abb19159402ce767c953a8fdf1d7570bca85dcbfed87f575beb96c3fb803/gluonnlp-0.4.0.tar.gz (160kB)\n",
      "\u001b[K    100% |████████████████████████████████| 163kB 124kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from gluonnlp) (1.14.5)\n",
      "Building wheels for collected packages: gluonnlp\n",
      "  Running setup.py bdist_wheel for gluonnlp ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/sonwoncheol/Library/Caches/pip/wheels/44/af/bc/20974a8cf42db70b261dfe1eb8e8bf8e52a76c8b192af4d4cd\n",
      "Successfully built gluonnlp\n",
      "Installing collected packages: gluonnlp\n",
      "Successfully installed gluonnlp-0.4.0\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gluonnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tagger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-58f98be73f20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMecab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmecab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMecab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/konlpy/tag/_mecab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dicpath)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdicpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/usr/local/lib/mecab/dic/mecab-ko-dic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-d %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdicpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s/data/tagset/mecab.json'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstallpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tagger' is not defined"
     ]
    }
   ],
   "source": [
    "from mxnet import gluon\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from mxnet import nd\n",
    "import gluonnlp as nlp\n",
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "import re\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab 객체 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "사과 배 과일 다이어트 : 다이어트에 성공하기 위해서는 식습관과 라이프 스타일을 대대적으로 바꿔야 한다. 여기에 간단한 상상력을 더하면 그 효과를 훨씬 끌어올릴 수 있다. \n",
    "다이어트는 의지와 관리가 핵심이다. 엄마 아빠 \n",
    "아들 딸 구분하지 말고 하나만 낳아서 잘 기르자.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력된 문장에 대해서 처리 단위가 되는 토큰으로 분리하고 적절한 전처리를 수행한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mecab_tokenize(source_str):\n",
    "    return source_str.split()\n",
    "    #return mecab.morphs(source_str.strip())\n",
    "\n",
    "\n",
    "\n",
    "counter = nlp.data.count_tokens(mecab_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({':': 1,\n",
       "         '간단한': 1,\n",
       "         '과일': 1,\n",
       "         '관리가': 1,\n",
       "         '구분하지': 1,\n",
       "         '그': 1,\n",
       "         '기르자.': 1,\n",
       "         '끌어올릴': 1,\n",
       "         '낳아서': 1,\n",
       "         '다이어트': 1,\n",
       "         '다이어트는': 1,\n",
       "         '다이어트에': 1,\n",
       "         '대대적으로': 1,\n",
       "         '더하면': 1,\n",
       "         '딸': 1,\n",
       "         '라이프': 1,\n",
       "         '말고': 1,\n",
       "         '바꿔야': 1,\n",
       "         '배': 1,\n",
       "         '사과': 1,\n",
       "         '상상력을': 1,\n",
       "         '성공하기': 1,\n",
       "         '수': 1,\n",
       "         '스타일을': 1,\n",
       "         '식습관과': 1,\n",
       "         '아들': 1,\n",
       "         '아빠': 1,\n",
       "         '엄마': 1,\n",
       "         '여기에': 1,\n",
       "         '위해서는': 1,\n",
       "         '의지와': 1,\n",
       "         '있다.': 1,\n",
       "         '잘': 1,\n",
       "         '하나만': 1,\n",
       "         '한다.': 1,\n",
       "         '핵심이다.': 1,\n",
       "         '효과를': 1,\n",
       "         '훨씬': 1})"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "counter라는 일종의 사전 객체 `{단어, 빈도}`를 기반으로 Vocab객체를 생성한다. \n",
    "Vocab 객체는 기 정의된 토큰 `<unk>, <pad>, <bos>, <eos>`을 보유하고 있으며, 사전에 포함될 최소 빈도수로 의미없는 토큰을 제거할 수 있는 인터페이스를 제공한다(`max_size`, `min_freq`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = nlp.Vocab(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`token_to_idx`과, `idx_to_token`와 같이 학습셋을 만들때 반드시 구축해야 되는 객체를 만들어 주며, `vocab`객체만 피클로 저장해 모델과 함께 적재할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<bos>',\n",
       " '<eos>',\n",
       " ['<unk>', '<pad>', '<bos>', '<eos>', ':', '간단한', '과일', '관리가', '그', '끌어올릴'],\n",
       " 0,\n",
       " [0, 0])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.bos_token, vocab.eos_token, vocab.idx_to_token[:10], vocab.token_to_idx['간단'], vocab[['간단', '을']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vocab에 임베딩 연결 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "GluonNLP가 없던 시점에는 아래와 같이 복잡한 방법으로 임베딩을 사용했다. \n",
    "\n",
    "1. 문장 토큰화 및 전처리 \n",
    "2. 토큰에 대한 딕셔너리 구성 \n",
    "3. index to token, token to index 자료구조 생성 \n",
    "4. 임베딩 다운로드 및 보유하고 있는 토큰에 해당되는 임베딩 엔트리로 임베딩 전처리 \n",
    "\n",
    "\n",
    "Vocab과 Gluon에서 제공하고 있는 임베딩을 이용해 손쉽게 기 학습된 임베딩을 사용해보자! \n",
    "\n",
    "여기서 임베딩은 Gluon에서 제공하는 다양한 기 학습된 임베딩 중 FastText 한글 임베딩을 연결해보겠다. \n",
    "\n",
    "FastText는 Facebook에서 개발한 임베딩 방법론이며, OOV(out of vocabulary)이슈를 효과적으로 해결한 방법론이다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the opposite direction, we can grab an idex given a token using `vocab.token_to_idx`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Attaching word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "아래와 같이 fasttext와 한글 위키 코퍼스로 학습된 임베딩을 가져온다. \n",
    "\n",
    "`fasttext` and the named argument `source='wiki.ko'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nlp.embedding.list_sources()`로 모든 기 학습된 임베딩 리스트 출력 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fasttext_simple = nlp.embedding.create('fasttext', source='wiki.ko')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만일 Gensim으로 따로 학습한 임베딩을 사용하고자 한다면 아래와 같이 vocab과 결합할 수 있다. \n",
    "\n",
    "`mdl`은 Gensim으로 학습된 모델이다. \n",
    "```\n",
    "gensim_embedding = nlp.embedding.TokenEmbedding(allow_extend=True)\n",
    "gensim_embedding[mdl.wv.index2word] = nd.array(mdl[mdl.wv.index2word])\n",
    "vocab.set_embedding(gensim_embedding)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To attach the newly loaded word embeddings `fasttext_simple` to indexed words in `vocab`, we simply call vocab's `set_embedding` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "vocab.set_embedding(fasttext_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결합된 임베딩 shape과 인덱스 크기 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31, 300), 31)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.embedding.idx_to_vec.shape, len(vocab.idx_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       "<NDArray 300 @cpu(0)>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.embedding['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [-8.0940e-02  1.3513e-01 -5.4361e-01 -2.9133e-01 -6.2355e-01  1.6854e-01\n",
       "   1.6384e-01 -6.8300e-02 -1.6073e-01 -2.2654e-01  1.9696e-01  3.4873e-01\n",
       "  -2.7258e-02  6.7921e-01 -7.2137e-01  4.3859e-01 -1.0719e-02 -2.3379e-02\n",
       "   2.7814e-02 -3.3025e-01 -3.3353e-01 -2.9284e-01 -9.1742e-02 -4.9350e-01\n",
       "  -5.4934e-02  7.7779e-02  3.1965e-01 -1.3066e-01  2.3457e-01  7.1430e-01\n",
       "  -1.7188e-01 -9.6297e-03  2.0930e-02 -4.2738e-01 -1.0519e-01  3.6542e-01\n",
       "   1.4909e-02  3.8155e-01  2.9169e-01 -6.7597e-02  5.3827e-02 -5.5055e-01\n",
       "  -5.6948e-01  8.3939e-02  5.2487e-01 -4.1104e-01 -7.9545e-02  1.7147e-01\n",
       "   6.7951e-01  7.3355e-02  1.2852e-01  2.7254e-01 -4.5853e-01  1.1028e-01\n",
       "   3.1504e-01 -3.2422e-01 -1.5859e-01  1.2325e-01 -2.3795e-01 -3.2770e-01\n",
       "   6.7784e-01  5.5509e-01  3.3643e-02 -4.4354e-01 -1.4814e-01 -2.6108e-01\n",
       "  -1.4608e-02 -6.2265e-01 -1.3015e-01 -1.2823e-02  2.0762e-01 -4.4773e-01\n",
       "   1.2261e-01 -5.7290e-03 -1.9332e-01  5.9167e-02  5.5691e-01  1.6890e-01\n",
       "   2.2230e-01  1.9412e-01  3.1604e-01  5.3051e-02 -4.9924e-03  2.8252e-01\n",
       "   1.3501e-01 -3.8774e-01 -1.8015e-01 -2.9684e-01  5.1144e-02  2.0836e-01\n",
       "  -5.3735e-01  9.5305e-02  2.9616e-02 -4.4602e-01 -4.7137e-02  3.6705e-01\n",
       "  -1.5986e-01  6.6282e-01  1.9584e-01 -1.9020e-01 -6.4962e-01 -4.5217e-01\n",
       "  -4.6095e-01 -6.5632e-01  2.0825e-01  2.3748e-01 -1.8556e-02  1.5411e-01\n",
       "  -4.1197e-01  3.0822e-01  1.2672e-01 -1.7503e-01 -3.5728e-01 -9.9337e-02\n",
       "   1.0821e-01 -4.9296e-01 -3.3645e-01  2.7874e-01 -1.8791e-01  1.3570e-01\n",
       "   3.8681e-01  1.8881e-02 -3.0556e-01  1.1950e-01 -4.9178e-01 -2.3220e-01\n",
       "   5.6412e-01 -4.3060e-01 -1.9533e-01  1.2739e-01 -2.9600e-01 -7.2288e-02\n",
       "  -1.5166e-01 -1.0890e-01  2.0257e-01 -4.9943e-02 -2.5853e-01  4.0685e-01\n",
       "   2.3637e-01 -3.3620e-01 -2.4557e-01  6.9762e-02 -5.0709e-02 -1.8112e-01\n",
       "   1.3899e-03  7.1332e-02 -6.8655e-02 -2.1785e-02  2.5171e-02 -1.7753e-01\n",
       "  -3.7204e-01 -1.8373e-03  1.8303e-01  1.1935e-01  2.0208e-01 -9.8460e-02\n",
       "  -2.3482e-01  1.8704e-01  5.5659e-02  2.1962e-02  1.9047e-01  2.6100e-01\n",
       "  -1.7788e-01  6.0092e-01  2.9326e-01  2.8505e-01  3.8015e-01  9.6783e-02\n",
       "  -1.5404e-01  1.8885e-01 -3.8993e-01 -2.9448e-02  3.7679e-01 -5.9737e-02\n",
       "   4.5899e-02  3.8854e-01  1.3529e-01 -1.7448e-01  4.7624e-01  2.1100e-01\n",
       "  -4.0401e-02  2.2489e-02 -1.4886e-01 -1.7334e-01  2.3716e-01 -1.9889e-01\n",
       "   1.6619e-01  5.2622e-02  1.7558e-01 -8.6196e-02  3.4562e-01  3.9061e-01\n",
       "  -2.9471e-01 -6.7619e-02 -3.2408e-01 -1.6216e-02 -3.1967e-02  1.5009e-01\n",
       "   3.9847e-01 -1.4082e-02  1.7633e-01 -9.4281e-02 -4.7723e-02  2.5828e-01\n",
       "   2.4360e-01  1.8255e-01  4.8461e-01 -4.7800e-01 -3.4825e-01 -1.9438e-02\n",
       "   2.1299e-01 -2.2184e-02 -2.4107e-01  2.1111e-01  1.5805e-01  2.1999e-01\n",
       "  -3.7911e-01 -1.1499e-02  5.7382e-02  1.5361e-01 -1.2230e-01  4.2185e-01\n",
       "  -5.8408e-02  4.1806e-01 -2.0153e-02  5.4779e-02 -4.0158e-04  1.0569e-01\n",
       "  -1.4511e-01 -1.1859e-01 -3.6510e-01  1.4525e-01 -2.4464e-01 -7.6039e-02\n",
       "  -2.6643e-01 -2.2329e-01  4.3731e-01 -5.1301e-01  7.4741e-01  3.4688e-01\n",
       "  -3.3166e-01 -9.2148e-02 -3.7423e-01  1.7711e-01 -8.0775e-01  4.8568e-01\n",
       "   8.4086e-02 -4.8045e-01  4.1778e-01 -3.0854e-01 -3.6291e-01  3.2975e-01\n",
       "  -2.1750e-01  4.1845e-02 -1.2744e-01 -7.7304e-02 -3.6703e-01 -1.2227e-01\n",
       "   2.9112e-01 -8.8063e-02 -3.7551e-01 -2.7710e-01  2.9051e-01 -8.3915e-02\n",
       "   3.7505e-01 -1.9352e-02  6.8160e-02 -2.0031e-01 -3.7888e-01 -8.0065e-02\n",
       "  -2.0621e-01  2.2677e-01  7.9492e-02 -8.6700e-02  5.1788e-02 -2.3471e-01\n",
       "  -3.5296e-01 -6.5624e-02 -6.6144e-02 -1.0316e-01  3.9204e-01 -1.5470e-01\n",
       "  -3.6786e-01 -4.5449e-01 -2.3352e-01 -5.2126e-01 -1.7362e-01  3.0275e-01\n",
       "  -3.8042e-01  1.8459e-01 -4.9248e-02  4.2196e-01  6.1592e-01  2.3283e-01\n",
       "  -4.2518e-02  1.4258e-01  1.8313e-01  3.9668e-01  4.8164e-01 -1.0803e-01]\n",
       " <NDArray 300 @cpu(0)>, \n",
       " [-2.2178e-01 -1.1102e-01  4.6541e-01 -6.1877e-02  6.2678e-02 -1.4466e-01\n",
       "  -1.4215e-02  2.0011e-01  4.6258e-01  1.6352e-02 -7.4499e-02  1.2514e-01\n",
       "   5.5249e-02  1.2801e-01  6.9373e-02  4.5977e-01 -5.8062e-02  1.2201e-01\n",
       "  -1.4998e-01 -3.0684e-01 -2.4986e-01 -2.1165e-01  1.1256e-01 -3.0883e-01\n",
       "   3.0237e-01  2.1550e-02  2.7651e-01 -1.0339e-01  3.5087e-01  3.9395e-01\n",
       "  -4.0554e-01 -2.8384e-02  2.0866e-01 -3.5324e-01 -2.7761e-01  2.9242e-01\n",
       "   4.2328e-02  1.6435e-01 -1.9711e-02  2.5047e-01 -4.1324e-01 -4.1130e-01\n",
       "  -2.6828e-01  1.8975e-01  1.3324e-01 -3.1936e-01  1.6937e-01  8.2746e-02\n",
       "   1.0103e-01  2.3398e-02 -1.7934e-01  1.1156e-01  2.8904e-01  1.9103e-01\n",
       "   4.6963e-02  1.5345e-02  5.0212e-03  3.1041e-01 -8.8372e-02 -5.4691e-01\n",
       "   2.7751e-01  6.2183e-01 -2.5562e-01 -2.9738e-01  2.7077e-01 -3.2802e-01\n",
       "  -2.3314e-01 -3.9202e-01  3.3975e-01 -6.5893e-01  6.5124e-01 -1.3584e-01\n",
       "   1.8240e-01 -6.9670e-01 -2.3483e-01 -2.7168e-01 -2.0223e-01  3.8996e-01\n",
       "   2.3725e-02  6.3285e-01  2.1829e-01  4.4319e-01  5.3472e-01  9.9405e-02\n",
       "  -3.5851e-01 -9.9638e-01 -1.9442e-01 -4.4434e-01  9.4284e-02  4.6179e-02\n",
       "  -3.3154e-01  2.5125e-01  1.7346e-01 -3.2490e-01  3.4138e-02 -4.3302e-03\n",
       "  -1.5750e-01  4.5221e-01  4.1015e-01 -1.2280e-02  1.7285e-01 -1.3096e-01\n",
       "  -2.8135e-01 -3.9054e-01  2.7119e-01  1.3678e-01  2.5704e-01  8.6692e-02\n",
       "  -3.4781e-01  2.7869e-01  6.3078e-02  2.0350e-01 -4.4366e-01 -2.2333e-01\n",
       "  -1.7883e-01 -4.2304e-01  1.5618e-03 -1.1020e-01  8.0640e-02  9.5702e-02\n",
       "  -4.2592e-01  6.5578e-02 -2.6145e-02  4.1889e-01 -1.1496e-01  1.2850e-03\n",
       "   2.5231e-01 -4.3385e-02  4.1519e-01  3.4583e-01 -2.8074e-01 -3.3683e-01\n",
       "  -6.1615e-02 -8.9918e-02  4.5890e-01 -3.3299e-01  1.5928e-01 -2.8752e-01\n",
       "  -8.2294e-02 -2.9218e-01 -1.6806e-01 -7.5952e-02  4.0410e-01  6.5999e-02\n",
       "  -4.0848e-02 -1.4521e-01  1.3565e-01  8.4954e-02  6.3995e-02 -1.1473e-01\n",
       "  -1.0869e-01 -1.4166e-01  3.3846e-01 -2.3773e-01  4.5470e-02 -9.4609e-02\n",
       "   2.4142e-01  1.0721e-02  1.5099e-01  2.1369e-01  5.5270e-01  2.9385e-01\n",
       "   1.0109e-01 -4.8930e-01  4.3011e-01  1.4758e-01 -9.4901e-02  7.0804e-01\n",
       "   4.4636e-02  1.0062e-01  6.6988e-02 -2.5706e-01 -2.9987e-02  1.1560e+00\n",
       "   1.4114e-01  6.1619e-02  8.4316e-02 -8.3230e-02  2.4894e-02 -9.3390e-02\n",
       "   1.3570e-01  1.9431e-02 -2.2848e-01 -1.6607e-01 -5.4535e-02  2.4074e-01\n",
       "   4.6739e-01 -4.8364e-02  5.6345e-01 -2.2862e-01  2.5807e-01  7.9581e-02\n",
       "  -1.5794e-01  3.5744e-01 -3.9889e-01 -4.7607e-01 -2.0618e-01  1.3388e-01\n",
       "   6.5922e-02 -5.2323e-01  1.8560e-01 -6.9763e-02 -5.7227e-02  1.0507e-01\n",
       "  -1.5237e-01 -1.0338e-02  3.1364e-01 -2.8665e-01 -3.7366e-01  1.7459e-01\n",
       "   2.2858e-01  1.8979e-01 -9.3668e-02  4.1257e-02 -2.7133e-01 -3.7057e-05\n",
       "  -1.4959e-01 -2.1678e-01 -3.8332e-03  3.7801e-01 -2.0198e-01  4.2839e-01\n",
       "   9.2664e-03 -5.9741e-01  9.9742e-02 -2.3094e-02 -7.4158e-03  3.6597e-02\n",
       "  -1.2929e-01 -7.6848e-02 -3.4902e-01  9.8642e-02 -1.6359e-01 -1.6294e-01\n",
       "  -2.0227e-01  2.0356e-01  5.0828e-02 -4.8479e-03  2.6663e-01  8.3204e-01\n",
       "   1.4004e-01  1.5445e-01 -1.1327e-01  1.8543e-01  4.0704e-02  1.0477e-02\n",
       "   2.4602e-01 -1.3742e-01  1.6034e-01 -1.0360e-01  3.0651e-01  1.0948e-01\n",
       "   6.9977e-02 -4.8683e-02 -1.4797e-01  4.4382e-01 -5.6509e-01 -2.4416e-01\n",
       "   6.1582e-01 -2.6037e-02  1.3897e-02 -3.0444e-01  4.3399e-01 -7.5926e-02\n",
       "   1.5658e-01 -1.0949e-01  1.5686e-01  4.0721e-01  1.3640e-01  1.7220e-02\n",
       "  -1.8959e-02 -3.3707e-01  6.2020e-01 -6.0976e-02 -2.8422e-01  3.6943e-03\n",
       "   1.9609e-01  4.9187e-01  2.0856e-01  1.8850e-01  1.2024e-01 -5.5086e-01\n",
       "  -2.5358e-02 -7.5673e-02  1.5767e-01 -3.8056e-01  2.5646e-01 -5.3275e-03\n",
       "  -7.7044e-02  6.6383e-01  3.1355e-02 -2.5800e-01  3.8075e-01 -9.6889e-02\n",
       "  -9.4744e-02  1.2137e-01 -4.3812e-02  3.8293e-01 -1.3362e-01  6.9340e-02]\n",
       " <NDArray 300 @cpu(0)>)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.embedding['사과'], vocab.embedding['배']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코사인 유사도 계산\n",
    "- -1 ~ 1 사이의 값, 1에 가까울 수록 유사도가 크다. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[-1.]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "from mxnet import nd\n",
    "def cos_sim(x, y):\n",
    "    return nd.dot(x, y) / (nd.norm(x) * nd.norm(y))\n",
    "\n",
    "x = nd.array([1,2])\n",
    "y = nd.array([-1,-2])\n",
    "print(cos_sim(x,x))\n",
    "print(cos_sim(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 조사가 붙은 경우 토큰 벡터 도출 가능 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0.8691836]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(vocab.embedding['다이어트는'], vocab.embedding['다이어트에'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0.342196]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(vocab.embedding['배'], vocab.embedding['과일'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0.5872627]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(vocab.embedding['사과'], vocab.embedding['과일'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0.3542343]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(vocab.embedding['배'], vocab.embedding['사과'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 단어 유사도 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "대량의 토큰 벡터중에서 입력된 토큰과 가장 유사한 top N개의 단어 추출 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def norm_vecs_by_row(x):\n",
    "    return x / nd.sqrt(nd.sum(x * x, axis=1) + 1E-10).reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_knn(vocab, k, word):\n",
    "    word_vec = vocab.embedding[word].reshape((-1, 1))\n",
    "    vocab_vecs = norm_vecs_by_row(vocab.embedding.idx_to_vec)\n",
    "    dot_prod = nd.dot(vocab_vecs, word_vec)\n",
    "    indices = nd.topk(dot_prod.reshape((len(vocab), )), k=k+1, ret_typ='indices')\n",
    "    indices = [int(i.asscalar()) for i in indices]\n",
    "    return vocab.to_tokens(indices[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['다이어트는', '다이어트에', '라이프']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_knn(vocab, 3, '다이어트')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word Analogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can also apply pre-trained word embeddings to the word\n",
    "analogy problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\"man : woman :: son : daughter\"를 analogy 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "'a : b :: c : d' ==  vec('c') + (vec('b')-vec('a')).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding file wiki.en.npz is not found. Downloading from Gluon Repository. This may take some time.\n",
      "Downloading /home/ubuntu/.mxnet/embedding/fasttext/wiki.en.npz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/embeddings/fasttext/wiki.en.npz...\n"
     ]
    }
   ],
   "source": [
    "fasttext_simple = nlp.embedding.create('fasttext', source='wiki.en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = nlp.Vocab(nlp.data.count_tokens(fasttext_simple.idx_to_token),min_freq=1)\n",
    "\n",
    "vocab.set_embedding(fasttext_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_top_k_by_analogy(vocab, k, word1, word2, word3):\n",
    "    word_vecs = vocab.embedding[word1, word2, word3]\n",
    "    word_diff = (word_vecs[1] - word_vecs[0] + word_vecs[2]).reshape((-1, 1))\n",
    "    vocab_vecs = norm_vecs_by_row(vocab.embedding.idx_to_vec)\n",
    "    dot_prod = nd.dot(vocab_vecs, word_diff)\n",
    "    indices = nd.topk(dot_prod.reshape((len(vocab), )), k=k, ret_typ='indices')\n",
    "    indices = [int(i.asscalar()) for i in indices]\n",
    "    return vocab.to_tokens(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Complete word analogy 'man : woman :: son :'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['daughter']"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_k_by_analogy(vocab, 1, 'man', 'woman', 'son')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "word analogy 'beijing : china :: tokyo : '."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['japan']"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_k_by_analogy(vocab, 1, 'beijing', 'china', 'tokyo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "word analogy 'bad : worst :: big : '."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['big']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_k_by_analogy(vocab, 1, 'bad', 'worst', 'big')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "word analogy 'do : did :: go :'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['went']"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_k_by_analogy(vocab, 1, 'do', 'did', 'go')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
