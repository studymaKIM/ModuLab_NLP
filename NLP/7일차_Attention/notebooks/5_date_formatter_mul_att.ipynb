{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from mxnet import nd, autograd, gluon\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet.ndarray.linalg import gemm2\n",
    "import datetime\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n"
     ]
    }
   ],
   "source": [
    "def padding(chars, maxlen):\n",
    "    if len(chars) < maxlen:\n",
    "        return chars + ' ' * (maxlen - len(chars))\n",
    "    else:\n",
    "        return chars[:maxlen]\n",
    "\n",
    "def gen_date():\n",
    "    rnd = int(np.random.uniform(low = 1000000000, high = 1350000000))\n",
    "    timestamp = datetime.fromtimestamp(rnd)\n",
    "    return str(timestamp.strftime('%Y-%B-%d %a')) # '%Y-%B-%d %H:%M:%S'\n",
    "\n",
    "def format_date(x):\n",
    "    return str(datetime.strptime(x, '%Y-%B-%d %a').strftime('%m/%d/%Y, %A')).lower() #'%H%M%S:%Y%m%d'\n",
    "\n",
    "N = 1000\n",
    "N_train = int(N * .9)\n",
    "N_validation = N - N_train\n",
    "\n",
    "in_seq_len = 32\n",
    "out_seq_len = 32\n",
    "\n",
    "added = set()\n",
    "questions = []\n",
    "answers = []\n",
    "answers_y = []\n",
    "\n",
    "while len(questions) < N:\n",
    "    if len(questions) % 1000 == 0:\n",
    "        print('i = {}'.format(len(questions)))\n",
    "    a = gen_date()\n",
    "    if a in added:\n",
    "        continue\n",
    "    question = '[{}]'.format(a)\n",
    "    answer = '[' + str(format_date(a)) + ']'\n",
    "    answer = padding(answer, out_seq_len)\n",
    "    answer_y = str(format_date(a)) + ']'\n",
    "    answer_y = padding(answer_y, out_seq_len)\n",
    "    \n",
    "    added.add(a)\n",
    "    questions.append(question)\n",
    "    answers.append(answer)\n",
    "    answers_y.append(answer_y)\n",
    "\n",
    "chars = list(set(''.join(questions[:20000])))\n",
    "chars.extend(['[', ']']) # Start and End of Expression\n",
    "chars.extend(list(set(''.join(answers[:20000]))))\n",
    "chars = np.sort(list(set(chars)))\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((len(questions), in_seq_len, len(chars)), dtype=np.integer)\n",
    "Y = np.zeros((len(questions), out_seq_len, len(chars)), dtype=np.integer)\n",
    "Z = np.zeros((len(questions), out_seq_len, len(chars)), dtype=np.integer)\n",
    "\n",
    "for i in range(N):\n",
    "    for t, char in enumerate(questions[i]):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    for t, char in enumerate(answers[i]):\n",
    "        Y[i, t, char_indices[char]] = 1\n",
    "    for t, char in enumerate(answers_y[i]):\n",
    "        Z[i, t, char_indices[char]] = 1\n",
    "    \n",
    "X_train, X_validation, Y_train, Y_validation, Z_train, Z_validation = \\\n",
    "    train_test_split(X, Y, Z, train_size=N_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_test(N):\n",
    "    q = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        question = gen_date()\n",
    "        answer_y = format_date(question)\n",
    "        q.append(question)\n",
    "        y.append(answer_y)\n",
    "    return(q,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['2005-November-03 Thu', '2012-May-28 Mon'],\n",
       " ['11/03/2005, thursday', '05/28/2012, monday'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_test(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class alignment(gluon.HybridBlock):\n",
    "    def __init__(self, n_hidden, **args):\n",
    "        super(alignment, self).__init__(**args)\n",
    "        with self.name_scope():\n",
    "            self.weight = self.params.get('weight', shape = (n_hidden, n_hidden), allow_deferred_init = True)\n",
    "        \n",
    "    def hybrid_forward(self, F, inputs, output, weight):\n",
    "        _s = F.dot(inputs, weight)\n",
    "        return gemm2(_s, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class format_translator(gluon.Block):\n",
    "    def __init__(self, n_hidden, in_seq_len, out_seq_len, vocab_size, ctx, **kwargs):\n",
    "        super(format_translator, self).__init__(**kwargs)\n",
    "        self.in_seq_len = in_seq_len\n",
    "        self.out_seq_len = out_seq_len\n",
    "        self.n_hidden = n_hidden\n",
    "        self.vocab_size = vocab_size\n",
    "        self.ctx = ctx\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.encoder = rnn.LSTMCell(hidden_size = n_hidden)\n",
    "            self.decoder = rnn.LSTMCell(hidden_size = n_hidden)\n",
    "            self.alignment = alignment(n_hidden)\n",
    "            self.attn_weight = nn.Dense(self.in_seq_len, in_units = self.in_seq_len)            \n",
    "            self.batchnorm = nn.BatchNorm(axis = 2)\n",
    "            self.dense = nn.Dense(self.vocab_size, flatten = False)\n",
    "            \n",
    "    def forward(self, inputs, outputs):\n",
    "        self.batch_size = inputs.shape[0]\n",
    "        enout, (next_h, next_c) = self.encoder.unroll(inputs = inputs, length = self.in_seq_len, merge_outputs = True)\n",
    "        \n",
    "        for i in range(self.out_seq_len):\n",
    "            # For each time step, caclculate context for attention\n",
    "            # Use enout(batch_size * in_seq_len * n_hidden)\n",
    "            _n_h = next_h.expand_dims(axis = 2)       \n",
    "            score_i = self.alignment(enout, _n_h)\n",
    "            alpha_i = nd.softmax(self.attn_weight(score_i))\n",
    "            alpha_expand = alpha_i.expand_dims(2) # (n_batch * in_seq_len * n_hidden)\n",
    "            alpha_expand = nd.repeat(alpha_expand,repeats= self.n_hidden, axis=2) # n_batch * in_seq_len * n_hidden\n",
    "            context = nd.multiply(alpha_expand, enout) # n_batch * in_seq_len * n_hidden\n",
    "            context = nd.sum(context, axis = 1) # n_batch * n_hidden\n",
    "            _in = nd.concat(outputs[:, i, :], context)\n",
    "            deout, (next_h, next_c) = self.decoder(_in, [next_h, next_c],)\n",
    "            if i == 0:\n",
    "                deouts = deout\n",
    "            else:\n",
    "                deouts = nd.concat(deouts, deout, dim = 1)   \n",
    "        deouts = nd.reshape(deouts, (-1, self.out_seq_len, self.n_hidden))\n",
    "        deouts = self.batchnorm(deouts)\n",
    "        deouts_fc = self.dense(deouts)\n",
    "        return deouts_fc\n",
    "    \n",
    "    def predict(self, input_str, char_indices, indices_char, input_digits = 9, lchars = 14, ctx = mx.cpu()):\n",
    "        # No label when evaluating new example. So try to put the result of the previous time step\n",
    "        alpha = []\n",
    "        input_str = '[' + input_str + ']'\n",
    "        X = nd.zeros((1, input_digits, lchars), ctx = ctx)\n",
    "        for t, char in enumerate(input_str):\n",
    "            X[0, t, char_indices[char]] = 1\n",
    "        Y_init = nd.zeros((1, lchars), ctx = ctx)\n",
    "        Y_init[0, char_indices['[']] = 1\n",
    "        enout, (next_h, next_c) = self.encoder.unroll(inputs = X, length = self.in_seq_len, merge_outputs = True)\n",
    "        deout = Y_init\n",
    "\n",
    "        for i in range(self.out_seq_len):\n",
    "            _n_h = next_h.expand_dims(axis = 2)\n",
    "            ####### Attention part: To get context vector at jth point of output sequence\n",
    "            score_i = self.alignment(enout, _n_h)\n",
    "            alpha_i = nd.softmax(self.attn_weight(score_i))\n",
    "            # alpha:(n_batch * in_seq_len) -> Expand alpha to (n_batch * in_seq_len * n_hidden)\n",
    "            alpha_expand = alpha_i.expand_dims(2)\n",
    "            alpha_expand = nd.repeat(alpha_expand,repeats= self.n_hidden, axis=2) # n_batch * time step * n_hidden\n",
    "            context = nd.multiply(alpha_expand, enout)\n",
    "            context = nd.sum(context, axis = 1) # n_batch * n_hidden\n",
    "            \n",
    "            _in = nd.concat(deout, context)\n",
    "            deout, (next_h, next_c) = self.decoder(_in, [next_h, next_c],)\n",
    "            deout = nd.expand_dims(deout, axis = 1)\n",
    "            deout = self.batchnorm(deout)\n",
    "            deout = deout[:, 0, :]\n",
    "            deout_sm = self.dense(deout)\n",
    "            deout = nd.one_hot(nd.argmax(nd.softmax(deout_sm, axis = 1), axis = 1), depth = self.vocab_size)\n",
    "            if i == 0:\n",
    "                ret_seq = indices_char[nd.argmax(deout_sm, axis = 1).asnumpy()[0].astype('int')]\n",
    "            else:\n",
    "                ret_seq += indices_char[nd.argmax(deout_sm, axis = 1).asnumpy()[0].astype('int')]\n",
    "                \n",
    "            if ret_seq[-1] == ']':\n",
    "                break\n",
    "            alpha.append(alpha_i.asnumpy())\n",
    "        return ret_seq.strip(']').strip(), np.squeeze(np.array(alpha), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_set = gluon.data.ArrayDataset(X_train, Y_train, Z_train)\n",
    "tr_data_iterator = gluon.data.DataLoader(tr_set, batch_size=256, shuffle=True)\n",
    "\n",
    "te_set =gluon.data.ArrayDataset(X_validation, Y_validation, Z_validation)\n",
    "te_data_iterator = gluon.data.DataLoader(te_set, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu()\n",
    "model = pattern_matcher(300, in_seq_len, out_seq_len, len(chars), ctx)\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx = ctx)\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), 'rmsprop')\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss(axis = 2, sparse_label = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pattern_matcher(\n",
      "  (encoder): LSTMCell(None -> 1200)\n",
      "  (decoder): LSTMCell(None -> 1200)\n",
      "  (attn_weight): Dense(32 -> 32, linear)\n",
      "  (batchnorm): BatchNorm(axis=2, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "  (dense): Dense(None -> 47, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(model, data_iter, loss_obj, ctx = ctx):\n",
    "    test_loss = []\n",
    "    for i, (x_data, y_data, z_data) in enumerate(data_iter):\n",
    "        x_data = x_data.as_in_context(ctx).astype('float32')\n",
    "        y_data = y_data.as_in_context(ctx).astype('float32')\n",
    "        z_data = z_data.as_in_context(ctx).astype('float32')\n",
    "        with autograd.predict_mode():\n",
    "            z_output = model(x_data, y_data)\n",
    "            loss_te = loss_obj(z_output, z_data)\n",
    "        curr_loss = nd.mean(loss_te).asscalar()\n",
    "        test_loss.append(curr_loss)\n",
    "    return np.mean(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m☒\u001b[0m 2004-October-15 Fri = 0(10/15/2004, friday) 0, attention (32, 32)\n",
      "\u001b[91m☒\u001b[0m 2012-July-07 Sat = 0(07/07/2012, saturday) 0, attention (32, 32)\n",
      "\u001b[91m☒\u001b[0m 2012-April-03 Tue = 0(04/03/2012, tuesday) 0, attention (32, 32)\n",
      "\u001b[91m☒\u001b[0m 2004-January-07 Wed = 0(01/07/2004, wednesday) 0, attention (32, 32)\n",
      "\u001b[91m☒\u001b[0m 2005-March-20 Sun = 0(03/20/2005, sunday) 0, attention (32, 32)\n",
      "\u001b[91m☒\u001b[0m 2008-May-17 Sat = 0(05/17/2008, saturday) 0, attention (32, 32)\n",
      "\u001b[91m☒\u001b[0m 2002-April-14 Sun = 0(04/14/2002, sunday) 0, attention (32, 32)\n",
      "\u001b[91m☒\u001b[0m 2009-December-08 Tue = 0(12/08/2009, tuesday) 0, attention (32, 32)\n",
      "\u001b[91m☒\u001b[0m 2005-July-02 Sat = 0(07/02/2005, saturday) 0, attention (32, 32)\n",
      "\u001b[91m☒\u001b[0m 2009-July-05 Sun = 0(07/05/2009, sunday) 0, attention (32, 32)\n",
      "Epoch 0. Train Loss: 2.395939, Test Loss : 3.6893427\n",
      "Epoch 1. Train Loss: 0.81353974, Test Loss : 3.6291702\n",
      "Epoch 2. Train Loss: 0.58116, Test Loss : 3.5943646\n",
      "Epoch 3. Train Loss: 0.47645617, Test Loss : 3.5426939\n",
      "Epoch 4. Train Loss: 0.4466805, Test Loss : 3.5034192\n",
      "Epoch 5. Train Loss: 0.39420557, Test Loss : 3.4386268\n",
      "Epoch 6. Train Loss: 0.3384366, Test Loss : 3.349009\n",
      "Epoch 7. Train Loss: 0.30531383, Test Loss : 3.2393503\n",
      "Epoch 8. Train Loss: 0.31576252, Test Loss : 3.1475675\n",
      "Epoch 9. Train Loss: 0.23709776, Test Loss : 3.0645065\n",
      "\u001b[91m☒\u001b[0m 2002-November-25 Mon = 12/202/20202, monday(11/25/2002, monday) 0, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2001-December-28 Fri = 12/22002, friday(12/28/2001, friday) 0, attention (16, 32)\n",
      "\u001b[91m☒\u001b[0m 2005-November-17 Thu = 11/2/2005, thursday(11/17/2005, thursday) 0, attention (19, 32)\n",
      "\u001b[91m☒\u001b[0m 2005-April-12 Tue = 02/2002, tuesday(04/12/2005, tuesday) 0, attention (16, 32)\n",
      "\u001b[91m☒\u001b[0m 2009-December-08 Tue = 09/22009, tuesday(12/08/2009, tuesday) 0, attention (17, 32)\n",
      "\u001b[91m☒\u001b[0m 2002-September-26 Thu = 09/2202/20202, tuesday(09/26/2002, thursday) 0, attention (22, 32)\n",
      "\u001b[91m☒\u001b[0m 2006-November-23 Thu = 12/2006, thursday(11/23/2006, thursday) 0, attention (17, 32)\n",
      "\u001b[91m☒\u001b[0m 2007-July-20 Fri = 07/2027/2002, friday(07/20/2007, friday) 0, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2005-June-23 Thu = 03/22002, thursday(06/23/2005, thursday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2008-August-13 Wed = 08/28/2008, wednesday(08/13/2008, wednesday) 0, attention (21, 32)\n",
      "Epoch 10. Train Loss: 0.24362935, Test Loss : 2.7528272\n",
      "Epoch 11. Train Loss: 0.21583247, Test Loss : 2.2676785\n",
      "Epoch 12. Train Loss: 0.19129337, Test Loss : 2.1205494\n",
      "Epoch 13. Train Loss: 0.17159396, Test Loss : 1.8808653\n",
      "Epoch 14. Train Loss: 0.12732711, Test Loss : 1.5721625\n",
      "Epoch 15. Train Loss: 0.12818532, Test Loss : 1.1925623\n",
      "Epoch 16. Train Loss: 0.103757665, Test Loss : 0.6732225\n",
      "Epoch 17. Train Loss: 0.10531356, Test Loss : 0.46686202\n",
      "Epoch 18. Train Loss: 0.11784342, Test Loss : 0.38273856\n",
      "Epoch 19. Train Loss: 0.078988075, Test Loss : 0.1648074\n",
      "\u001b[92m☑\u001b[0m 2004-April-04 Sun = 04/04/2004, sunday(04/04/2004, sunday) 1, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2009-September-25 Fri = 09/202/2009, friday(09/25/2009, friday) 0, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-August-14 Sat = 08/14/2004, saturday(08/14/2004, saturday) 1, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2009-September-26 Sat = 09/209/2009, saturday(09/26/2009, saturday) 0, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-February-07 Sat = 02/07/2004, saturday(02/07/2004, saturday) 1, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2001-October-30 Tue = 01/03/2000, tuesday(10/30/2001, tuesday) 0, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-June-24 Tue = 06/24/2008, tuesday(06/24/2008, tuesday) 1, attention (19, 32)\n",
      "\u001b[91m☒\u001b[0m 2006-September-30 Sat = 09/2003, saturday(09/30/2006, saturday) 0, attention (17, 32)\n",
      "\u001b[91m☒\u001b[0m 2001-November-02 Fri = 11/200/200, friday(11/02/2001, friday) 0, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-July-23 Wed = 07/23/2003, wednesday(07/23/2003, wednesday) 1, attention (21, 32)\n",
      "Epoch 20. Train Loss: 0.048643723, Test Loss : 0.12532508\n",
      "Epoch 21. Train Loss: 0.04462936, Test Loss : 0.13035692\n",
      "Epoch 22. Train Loss: 0.08318296, Test Loss : 0.089696385\n",
      "Epoch 23. Train Loss: 0.049106877, Test Loss : 0.04983679\n",
      "Epoch 24. Train Loss: 0.034303486, Test Loss : 0.040010743\n",
      "Epoch 25. Train Loss: 0.03667003, Test Loss : 0.070397966\n",
      "Epoch 26. Train Loss: 0.098361716, Test Loss : 0.20250206\n",
      "Epoch 27. Train Loss: 0.0492296, Test Loss : 0.03407221\n",
      "Epoch 28. Train Loss: 0.017184082, Test Loss : 0.024848979\n",
      "Epoch 29. Train Loss: 0.016414389, Test Loss : 0.019909149\n",
      "\u001b[92m☑\u001b[0m 2002-November-07 Thu = 11/07/2002, thursday(11/07/2002, thursday) 1, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2001-October-03 Wed = 10/03/2000, wednesday(10/03/2001, wednesday) 0, attention (21, 32)\n",
      "\u001b[91m☒\u001b[0m 2010-May-06 Thu = 05/06/2001, thursday(05/06/2010, thursday) 0, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2005-November-26 Sat = 11/26/2005, saturday(11/26/2005, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-May-25 Tue = 05/25/2004, tuesday(05/25/2004, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-March-10 Tue = 03/10/2009, tuesday(03/10/2009, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-May-16 Fri = 05/16/2008, friday(05/16/2008, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-March-29 Sat = 03/29/2008, saturday(03/29/2008, saturday) 1, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2011-January-02 Sun = 10/201/2011, sunday(01/02/2011, sunday) 0, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2012-July-08 Sun = 07/08/2012, sunday(07/08/2012, sunday) 1, attention (18, 32)\n",
      "Epoch 30. Train Loss: 0.017033001, Test Loss : 0.06262737\n",
      "Epoch 31. Train Loss: 0.09136516, Test Loss : 0.059815265\n",
      "Epoch 32. Train Loss: 0.019968655, Test Loss : 0.018450666\n",
      "Epoch 33. Train Loss: 0.0107776, Test Loss : 0.015124925\n",
      "Epoch 34. Train Loss: 0.009608275, Test Loss : 0.011131956\n",
      "Epoch 35. Train Loss: 0.009323087, Test Loss : 0.012413505\n",
      "Epoch 36. Train Loss: 0.025338097, Test Loss : 0.0836604\n",
      "Epoch 37. Train Loss: 0.050851848, Test Loss : 0.044700082\n",
      "Epoch 38. Train Loss: 0.016948733, Test Loss : 0.008857282\n",
      "Epoch 39. Train Loss: 0.0069478066, Test Loss : 0.008137796\n",
      "\u001b[92m☑\u001b[0m 2004-December-27 Mon = 12/27/2004, monday(12/27/2004, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-September-02 Thu = 09/02/2004, thursday(09/02/2004, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2011-March-01 Tue = 03/01/2011, tuesday(03/01/2011, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-March-15 Wed = 03/15/2006, wednesday(03/15/2006, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-April-11 Sun = 04/11/2010, sunday(04/11/2010, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2002-March-13 Wed = 03/13/2002, wednesday(03/13/2002, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-May-15 Sat = 05/15/2004, saturday(05/15/2004, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-August-14 Sat = 08/14/2010, saturday(08/14/2010, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-March-23 Sun = 03/23/2008, sunday(03/23/2008, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-September-04 Sat = 09/04/2004, saturday(09/04/2004, saturday) 1, attention (20, 32)\n",
      "Epoch 40. Train Loss: 0.006551991, Test Loss : 0.010062315\n",
      "Epoch 41. Train Loss: 0.005654849, Test Loss : 0.005807136\n",
      "Epoch 42. Train Loss: 0.0052017756, Test Loss : 0.015610882\n",
      "Epoch 43. Train Loss: 0.01394132, Test Loss : 0.06323563\n",
      "Epoch 44. Train Loss: 0.091938525, Test Loss : 0.111586444\n",
      "Epoch 45. Train Loss: 0.01638319, Test Loss : 0.008136414\n",
      "Epoch 46. Train Loss: 0.0047055706, Test Loss : 0.0052270805\n",
      "Epoch 47. Train Loss: 0.0035846476, Test Loss : 0.0046141185\n",
      "Epoch 48. Train Loss: 0.0031456135, Test Loss : 0.0038355037\n",
      "Epoch 49. Train Loss: 0.0029468627, Test Loss : 0.004418081\n",
      "\u001b[92m☑\u001b[0m 2008-July-18 Fri = 07/18/2008, friday(07/18/2008, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-May-09 Sun = 05/09/2010, sunday(05/09/2010, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2002-November-24 Sun = 11/24/2002, sunday(11/24/2002, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-November-16 Sun = 11/16/2003, sunday(11/16/2003, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2002-July-04 Thu = 07/04/2002, thursday(07/04/2002, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2011-May-26 Thu = 05/26/2011, thursday(05/26/2011, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2012-June-15 Fri = 06/15/2012, friday(06/15/2012, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-September-08 Mon = 09/08/2008, monday(09/08/2008, monday) 1, attention (18, 32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m☑\u001b[0m 2008-February-07 Thu = 02/07/2008, thursday(02/07/2008, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-June-03 Sat = 06/03/2006, saturday(06/03/2006, saturday) 1, attention (20, 32)\n",
      "Epoch 50. Train Loss: 0.0026186996, Test Loss : 0.004156586\n",
      "Epoch 51. Train Loss: 0.0022333572, Test Loss : 0.0039726254\n",
      "Epoch 52. Train Loss: 0.0021800331, Test Loss : 0.0037313264\n",
      "Epoch 53. Train Loss: 0.0019098482, Test Loss : 0.0033289378\n",
      "Epoch 54. Train Loss: 0.0017121483, Test Loss : 0.003043403\n",
      "Epoch 55. Train Loss: 0.04322721, Test Loss : 0.33151358\n",
      "Epoch 56. Train Loss: 0.053014234, Test Loss : 0.12193992\n",
      "Epoch 57. Train Loss: 0.006292376, Test Loss : 0.008294666\n",
      "Epoch 58. Train Loss: 0.0022521527, Test Loss : 0.004821221\n",
      "Epoch 59. Train Loss: 0.0019050635, Test Loss : 0.003563597\n",
      "\u001b[92m☑\u001b[0m 2003-October-16 Thu = 10/16/2003, thursday(10/16/2003, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2002-March-30 Sat = 03/30/2002, saturday(03/30/2002, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2012-March-10 Sat = 03/10/2012, saturday(03/10/2012, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-February-04 Tue = 02/04/2003, tuesday(02/04/2003, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-May-24 Sun = 05/24/2009, sunday(05/24/2009, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-April-24 Thu = 04/24/2003, thursday(04/24/2003, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-February-04 Thu = 02/04/2010, thursday(02/04/2010, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-November-01 Mon = 11/01/2004, monday(11/01/2004, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-April-13 Mon = 04/13/2009, monday(04/13/2009, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2005-October-28 Fri = 10/28/2005, friday(10/28/2005, friday) 1, attention (18, 32)\n",
      "Epoch 60. Train Loss: 0.0016636279, Test Loss : 0.0029609033\n",
      "Epoch 61. Train Loss: 0.0015197033, Test Loss : 0.0020774659\n",
      "Epoch 62. Train Loss: 0.001361608, Test Loss : 0.0020433003\n",
      "Epoch 63. Train Loss: 0.0012261169, Test Loss : 0.0015689218\n",
      "Epoch 64. Train Loss: 0.0011389138, Test Loss : 0.0014017728\n",
      "Epoch 65. Train Loss: 0.0010055564, Test Loss : 0.0013242891\n",
      "Epoch 66. Train Loss: 0.00092250155, Test Loss : 0.0014077356\n",
      "Epoch 67. Train Loss: 0.0008535872, Test Loss : 0.0013114966\n",
      "Epoch 68. Train Loss: 0.00077448913, Test Loss : 0.0011041993\n",
      "Epoch 69. Train Loss: 0.0006432206, Test Loss : 0.00097213493\n",
      "\u001b[92m☑\u001b[0m 2011-September-29 Thu = 09/29/2011, thursday(09/29/2011, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-October-19 Tue = 10/19/2010, tuesday(10/19/2010, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2001-December-26 Wed = 12/26/2001, wednesday(12/26/2001, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-July-09 Fri = 07/09/2010, friday(07/09/2010, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2012-September-03 Mon = 09/03/2012, monday(09/03/2012, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-October-26 Tue = 10/26/2004, tuesday(10/26/2004, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-January-15 Sun = 01/15/2006, sunday(01/15/2006, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-September-25 Mon = 09/25/2006, monday(09/25/2006, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-May-28 Thu = 05/28/2009, thursday(05/28/2009, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-October-13 Sat = 10/13/2007, saturday(10/13/2007, saturday) 1, attention (20, 32)\n",
      "Epoch 70. Train Loss: 0.0005909746, Test Loss : 0.0008593224\n",
      "Epoch 71. Train Loss: 0.0005336772, Test Loss : 0.00085003575\n",
      "Epoch 72. Train Loss: 0.00049668417, Test Loss : 0.0007585304\n",
      "Epoch 73. Train Loss: 0.0005218715, Test Loss : 0.000692947\n",
      "Epoch 74. Train Loss: 0.0005592791, Test Loss : 0.0053648474\n",
      "Epoch 75. Train Loss: 0.069049165, Test Loss : 0.08921654\n",
      "Epoch 76. Train Loss: 0.006193761, Test Loss : 0.00414369\n",
      "Epoch 77. Train Loss: 0.0011503365, Test Loss : 0.0017947321\n",
      "Epoch 78. Train Loss: 0.00087895466, Test Loss : 0.0010334254\n",
      "Epoch 79. Train Loss: 0.00072410173, Test Loss : 0.0008695156\n",
      "\u001b[92m☑\u001b[0m 2009-February-21 Sat = 02/21/2009, saturday(02/21/2009, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2012-August-25 Sat = 08/25/2012, saturday(08/25/2012, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-April-19 Mon = 04/19/2004, monday(04/19/2004, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-August-23 Mon = 08/23/2010, monday(08/23/2010, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2005-May-24 Tue = 05/24/2005, tuesday(05/24/2005, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2012-March-03 Sat = 03/03/2012, saturday(03/03/2012, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-December-12 Sat = 12/12/2009, saturday(12/12/2009, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-October-19 Fri = 10/19/2007, friday(10/19/2007, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-June-11 Fri = 06/11/2010, friday(06/11/2010, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2005-December-20 Tue = 12/20/2005, tuesday(12/20/2005, tuesday) 1, attention (19, 32)\n",
      "Epoch 80. Train Loss: 0.000625553, Test Loss : 0.0008077294\n",
      "Epoch 81. Train Loss: 0.000553912, Test Loss : 0.0006888544\n",
      "Epoch 82. Train Loss: 0.0005186041, Test Loss : 0.0006262642\n",
      "Epoch 83. Train Loss: 0.0004535477, Test Loss : 0.00059538806\n",
      "Epoch 84. Train Loss: 0.00040666148, Test Loss : 0.00062461995\n",
      "Epoch 85. Train Loss: 0.00037516005, Test Loss : 0.00049952464\n",
      "Epoch 86. Train Loss: 0.0003666471, Test Loss : 0.00044100828\n",
      "Epoch 87. Train Loss: 0.00032224265, Test Loss : 0.0004198145\n",
      "Epoch 88. Train Loss: 0.00030449376, Test Loss : 0.00038332067\n",
      "Epoch 89. Train Loss: 0.00027889773, Test Loss : 0.00036487033\n",
      "\u001b[92m☑\u001b[0m 2010-November-12 Fri = 11/12/2010, friday(11/12/2010, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2002-December-20 Fri = 12/20/2002, friday(12/20/2002, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-July-26 Mon = 07/26/2010, monday(07/26/2010, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-June-17 Sun = 06/17/2007, sunday(06/17/2007, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2002-April-15 Mon = 04/15/2002, monday(04/15/2002, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-December-27 Thu = 12/27/2007, thursday(12/27/2007, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2001-October-01 Mon = 10/01/2001, monday(10/01/2001, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2005-November-16 Wed = 11/16/2005, wednesday(11/16/2005, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-June-15 Sun = 06/15/2008, sunday(06/15/2008, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-January-31 Tue = 01/31/2006, tuesday(01/31/2006, tuesday) 1, attention (19, 32)\n",
      "Epoch 90. Train Loss: 0.00025856067, Test Loss : 0.00034013073\n",
      "Epoch 91. Train Loss: 0.00023861196, Test Loss : 0.00032245863\n",
      "Epoch 92. Train Loss: 0.00023057962, Test Loss : 0.0003244562\n",
      "Epoch 93. Train Loss: 0.00022266744, Test Loss : 0.00027552404\n",
      "Epoch 94. Train Loss: 0.00020125922, Test Loss : 0.0002582897\n",
      "Epoch 95. Train Loss: 0.00019764, Test Loss : 0.00029526206\n",
      "Epoch 96. Train Loss: 0.00018640331, Test Loss : 0.00023889015\n",
      "Epoch 97. Train Loss: 0.00017455612, Test Loss : 0.00022668431\n",
      "Epoch 98. Train Loss: 0.00016539487, Test Loss : 0.00021185391\n",
      "Epoch 99. Train Loss: 0.0001554766, Test Loss : 0.00019602457\n",
      "\u001b[92m☑\u001b[0m 2011-June-21 Tue = 06/21/2011, tuesday(06/21/2011, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2005-March-12 Sat = 03/12/2005, saturday(03/12/2005, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-March-12 Wed = 03/12/2008, wednesday(03/12/2008, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-October-30 Thu = 10/30/2003, thursday(10/30/2003, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-February-22 Sun = 02/22/2004, sunday(02/22/2004, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-December-04 Thu = 12/04/2008, thursday(12/04/2008, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-June-21 Sat = 06/21/2008, saturday(06/21/2008, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2002-March-02 Sat = 03/02/2002, saturday(03/02/2002, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-May-22 Thu = 05/22/2008, thursday(05/22/2008, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-June-03 Tue = 06/03/2003, tuesday(06/03/2003, tuesday) 1, attention (19, 32)\n",
      "Epoch 100. Train Loss: 0.0001473783, Test Loss : 0.00018604854\n",
      "Epoch 101. Train Loss: 0.00013658036, Test Loss : 0.00017662939\n",
      "Epoch 102. Train Loss: 0.00013490408, Test Loss : 0.00018255446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103. Train Loss: 0.00012805413, Test Loss : 0.00016931038\n",
      "Epoch 104. Train Loss: 0.00012011871, Test Loss : 0.00015438508\n",
      "Epoch 105. Train Loss: 0.00011780177, Test Loss : 0.0001584128\n",
      "Epoch 106. Train Loss: 0.00012191325, Test Loss : 0.00014727496\n",
      "Epoch 107. Train Loss: 0.00010968108, Test Loss : 0.00014118655\n",
      "Epoch 108. Train Loss: 0.00010759296, Test Loss : 0.00013682702\n",
      "Epoch 109. Train Loss: 0.00010360351, Test Loss : 0.00013617246\n",
      "\u001b[92m☑\u001b[0m 2010-March-30 Tue = 03/30/2010, tuesday(03/30/2010, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-July-27 Thu = 07/27/2006, thursday(07/27/2006, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2012-August-06 Mon = 08/06/2012, monday(08/06/2012, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2005-May-30 Mon = 05/30/2005, monday(05/30/2005, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-September-28 Thu = 09/28/2006, thursday(09/28/2006, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-December-31 Fri = 12/31/2004, friday(12/31/2004, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-October-19 Tue = 10/19/2004, tuesday(10/19/2004, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2012-July-14 Sat = 07/14/2012, saturday(07/14/2012, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-November-29 Sun = 11/29/2009, sunday(11/29/2009, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-April-16 Sun = 04/16/2006, sunday(04/16/2006, sunday) 1, attention (18, 32)\n",
      "Epoch 110. Train Loss: 0.00010304227, Test Loss : 0.00013325716\n",
      "Epoch 111. Train Loss: 9.676088e-05, Test Loss : 0.00011951247\n",
      "Epoch 112. Train Loss: 9.6909775e-05, Test Loss : 0.00011590841\n",
      "Epoch 113. Train Loss: 9.3541574e-05, Test Loss : 0.0001245807\n",
      "Epoch 114. Train Loss: 8.7668435e-05, Test Loss : 0.0001108555\n",
      "Epoch 115. Train Loss: 8.470928e-05, Test Loss : 0.00010923006\n",
      "Epoch 116. Train Loss: 8.312468e-05, Test Loss : 0.000103665676\n",
      "Epoch 117. Train Loss: 8.0412836e-05, Test Loss : 0.00010242141\n",
      "Epoch 118. Train Loss: 7.9324476e-05, Test Loss : 0.000102230646\n",
      "Epoch 119. Train Loss: 7.985613e-05, Test Loss : 0.00010050098\n",
      "\u001b[92m☑\u001b[0m 2004-September-17 Fri = 09/17/2004, friday(09/17/2004, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-September-15 Fri = 09/15/2006, friday(09/15/2006, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-February-13 Fri = 02/13/2004, friday(02/13/2004, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-June-04 Sun = 06/04/2006, sunday(06/04/2006, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2011-July-18 Mon = 07/18/2011, monday(07/18/2011, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-May-04 Thu = 05/04/2006, thursday(05/04/2006, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-December-29 Fri = 12/29/2006, friday(12/29/2006, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-May-26 Tue = 05/26/2009, tuesday(05/26/2009, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-December-02 Sat = 12/02/2006, saturday(12/02/2006, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-September-08 Sat = 09/08/2007, saturday(09/08/2007, saturday) 1, attention (20, 32)\n",
      "Epoch 120. Train Loss: 7.9273574e-05, Test Loss : 0.000101960475\n",
      "Epoch 121. Train Loss: 7.618311e-05, Test Loss : 9.4168034e-05\n",
      "Epoch 122. Train Loss: 7.27502e-05, Test Loss : 9.451843e-05\n",
      "Epoch 123. Train Loss: 7.146366e-05, Test Loss : 8.998253e-05\n",
      "Epoch 124. Train Loss: 6.915475e-05, Test Loss : 8.721163e-05\n",
      "Epoch 125. Train Loss: 6.800389e-05, Test Loss : 8.458477e-05\n",
      "Epoch 126. Train Loss: 6.6300934e-05, Test Loss : 8.409526e-05\n",
      "Epoch 127. Train Loss: 6.622625e-05, Test Loss : 8.458131e-05\n",
      "Epoch 128. Train Loss: 6.35246e-05, Test Loss : 8.234825e-05\n",
      "Epoch 129. Train Loss: 6.31186e-05, Test Loss : 8.199049e-05\n",
      "\u001b[92m☑\u001b[0m 2006-June-09 Fri = 06/09/2006, friday(06/09/2006, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-October-07 Sun = 10/07/2007, sunday(10/07/2007, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2002-March-28 Thu = 03/28/2002, thursday(03/28/2002, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-June-06 Fri = 06/06/2003, friday(06/06/2003, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2001-October-11 Thu = 10/11/2001, thursday(10/11/2001, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2012-March-27 Tue = 03/27/2012, tuesday(03/27/2012, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-October-14 Tue = 10/14/2008, tuesday(10/14/2008, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-February-29 Fri = 02/29/2008, friday(02/29/2008, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-August-16 Sat = 08/16/2003, saturday(08/16/2003, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2005-May-21 Sat = 05/21/2005, saturday(05/21/2005, saturday) 1, attention (20, 32)\n",
      "Epoch 130. Train Loss: 6.269904e-05, Test Loss : 7.8628545e-05\n",
      "Epoch 131. Train Loss: 6.006429e-05, Test Loss : 7.767235e-05\n",
      "Epoch 132. Train Loss: 6.130533e-05, Test Loss : 8.168041e-05\n",
      "Epoch 133. Train Loss: 5.963735e-05, Test Loss : 7.489726e-05\n",
      "Epoch 134. Train Loss: 5.6514735e-05, Test Loss : 7.225148e-05\n",
      "Epoch 135. Train Loss: 5.526807e-05, Test Loss : 7.215584e-05\n",
      "Epoch 136. Train Loss: 5.61455e-05, Test Loss : 7.150126e-05\n",
      "Epoch 137. Train Loss: 5.415327e-05, Test Loss : 6.984774e-05\n",
      "Epoch 138. Train Loss: 5.2810774e-05, Test Loss : 6.81414e-05\n",
      "Epoch 139. Train Loss: 5.2481733e-05, Test Loss : 6.770087e-05\n",
      "\u001b[92m☑\u001b[0m 2003-June-03 Tue = 06/03/2003, tuesday(06/03/2003, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-September-11 Sat = 09/11/2004, saturday(09/11/2004, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-January-04 Fri = 01/04/2008, friday(01/04/2008, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-May-20 Tue = 05/20/2008, tuesday(05/20/2008, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-March-10 Tue = 03/10/2009, tuesday(03/10/2009, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-October-23 Mon = 10/23/2006, monday(10/23/2006, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-July-19 Sat = 07/19/2008, saturday(07/19/2008, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-November-24 Wed = 11/24/2004, wednesday(11/24/2004, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2002-January-19 Sat = 01/19/2002, saturday(01/19/2002, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-July-19 Sun = 07/19/2009, sunday(07/19/2009, sunday) 1, attention (18, 32)\n",
      "Epoch 140. Train Loss: 5.353201e-05, Test Loss : 6.786294e-05\n",
      "Epoch 141. Train Loss: 5.118094e-05, Test Loss : 6.596163e-05\n",
      "Epoch 142. Train Loss: 5.0686147e-05, Test Loss : 6.445481e-05\n",
      "Epoch 143. Train Loss: 4.9514045e-05, Test Loss : 6.357137e-05\n",
      "Epoch 144. Train Loss: 4.771497e-05, Test Loss : 6.235084e-05\n",
      "Epoch 145. Train Loss: 4.8769147e-05, Test Loss : 6.123939e-05\n",
      "Epoch 146. Train Loss: 4.7398666e-05, Test Loss : 6.0042883e-05\n",
      "Epoch 147. Train Loss: 4.6929916e-05, Test Loss : 6.0705734e-05\n",
      "Epoch 148. Train Loss: 4.6687885e-05, Test Loss : 5.8645022e-05\n",
      "Epoch 149. Train Loss: 4.58178e-05, Test Loss : 5.8074827e-05\n",
      "\u001b[92m☑\u001b[0m 2002-October-01 Tue = 10/01/2002, tuesday(10/01/2002, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-August-26 Tue = 08/26/2008, tuesday(08/26/2008, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-February-02 Mon = 02/02/2009, monday(02/02/2009, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-December-11 Thu = 12/11/2003, thursday(12/11/2003, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2011-November-27 Sun = 11/27/2011, sunday(11/27/2011, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2002-March-18 Mon = 03/18/2002, monday(03/18/2002, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-November-01 Thu = 11/01/2007, thursday(11/01/2007, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2011-October-02 Sun = 10/02/2011, sunday(10/02/2011, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-June-13 Sun = 06/13/2010, sunday(06/13/2010, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-June-04 Mon = 06/04/2007, monday(06/04/2007, monday) 1, attention (18, 32)\n",
      "Epoch 150. Train Loss: 4.538602e-05, Test Loss : 5.6955647e-05\n",
      "Epoch 151. Train Loss: 4.397441e-05, Test Loss : 5.711332e-05\n",
      "Epoch 152. Train Loss: 4.348435e-05, Test Loss : 5.621584e-05\n",
      "Epoch 153. Train Loss: 4.2982334e-05, Test Loss : 5.732665e-05\n",
      "Epoch 154. Train Loss: 4.202825e-05, Test Loss : 5.444537e-05\n",
      "Epoch 155. Train Loss: 4.220011e-05, Test Loss : 5.577446e-05\n",
      "Epoch 156. Train Loss: 4.0985207e-05, Test Loss : 5.459918e-05\n",
      "Epoch 157. Train Loss: 4.0811592e-05, Test Loss : 5.3223775e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158. Train Loss: 4.166052e-05, Test Loss : 5.307183e-05\n",
      "Epoch 159. Train Loss: 4.0207044e-05, Test Loss : 5.1515206e-05\n",
      "\u001b[92m☑\u001b[0m 2005-June-25 Sat = 06/25/2005, saturday(06/25/2005, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-April-14 Mon = 04/14/2008, monday(04/14/2008, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2012-March-21 Wed = 03/21/2012, wednesday(03/21/2012, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-July-02 Thu = 07/02/2009, thursday(07/02/2009, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2001-December-25 Tue = 12/25/2001, tuesday(12/25/2001, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-December-17 Sun = 12/17/2006, sunday(12/17/2006, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-August-05 Tue = 08/05/2003, tuesday(08/05/2003, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-February-25 Sun = 02/25/2007, sunday(02/25/2007, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-May-18 Tue = 05/18/2010, tuesday(05/18/2010, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-August-19 Tue = 08/19/2008, tuesday(08/19/2008, tuesday) 1, attention (19, 32)\n",
      "Epoch 160. Train Loss: 3.9598737e-05, Test Loss : 5.108196e-05\n",
      "Epoch 161. Train Loss: 3.8566275e-05, Test Loss : 5.090057e-05\n",
      "Epoch 162. Train Loss: 3.9267543e-05, Test Loss : 5.047022e-05\n",
      "Epoch 163. Train Loss: 3.811172e-05, Test Loss : 4.9239658e-05\n",
      "Epoch 164. Train Loss: 3.7459846e-05, Test Loss : 4.9104452e-05\n",
      "Epoch 165. Train Loss: 3.7224112e-05, Test Loss : 4.8355476e-05\n",
      "Epoch 166. Train Loss: 3.6327016e-05, Test Loss : 4.7763366e-05\n",
      "Epoch 167. Train Loss: 3.634811e-05, Test Loss : 4.7303467e-05\n",
      "Epoch 168. Train Loss: 3.5736164e-05, Test Loss : 4.6726444e-05\n",
      "Epoch 169. Train Loss: 3.6172794e-05, Test Loss : 4.6852994e-05\n",
      "\u001b[92m☑\u001b[0m 2006-August-18 Fri = 08/18/2006, friday(08/18/2006, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-August-09 Thu = 08/09/2007, thursday(08/09/2007, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-January-13 Fri = 01/13/2006, friday(01/13/2006, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-September-28 Sun = 09/28/2003, sunday(09/28/2003, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-March-13 Mon = 03/13/2006, monday(03/13/2006, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-August-22 Sun = 08/22/2004, sunday(08/22/2004, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-September-03 Mon = 09/03/2007, monday(09/03/2007, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-October-06 Tue = 10/06/2009, tuesday(10/06/2009, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-October-09 Sat = 10/09/2010, saturday(10/09/2010, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-February-21 Sun = 02/21/2010, sunday(02/21/2010, sunday) 1, attention (18, 32)\n",
      "Epoch 170. Train Loss: 3.5487494e-05, Test Loss : 4.6168825e-05\n",
      "Epoch 171. Train Loss: 3.518171e-05, Test Loss : 4.5527042e-05\n",
      "Epoch 172. Train Loss: 3.4989083e-05, Test Loss : 4.5231256e-05\n",
      "Epoch 173. Train Loss: 3.4501325e-05, Test Loss : 4.5333203e-05\n",
      "Epoch 174. Train Loss: 3.387766e-05, Test Loss : 4.4148317e-05\n",
      "Epoch 175. Train Loss: 3.3108074e-05, Test Loss : 4.3801414e-05\n",
      "Epoch 176. Train Loss: 3.3402106e-05, Test Loss : 4.293883e-05\n",
      "Epoch 177. Train Loss: 3.255265e-05, Test Loss : 4.2647607e-05\n",
      "Epoch 178. Train Loss: 3.3015687e-05, Test Loss : 4.359561e-05\n",
      "Epoch 179. Train Loss: 3.2470092e-05, Test Loss : 4.1968753e-05\n",
      "\u001b[92m☑\u001b[0m 2002-August-13 Tue = 08/13/2002, tuesday(08/13/2002, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2011-November-22 Tue = 11/22/2011, tuesday(11/22/2011, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2012-May-14 Mon = 05/14/2012, monday(05/14/2012, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2005-July-04 Mon = 07/04/2005, monday(07/04/2005, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-August-19 Sat = 08/19/2006, saturday(08/19/2006, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2002-May-30 Thu = 05/30/2002, thursday(05/30/2002, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2012-August-23 Thu = 08/23/2012, thursday(08/23/2012, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-February-03 Mon = 02/03/2003, monday(02/03/2003, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-April-24 Sat = 04/24/2010, saturday(04/24/2010, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2012-September-15 Sat = 09/15/2012, saturday(09/15/2012, saturday) 1, attention (20, 32)\n",
      "Epoch 180. Train Loss: 3.2610125e-05, Test Loss : 4.1664774e-05\n",
      "Epoch 181. Train Loss: 3.1348965e-05, Test Loss : 4.119551e-05\n",
      "Epoch 182. Train Loss: 3.0770712e-05, Test Loss : 4.103226e-05\n",
      "Epoch 183. Train Loss: 3.0869352e-05, Test Loss : 4.0826824e-05\n",
      "Epoch 184. Train Loss: 3.040446e-05, Test Loss : 4.0086503e-05\n",
      "Epoch 185. Train Loss: 3.098913e-05, Test Loss : 3.9693157e-05\n",
      "Epoch 186. Train Loss: 2.9914008e-05, Test Loss : 3.9459228e-05\n",
      "Epoch 187. Train Loss: 2.9958255e-05, Test Loss : 3.905072e-05\n",
      "Epoch 188. Train Loss: 2.9633662e-05, Test Loss : 3.893762e-05\n",
      "Epoch 189. Train Loss: 2.9252895e-05, Test Loss : 3.8179467e-05\n",
      "\u001b[92m☑\u001b[0m 2010-February-13 Sat = 02/13/2010, saturday(02/13/2010, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-August-06 Sun = 08/06/2006, sunday(08/06/2006, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-July-01 Sat = 07/01/2006, saturday(07/01/2006, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-December-21 Fri = 12/21/2007, friday(12/21/2007, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2005-April-02 Sat = 04/02/2005, saturday(04/02/2005, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-June-12 Thu = 06/12/2008, thursday(06/12/2008, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-November-12 Wed = 11/12/2003, wednesday(11/12/2003, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-September-06 Sun = 09/06/2009, sunday(09/06/2009, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-November-25 Sat = 11/25/2006, saturday(11/25/2006, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-February-05 Sun = 02/05/2006, sunday(02/05/2006, sunday) 1, attention (18, 32)\n",
      "Epoch 190. Train Loss: 2.898302e-05, Test Loss : 3.7746195e-05\n",
      "Epoch 191. Train Loss: 2.9021461e-05, Test Loss : 3.822516e-05\n",
      "Epoch 192. Train Loss: 2.881738e-05, Test Loss : 3.8428552e-05\n",
      "Epoch 193. Train Loss: 2.8388755e-05, Test Loss : 3.7376834e-05\n",
      "Epoch 194. Train Loss: 2.8178285e-05, Test Loss : 3.692883e-05\n",
      "Epoch 195. Train Loss: 2.7943539e-05, Test Loss : 3.6425772e-05\n",
      "Epoch 196. Train Loss: 2.7949205e-05, Test Loss : 3.6704856e-05\n",
      "Epoch 197. Train Loss: 2.77475e-05, Test Loss : 3.7054695e-05\n",
      "Epoch 198. Train Loss: 2.7302116e-05, Test Loss : 3.5958034e-05\n",
      "Epoch 199. Train Loss: 2.819196e-05, Test Loss : 3.6513684e-05\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "\n",
    "tot_test_loss = []\n",
    "tot_train_loss = []\n",
    "for e in range(epochs):\n",
    "    train_loss = []\n",
    "    for i, (x_data, y_data, z_data) in enumerate(tr_data_iterator):\n",
    "        x_data = x_data.as_in_context(ctx).astype('float32')\n",
    "        y_data = y_data.as_in_context(ctx).astype('float32')\n",
    "        z_data = z_data.as_in_context(ctx).astype('float32')\n",
    "        \n",
    "        with autograd.record():\n",
    "            z_output = model(x_data, y_data)\n",
    "            loss_ = loss(z_output, z_data)\n",
    "        loss_.backward()\n",
    "        trainer.step(x_data.shape[0])\n",
    "        curr_loss = nd.mean(loss_).asscalar()\n",
    "        train_loss.append(curr_loss)\n",
    "        \n",
    "    if e % 10 == 0:\n",
    "        q, y = gen_test(10)\n",
    "        for i in range(10):\n",
    "            with autograd.predict_mode():\n",
    "                p, attn = model.predict(q[i], char_indices, indices_char, input_digits = in_seq_len, lchars = len(chars))\n",
    "                p = p.strip()\n",
    "                iscorr = 1 if p == y[i] else 0\n",
    "                if iscorr == 1:\n",
    "                    print(colors.ok + '☑' + colors.close, end=' ')\n",
    "                else:\n",
    "                    print(colors.fail + '☒' + colors.close, end=' ')\n",
    "                print(\"{} = {}({}) {}, attention {}\".format(q[i], p, y[i], str(iscorr), attn.shape))\n",
    "    #caculate test loss\n",
    "    test_loss = calculate_loss(model, te_data_iterator, loss_obj = loss, ctx=ctx) \n",
    "\n",
    "    print(\"Epoch %s. Train Loss: %s, Test Loss : %s\" % (e, np.mean(train_loss), test_loss))    \n",
    "    tot_test_loss.append(test_loss)\n",
    "    tot_train_loss.append(np.mean(train_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(net, data, ctx = mx.cpu()):\n",
    "    from matplotlib import pyplot as plt\n",
    "    import seaborn as sns\n",
    "    sns.set()\n",
    "    p =[]\n",
    "    attn = []\n",
    "    for i, d  in enumerate(data):\n",
    "        _p, _attn = net.predict(d, char_indices, indices_char, input_digits = in_seq_len, lchars = len(chars))\n",
    "        p.append(_p.strip())\n",
    "        attn.append(_attn)\n",
    "\n",
    "    fig, axes = plt.subplots(np.int(np.ceil(len(data) / 1)), 1, sharex = False, sharey = False)\n",
    "    plt.subplots_adjust(hspace=1)\n",
    "\n",
    "    if len(data) > 1:\n",
    "        fig.set_size_inches(5, 40)\n",
    "    else:\n",
    "        fig.set_size_inches(10, 10)\n",
    "    plt.subplots_adjust(hspace=1)\n",
    "    \n",
    "    for i, (d, p, a) in enumerate(zip(data, p, attn)):\n",
    "        _col = list(d)\n",
    "        _idx = list(p)\n",
    "        _val = a[:len(p), :len(d)]\n",
    "        print('input: {}, length: {}'.format(d,len(d)))\n",
    "        print('prediction: {}, length:{}'.format(p,len(p)))\n",
    "        print('attention shape= {}'.format(a.shape))\n",
    "        print('check attn = {}'.format(np.sum(a, axis = 1)))\n",
    "        print('val shape= {}'.format(_val.shape))\n",
    "        if len(data) > 1:\n",
    "            sns.heatmap(pd.DataFrame(_val, index = _idx, columns = _col), ax = axes.flat[i], cmap = 'RdYlGn', linewidths = .3)\n",
    "        else:\n",
    "            sns.heatmap(pd.DataFrame(_val, index = _idx, columns = _col), cmap = 'RdYlGn', linewidths = .3)\n",
    "        #axes.flat[i].set_title('Label: {}, Pred: {}'.format(_label[i], np.int(_pred[i])))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 2009-December-25 Fri, length: 20\n",
      "prediction: 12/25/2009, friday, length:18\n",
      "attention shape= (18, 32)\n",
      "check attn = [0.99999994 1.         1.         1.         1.         1.\n",
      " 0.99999994 0.99999994 1.         1.         1.         1.\n",
      " 1.         1.         1.0000001  0.99999994 1.         1.        ]\n",
      "val shape= (18, 20)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example = [gen_date() for _ in range(1)]\n",
    "plot_attention(model, example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
