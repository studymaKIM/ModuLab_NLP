{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REF\n",
    "* https://github.com/dmlc/gluon-nlp/blob/master/docs/api/notes/data_api.rst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-04T04:17:55.562224Z",
     "start_time": "2018-07-04T04:17:54.161752Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import accuracy_score, auc\n",
    "from mxnet import gluon\n",
    "\n",
    "\n",
    "import time, re\n",
    "import multiprocessing as mp\n",
    "import itertools\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import mxnet as mx\n",
    "import spacy\n",
    "os.environ['MXNET_ENGINE_TYPE'] = 'NaiveEngine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 20\n",
    "MAX_VOCAB = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T03:20:57.338763Z",
     "start_time": "2018-07-20T03:18:07.883505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count words and build vocab...\n",
      "Prepare data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yn</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   yn  index\n",
       "0   0   3091\n",
       "1   1   3995"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "word_freq = collections.Counter()\n",
    "max_len = 0\n",
    "num_rec = 0\n",
    "print('Count words and build vocab...')\n",
    "with open('../data/umich-sentiment-train.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        _lab, _sen = line.decode('utf8').strip().split('\\t')\n",
    "        words = [token.lemma_ for token in nlp(_sen) if token.is_alpha] # Stop word제거 안한 상태 \n",
    "        # 제거를 위해 [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "        if len(words) > max_len:\n",
    "            max_len = len(words)\n",
    "        for word in words:\n",
    "            word_freq[word] += 1\n",
    "        num_rec += 1\n",
    "\n",
    "# most_common output -> list\n",
    "word2idx = {x[0]: i+2 for i, x in enumerate(word_freq.most_common(MAX_VOCAB - 2))}\n",
    "word2idx ['PAD'] = 0\n",
    "word2idx['UNK'] = 1\n",
    "\n",
    "idx2word= {i:v for v, i in word2idx.items()}\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "print('Prepare data...')\n",
    "y = []\n",
    "x = []\n",
    "origin_txt = []\n",
    "with open('../data/umich-sentiment-train.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        _label, _sen = line.decode('utf8').strip().split('\\t')\n",
    "        origin_txt.append(_sen)\n",
    "        y.append(int(_label))\n",
    "        words = [token.lemma_ for token in nlp(_sen) if token.is_alpha] # Stop word제거 안한 상태\n",
    "        words = [x for x in words if x != '-PRON-'] # '-PRON-' 제거\n",
    "        _seq = []\n",
    "        for word in words:\n",
    "            if word in word2idx.keys():\n",
    "                _seq.append(word2idx[word])\n",
    "            else:\n",
    "                _seq.append(word2idx['UNK'])\n",
    "        if len(_seq) < MAX_SENTENCE_LENGTH:\n",
    "            _seq.extend([0] * ((MAX_SENTENCE_LENGTH) - len(_seq)))\n",
    "        else:\n",
    "            _seq = _seq[:MAX_SENTENCE_LENGTH]\n",
    "        x.append(_seq)\n",
    "\n",
    "pd.DataFrame(y, columns = ['yn']).reset_index().groupby('yn').count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T04:16:52.755123Z",
     "start_time": "2018-07-20T04:16:52.645286Z"
    }
   },
   "outputs": [],
   "source": [
    "## Data process - tr/va split and define iterator\n",
    "\n",
    "tr_idx = np.random.choice(range(len(x)), int(len(x) * .8))\n",
    "va_idx = [x for x in range(len(x)) if x not in tr_idx]\n",
    "\n",
    "tr_x = [x[i] for i in tr_idx]\n",
    "tr_y = [y[i] for i in tr_idx]\n",
    "tr_origin = [origin_txt[i] for i in tr_idx]\n",
    "\n",
    "va_x = [x[i] for i in va_idx]\n",
    "va_y = [y[i] for i in va_idx]\n",
    "va_origin = [origin_txt[i] for i in va_idx]\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "learning_rate = .0002\n",
    "log_interval = 100\n",
    "\n",
    "\n",
    "train_data = mx.io.NDArrayIter(data=[tr_x, tr_y], batch_size=batch_size, shuffle = False)\n",
    "valid_data = mx.io.NDArrayIter(data=[va_x, va_y], batch_size=batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T04:16:57.940309Z",
     "start_time": "2018-07-20T04:16:57.935557Z"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon, autograd, nd\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet.ndarray.linalg import gemm2\n",
    "import mxnet as mx\n",
    "context = mx.gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:17:32.970927Z",
     "start_time": "2018-07-21T14:17:32.955877Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sentence_Representation(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Sentence_Representation, self).__init__()\n",
    "        for (k, v) in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "        with self.name_scope():\n",
    "            self.embed = nn.Embedding(self.vocab_size, self.emb_dim)\n",
    "            self.conv1 = nn.Conv2D(channels = 8, kernel_size = (3, self.emb_dim), activation = 'relu')\n",
    "            self.maxpool1 = nn.MaxPool2D(pool_size = (self.max_sentence_length -3 + 1, 1), strides = (1, 1))\n",
    "            self.conv2 = nn.Conv2D(channels = 8, kernel_size = (4, self.emb_dim), activation = 'relu')            \n",
    "            self.maxpool2 = nn.MaxPool2D(pool_size = (self.max_sentence_length -4 + 1, 1), strides = (1, 1))\n",
    "            self.conv3 = nn.Conv2D(channels = 8, kernel_size = (5, self.emb_dim), activation = 'relu')\n",
    "            self.maxpool3 = nn.MaxPool2D(pool_size = (self.max_sentence_length -5 + 1, 1), strides = (1, 1))\n",
    "            self.conv4 = nn.Conv2D(channels = 8, kernel_size = (6, self.emb_dim), activation = 'relu') \n",
    "            self.maxpool4 = nn.MaxPool2D(pool_size = (self.max_sentence_length -6 + 1, 1), strides = (1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embed(x) # batch * time step * embedding\n",
    "        embeds = embeds.expand_dims(axis = 1)\n",
    "        _x1 = self.conv1(embeds)\n",
    "        _x1 = self.maxpool1(_x1)\n",
    "        _x1 = nd.reshape(_x1, shape = (-1, 8))\n",
    "        \n",
    "        _x2 = self.conv2(embeds)\n",
    "        _x2 = self.maxpool2(_x2)\n",
    "        _x2 = nd.reshape(_x2, shape = (-1, 8))\n",
    "        \n",
    "        _x3 = self.conv3(embeds)\n",
    "        _x3 = self.maxpool3(_x3)\n",
    "        _x3 = nd.reshape(_x3, shape = (-1, 8))\n",
    "        \n",
    "        _x4 = self.conv4(embeds)\n",
    "        _x4 = self.maxpool4(_x4)\n",
    "        _x4 = nd.reshape(_x4, shape = (-1, 8))\n",
    "\n",
    "        _x = nd.concat(_x1, _x2, _x3, _x4)\n",
    "        return _x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nn.Sequential()\n",
    "classifier.add(nn.Dense(16, activation = 'relu'))\n",
    "classifier.add(nn.Dense(8, activation = 'relu'))\n",
    "classifier.add(nn.Dense(1))\n",
    "classifier.collect_params().initialize(mx.init.Xavier(), ctx = context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment analysis classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:17:35.867152Z",
     "start_time": "2018-07-21T14:17:35.858191Z"
    }
   },
   "outputs": [],
   "source": [
    "class SA_CNN_Classifier(nn.Block):\n",
    "    def __init__(self, sen_rep, classifier, context, **kwargs):\n",
    "        super(SA_CNN_Classifier, self).__init__(**kwargs)\n",
    "        self.context = context\n",
    "        with self.name_scope():\n",
    "            self.sen_rep = sen_rep\n",
    "            self.classifier = classifier\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # sentence representation할 때 hidden의 context가 cpu여서 오류 발생. context를 gpu로 전환\n",
    "        x = self.sen_rep(x)\n",
    "        res = self.classifier(x)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate sentiment classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:17:38.550227Z",
     "start_time": "2018-07-21T14:17:38.536795Z"
    }
   },
   "outputs": [],
   "source": [
    "emb_dim = 50 # Emb dim\n",
    "param = {'emb_dim': emb_dim, 'vocab_size': vocab_size, 'max_sentence_length': MAX_SENTENCE_LENGTH, 'dropout': .2}\n",
    "sen_rep = Sentence_Representation(**param)\n",
    "sen_rep.collect_params().initialize(mx.init.Xavier(), ctx = context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:17:39.912316Z",
     "start_time": "2018-07-21T14:17:39.906486Z"
    }
   },
   "outputs": [],
   "source": [
    "sa = SA_CNN_Classifier(sen_rep, classifier, context)\n",
    "loss = gluon.loss.SigmoidBCELoss()\n",
    "trainer = gluon.Trainer(sa.collect_params(), 'adam', {'learning_rate': 1e-3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:17:41.166470Z",
     "start_time": "2018-07-21T14:17:41.155606Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(net, dataIterator, context):\n",
    "    dataIterator.reset()\n",
    "    loss = gluon.loss.SigmoidBCELoss()\n",
    "    total_L = 0.0\n",
    "    total_sample_num = 0\n",
    "    total_correct_num = 0\n",
    "    start_log_interval_time = time.time()\n",
    "    for i, batch in enumerate(dataIterator):\n",
    "        data =  batch.data[0].as_in_context(context)\n",
    "        label = batch.data[1].as_in_context(context)\n",
    "        output = net(data)\n",
    "        L = loss(output, label)\n",
    "        pred = (output > 0.5).reshape((-1,))\n",
    "        total_L += L.sum().asscalar()\n",
    "        total_sample_num += len(label)\n",
    "        total_correct_num += (pred == label).sum().asscalar()\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            print('[Batch {}/{}] elapsed {:.2f} s'.format(\n",
    "                i + 1, dataIterator.num_data//dataIterator.batch_size,\n",
    "                time.time() - start_log_interval_time))\n",
    "            start_log_interval_time = time.time()\n",
    "    avg_L = total_L / float(total_sample_num)\n",
    "    acc = total_correct_num / float(total_sample_num)\n",
    "    return avg_L, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:36:02.670468Z",
     "start_time": "2018-07-21T14:35:18.885713Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0659e504ddb423991f5158efc4be8e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch', max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 100/354] elapsed 0.88 s,                     avg loss 0.027326, throughput 1.82K wps\n",
      "[Epoch 0 Batch 200/354] elapsed 0.81 s,                     avg loss 0.003084, throughput 1.97K wps\n",
      "[Epoch 0 Batch 300/354] elapsed 0.81 s,                     avg loss 0.001511, throughput 1.98K wps\n",
      "[Batch 100/199] elapsed 0.37 s\n",
      "[Epoch 0] train avg loss 0.009157, valid acc 0.99,         valid avg loss 0.016872, throughput 1.93K wps\n",
      "[Epoch 1 Batch 100/354] elapsed 0.81 s,                     avg loss 0.000595, throughput 1.97K wps\n",
      "[Epoch 1 Batch 200/354] elapsed 0.81 s,                     avg loss 0.000603, throughput 1.98K wps\n",
      "[Epoch 1 Batch 300/354] elapsed 0.81 s,                     avg loss 0.000287, throughput 1.97K wps\n",
      "[Batch 100/199] elapsed 0.37 s\n",
      "[Epoch 1] train avg loss 0.000450, valid acc 0.99,         valid avg loss 0.014424, throughput 1.97K wps\n",
      "[Epoch 2 Batch 100/354] elapsed 0.81 s,                     avg loss 0.000188, throughput 1.97K wps\n",
      "[Epoch 2 Batch 200/354] elapsed 0.81 s,                     avg loss 0.000439, throughput 1.97K wps\n",
      "[Epoch 2 Batch 300/354] elapsed 0.81 s,                     avg loss 0.000063, throughput 1.98K wps\n",
      "[Batch 100/199] elapsed 0.37 s\n",
      "[Epoch 2] train avg loss 0.000202, valid acc 0.99,         valid avg loss 0.014601, throughput 1.97K wps\n",
      "[Epoch 3 Batch 100/354] elapsed 0.81 s,                     avg loss 0.000138, throughput 1.98K wps\n",
      "[Epoch 3 Batch 200/354] elapsed 0.81 s,                     avg loss 0.000343, throughput 1.98K wps\n",
      "[Epoch 3 Batch 300/354] elapsed 0.81 s,                     avg loss 0.000044, throughput 1.98K wps\n",
      "[Batch 100/199] elapsed 0.37 s\n",
      "[Epoch 3] train avg loss 0.000152, valid acc 0.99,         valid avg loss 0.015471, throughput 1.97K wps\n",
      "[Epoch 4 Batch 100/354] elapsed 0.81 s,                     avg loss 0.000126, throughput 1.98K wps\n",
      "[Epoch 4 Batch 200/354] elapsed 0.81 s,                     avg loss 0.000204, throughput 1.97K wps\n",
      "[Epoch 4 Batch 300/354] elapsed 0.81 s,                     avg loss 0.000016, throughput 1.98K wps\n",
      "[Batch 100/199] elapsed 0.37 s\n",
      "[Epoch 4] train avg loss 0.000100, valid acc 0.99,         valid avg loss 0.015666, throughput 1.97K wps\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 5\n",
    "for epoch in tqdm_notebook(range(n_epoch), desc = 'epoch'):\n",
    "    ## Training\n",
    "    train_data.reset()\n",
    "    # Epoch training stats\n",
    "    start_epoch_time = time.time()\n",
    "    epoch_L = 0.0\n",
    "    epoch_sent_num = 0\n",
    "    epoch_wc = 0\n",
    "    # Log interval training stats\n",
    "    start_log_interval_time = time.time()\n",
    "    log_interval_wc = 0\n",
    "    log_interval_sent_num = 0\n",
    "    log_interval_L = 0.0\n",
    "    \n",
    "    for i, batch in enumerate(train_data):\n",
    "        _data = batch.data[0].as_in_context(context)\n",
    "        _label = batch.data[1].as_in_context(context)\n",
    "        L = 0\n",
    "        wc = len(_data)\n",
    "        log_interval_wc += wc\n",
    "        epoch_wc += wc\n",
    "        log_interval_sent_num += _data.shape[1]\n",
    "        epoch_sent_num += _data.shape[1]\n",
    "        with autograd.record():\n",
    "            _out = sa(_data)\n",
    "            L = L + loss(_out, _label).mean().as_in_context(context)\n",
    "        L.backward()\n",
    "        trainer.step(_data.shape[0])\n",
    "        log_interval_L += L.asscalar()\n",
    "        epoch_L += L.asscalar()\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            tqdm.write('[Epoch {} Batch {}/{}] elapsed {:.2f} s, \\\n",
    "                    avg loss {:.6f}, throughput {:.2f}K wps'.format(\n",
    "                    epoch, i + 1, train_data.num_data//train_data.batch_size,\n",
    "                    time.time() - start_log_interval_time,\n",
    "                    log_interval_L / log_interval_sent_num,\n",
    "                    log_interval_wc / 1000 / (time.time() - start_log_interval_time)))\n",
    "            # Clear log interval training stats\n",
    "            start_log_interval_time = time.time()\n",
    "            log_interval_wc = 0\n",
    "            log_interval_sent_num = 0\n",
    "            log_interval_L = 0\n",
    "    end_epoch_time = time.time()\n",
    "    test_avg_L, test_acc = evaluate(sa, valid_data, context)\n",
    "    tqdm.write('[Epoch {}] train avg loss {:.6f}, valid acc {:.2f}, \\\n",
    "        valid avg loss {:.6f}, throughput {:.2f}K wps'.format(\n",
    "        epoch, epoch_L / epoch_sent_num,\n",
    "        test_acc, test_avg_L, epoch_wc / 1000 /\n",
    "        (end_epoch_time - start_epoch_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:30:03.714909Z",
     "start_time": "2018-07-21T14:30:03.704239Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_pred(net, iterator):\n",
    "    pred_sa = []\n",
    "    label_sa = []\n",
    "    va_text = []\n",
    "    iterator.reset()\n",
    "    for i, batch in enumerate(iterator):\n",
    "        if i % 100 == 0:\n",
    "            print('i = {}'.format(i))\n",
    "        data =  batch.data[0].as_in_context(context)\n",
    "        label = batch.data[1].as_in_context(context)\n",
    "        output = net(data)\n",
    "        L = loss(output, label)\n",
    "        pred = (nd.sigmoid(output) > 0.5).reshape((-1,))\n",
    "        pred_sa.extend(pred.asnumpy())\n",
    "        label_sa.extend(label.asnumpy())\n",
    "        va_text.extend([' '.join([idx2word[np.int(x)] for x in y.asnumpy() if idx2word[np.int(x)] is not 'PAD']) for y in data])\n",
    "    pred_sa_pd = pd.DataFrame(pred_sa, columns  = ['pred_sa'])\n",
    "    label_pd = pd.DataFrame(label_sa, columns = ['label'])\n",
    "    text_pd = pd.DataFrame(va_text, columns = ['text'])\n",
    "    res = pd.concat([text_pd, pred_sa_pd, label_pd], axis = 1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:30:17.312238Z",
     "start_time": "2018-07-21T14:30:06.907985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n",
      "i = 100\n"
     ]
    }
   ],
   "source": [
    "result = get_pred(sa, valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:30:27.564873Z",
     "start_time": "2018-07-21T14:30:27.555956Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[result.pred_sa != result.label].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:30:43.777958Z",
     "start_time": "2018-07-21T14:30:43.764692Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pred_sa</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>that not even an exaggeration and at midnight ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>the da vinci code backtory on various religiou...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>like mission impossible but hate tom cruise ge...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>harry potter and the philosopher stone rowling...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>be plan so no biggie be admire sister harry po...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>this be undoubtedly a big deal as do not purch...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>write a harry potter poem for a chance to win ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>enjoy discuss harry potter and know there be s...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>harry potter and the veil of darkness by ocean...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>enjoy take harry potter quiz</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  pred_sa  label\n",
       "1    that not even an exaggeration and at midnight ...      0.0    1.0\n",
       "14   the da vinci code backtory on various religiou...      0.0    1.0\n",
       "566  like mission impossible but hate tom cruise ge...      0.0    1.0\n",
       "914  harry potter and the philosopher stone rowling...      0.0    1.0\n",
       "921  be plan so no biggie be admire sister harry po...      0.0    1.0\n",
       "924  this be undoubtedly a big deal as do not purch...      0.0    1.0\n",
       "937  write a harry potter poem for a chance to win ...      0.0    1.0\n",
       "948  enjoy discuss harry potter and know there be s...      0.0    1.0\n",
       "978  harry potter and the veil of darkness by ocean...      0.0    1.0\n",
       "981                       enjoy take harry potter quiz      0.0    1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[result.pred_sa != result.label].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that not even an exaggeration and at midnight go to wal mart to buy the da vinci code which be --- Label:1.0\n",
      "the da vinci code backtory on various religious historical figure and such be interesting at time but more of scifi --- Label:1.0\n",
      "like mission impossible but hate tom cruise get that straight update day in a row like magic and shit --- Label:1.0\n",
      "harry potter and the philosopher stone rowling strangely a fan of hp fanfic but not of the book --- Label:1.0\n",
      "be plan so no biggie be admire sister harry potter collection while wait --- Label:1.0\n",
      "this be undoubtedly a big deal as do not purchase movie a la dvd the only one own be series --- Label:1.0\n",
      "write a harry potter poem for a chance to win a fabulous harry potter prize --- Label:1.0\n",
      "enjoy discuss harry potter and know there be still a lot for to learn about --- Label:1.0\n",
      "harry potter and the veil of darkness by oceans phoenix word total and count year fic actually look fairly interesting --- Label:1.0\n",
      "enjoy take harry potter quiz --- Label:1.0\n",
      "harry potter the goblet of fire be good folow on from the other movie --- Label:1.0\n",
      "idk why but harry potter icon be always the funniest --- Label:1.0\n",
      "be think should have a gay cowboy party in honor of val star performance in titus and because love brokeback --- Label:1.0\n",
      "that like ask if brokeback mountain be a good rep of cowboy --- Label:1.0\n",
      "yesterday an asian tourist get off one of horse and hug one of the wrangler and tell love brokeback mountain --- Label:1.0\n",
      "mission impossible three be useless --- Label:0.0\n",
      "harry potter club be unbelievably obnoxious tonight when only believably obnoxious on any other day --- Label:0.0\n",
      "judge by the harry potter craze and the money that have be generate by the book and movie no one --- Label:0.0\n",
      "after the festivity madre and see brokeback mountain which be great yet depress all at once --- Label:0.0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d0a97c580e16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwrong\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_sa\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} --- Label:{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrong\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrong\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/venv/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1477\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1478\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1480\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2101\u001b[0m             \u001b[0;31m# validate the location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2102\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2104\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2007\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2009\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2011\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "wrong = result[result.pred_sa != result.label]\n",
    "for i in range(20):\n",
    "    print('{} --- Label:{}'.format(wrong['text'].iloc[i], wrong['label'].iloc[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
