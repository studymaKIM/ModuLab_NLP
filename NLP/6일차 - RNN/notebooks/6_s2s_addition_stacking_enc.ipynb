{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from mxnet import nd, autograd, gluon\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import nn, rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def n(digits =3):\n",
    "    number = ''\n",
    "    for i in range(np.random.randint(1, digits + 1)):\n",
    "        number += np.random.choice(list('0123456789'))\n",
    "    return int(number)\n",
    "\n",
    "def padding(chars, maxlen):\n",
    "    return chars + ' ' * (maxlen - len(chars))\n",
    "\n",
    "N = 50000\n",
    "N_train = int(N * .9)\n",
    "N_validation = N - N_train\n",
    "\n",
    "digits = 3\n",
    "input_digits = digits * 2 + 3\n",
    "output_digits = digits + 3\n",
    "\n",
    "added = set()\n",
    "questions = []\n",
    "answers = []\n",
    "answers_y = []\n",
    "\n",
    "while len(questions) < N:\n",
    "    a, b = n(), n()\n",
    "    pair = tuple(sorted((a, b)))\n",
    "    if pair in added:\n",
    "        continue\n",
    "        \n",
    "    question = 'S{}+{}E'.format(a, b)\n",
    "    question = padding(question, input_digits)\n",
    "    answer = 'S' + str(a + b) + 'E'\n",
    "    answer = padding(answer, output_digits)\n",
    "    answer_y = str(a + b) + 'E'\n",
    "    answer_y = padding(answer_y, output_digits)\n",
    "    \n",
    "    added.add(pair)\n",
    "    questions.append(question)\n",
    "    answers.append(answer)\n",
    "    answers_y.append(answer_y)\n",
    "    \n",
    "chars = '0123456789+SE '\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "X = np.zeros((len(questions), input_digits, len(chars)), dtype=np.integer)\n",
    "Y = np.zeros((len(questions), digits + 3, len(chars)), dtype=np.integer)\n",
    "Z = np.zeros((len(questions), digits + 3, len(chars)), dtype=np.integer)\n",
    "\n",
    "for i in range(N):\n",
    "    for t, char in enumerate(questions[i]):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    for t, char in enumerate(answers[i]):\n",
    "        Y[i, t, char_indices[char]] = 1\n",
    "    for t, char in enumerate(answers_y[i]):\n",
    "        Z[i, t, char_indices[char]] = 1\n",
    "    \n",
    "X_train, X_validation, Y_train, Y_validation, Z_train, Z_validation = \\\n",
    "    train_test_split(X, Y, Z, train_size=N_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_n_test(N):\n",
    "    q = []\n",
    "    y = []\n",
    "    for i in range(N):\n",
    "        a, b = n(), n() \n",
    "        question = '{}+{}'.format(a, b)\n",
    "        answer_y = str(a + b)\n",
    "        q.append(question)\n",
    "        y.append(answer_y)\n",
    "    return(q,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM vs LSTMCell\n",
    "\n",
    "  * LSTM과 LSTMCell은 서로 다른 형태의 parameter를 지님\n",
    "  * LSTMCell은 1번의 time step을 도는 것을 가정, LSTM은 주어진 timestep 모두를 도는 것으로 가정\n",
    "  \n",
    "  * LSTM의 state parameter는 마지막 time step의 state로 hidden state와 cell state에 해당하는 2개의 (num_layer, batch_size, n_hidden) 행렬로 구성된 list임 \n",
    "  * LSTMCell의 state parameter는 hidden state와 cell state에 해당하는 2개의 (batch_size, n_hidden) 행렬로 구성된 list임\n",
    "  * LSTM의 output은 time step만큼의 hidden state값을 모두 포함, LSTMCell의 output은 1 time step만큼의 hidden state값만 포함\n",
    "  * LSTMCell의 unroll을 쓰면, T 시점만큼 LSTM Cell을 반복적으로 계산한후 output은 각 time step의 (hidden state, cell state)를 state는 마지막 time step의 (hidden state, cell state)를 돌려줌\n",
    "  * Encoder에 LSTM을 쓰고, Decoder에 LSTMCell을 쓰는 경우에는 state parameter에 유의해야 함\n",
    "      * Encoder에 LSTM을 쓰는 이유는 stacking을 위해서이고, Decoder에 LSTMCell을 쓰는 이유는 generation을 위해서임.\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class calculator(gluon.Block):\n",
    "    def __init__(self, n_hidden, in_seq_len, out_seq_len, vocab_size, enc_layer, dec_layer = 1, **kwargs):\n",
    "        super(calculator, self).__init__(**kwargs)\n",
    "        self.in_seq_len = in_seq_len\n",
    "        self.out_seq_len = out_seq_len\n",
    "        self.n_hidden = n_hidden\n",
    "        self.vocab_size = vocab_size\n",
    "        self.enc_layer = enc_layer\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.encoder = rnn.LSTM(hidden_size = n_hidden, num_layers = enc_layer, layout = 'NTC')\n",
    "            self.decoder = rnn.LSTMCell(hidden_size = n_hidden)\n",
    "            self.batchnorm = nn.BatchNorm(axis = 2)\n",
    "            self.dense = nn.Dense(self.vocab_size, flatten = False)\n",
    "            \n",
    "    def forward(self, inputs, outputs):\n",
    "        # API says: num_layers, batch_size, num_hidden\n",
    "        self.batch_size = inputs.shape[0]\n",
    "        begin_state = self.encoder.begin_state(batch_size = self.batch_size, ctx = ctx)\n",
    "        enout, (h, c) = self.encoder(inputs, begin_state) # h, c: n_layer * batch_size * n_hidden\n",
    "        # Pick the hidden states and cell states at the last time step in the second layer\n",
    "        next_h = h[1] # batch_size * n_hidden\n",
    "        next_c = c[1] # batch_size * n_hidden\n",
    "        #next_h = nd.mean(h, axis = 0) #: Does not work\n",
    "        #next_c = nd.mean(c, axis = 0) # Does not work\n",
    "        for i in range(self.out_seq_len):\n",
    "            deout, (next_h, next_c) = self.decoder(outputs[:, i, :], [next_h, next_c],)\n",
    "            if i == 0:\n",
    "                deouts = deout\n",
    "            else:\n",
    "                deouts = nd.concat(deouts, deout, dim = 1)\n",
    "        \n",
    "        deouts = nd.reshape(deouts, (-1, self.out_seq_len, self.n_hidden))\n",
    "        deouts = self.batchnorm(deouts)\n",
    "        deouts_fc = self.dense(deouts)\n",
    "        return deouts_fc\n",
    "    \n",
    "    def calculation(self, input_str, char_indices, indices_char, input_digits = 9, lchars = 14, ctx = mx.gpu()):\n",
    "        input_str = 'S' + input_str + 'E'\n",
    "        X = nd.zeros((1, input_digits, lchars), ctx = ctx)\n",
    "        for t, char in enumerate(input_str):\n",
    "            X[0, t, char_indices[char]] = 1\n",
    "        Y_init = nd.zeros((1, lchars), ctx = ctx)\n",
    "        Y_init[0, char_indices['S']] = 1\n",
    "        begin_state = self.encoder.begin_state(batch_size = 1, ctx = ctx)\n",
    "        enout, (h, c) = self.encoder(X, begin_state)\n",
    "        next_h = h[1]\n",
    "        next_c = c[1]\n",
    "        deout = Y_init\n",
    "        \n",
    "        for i in range(self.out_seq_len):\n",
    "            deout, (next_h, next_c) = self.decoder(deout, [next_h, next_c])\n",
    "            deout = nd.expand_dims(deout, axis = 1)\n",
    "            deout = self.batchnorm(deout)\n",
    "            deout = deout[:, 0, :]\n",
    "\n",
    "            deout_sm = self.dense(deout)\n",
    "            deout = nd.one_hot(nd.argmax(nd.softmax(deout_sm, axis = 1), axis = 1), depth = self.vocab_size)\n",
    "            if i == 0:\n",
    "                ret_seq = indices_char[nd.argmax(deout_sm, axis = 1).asnumpy()[0].astype('int')]\n",
    "            else:\n",
    "                ret_seq += indices_char[nd.argmax(deout_sm, axis = 1).asnumpy()[0].astype('int')]\n",
    "\n",
    "            if ret_seq[-1] == ' ' or ret_seq[-1] == 'E':\n",
    "                break\n",
    "        return ret_seq.strip('E').strip()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_set = gluon.data.ArrayDataset(X_train, Y_train, Z_train)\n",
    "tr_data_iterator = gluon.data.DataLoader(tr_set, batch_size=256, shuffle=True)\n",
    "\n",
    "te_set =gluon.data.ArrayDataset(X_validation, Y_validation, Z_validation)\n",
    "te_data_iterator = gluon.data.DataLoader(te_set, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu()\n",
    "model = calculator(300, 9, 6, 14, 2)\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx = ctx)\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), 'rmsprop')\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss(axis = 2, sparse_label = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculator(\n",
      "  (encoder): LSTM(None -> 300, NTC, num_layers=2)\n",
      "  (decoder): LSTMCell(None -> 1200)\n",
      "  (batchnorm): BatchNorm(axis=2, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "  (dense): Dense(None -> 14, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(model, data_iter, loss_obj, ctx = ctx):\n",
    "    test_loss = []\n",
    "    for i, (x_data, y_data, z_data) in enumerate(data_iter):\n",
    "        x_data = x_data.as_in_context(ctx).astype('float32')\n",
    "        y_data = y_data.as_in_context(ctx).astype('float32')\n",
    "        z_data = z_data.as_in_context(ctx).astype('float32')\n",
    "        with autograd.predict_mode():\n",
    "            z_output = model(x_data, y_data)\n",
    "            loss_te = loss_obj(z_output, z_data)\n",
    "        curr_loss = nd.mean(loss_te).asscalar()\n",
    "        test_loss.append(curr_loss)\n",
    "    return np.mean(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m☒\u001b[0m 3+378 = 1000(381) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 2+2 = 220(4) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 52+80 = 1000(132) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 10+0 = 100(10) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 314+34 = 100(348) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 211+9 = 100(220) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 9+507 = 1000(516) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 58+656 = 665(714) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 6+511 = 610(517) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 1+981 = 1000(982) 1/0 0\n",
      "Epoch 0. Train Loss: 1.1929061, Test Loss : 1.1478088\n",
      "Epoch 1. Train Loss: 1.1250992, Test Loss : 1.1116891\n",
      "Epoch 2. Train Loss: 1.1033903, Test Loss : 1.0882914\n",
      "Epoch 3. Train Loss: 1.0533277, Test Loss : 1.1154244\n",
      "Epoch 4. Train Loss: 0.9389276, Test Loss : 0.89138335\n",
      "Epoch 5. Train Loss: 0.8449762, Test Loss : 0.8335762\n",
      "Epoch 6. Train Loss: 0.76102006, Test Loss : 0.7228872\n",
      "Epoch 7. Train Loss: 0.66424537, Test Loss : 0.64262503\n",
      "Epoch 8. Train Loss: 0.57336044, Test Loss : 0.5441426\n",
      "Epoch 9. Train Loss: 0.4706582, Test Loss : 0.39923835\n",
      "\u001b[91m☒\u001b[0m 39+90 = 139(129) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 303+62 = 365(365) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 26+129 = 255(155) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 687+515 = 1212(1202) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 505+4 = 519(509) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 555+334 = 899(889) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 9+9 = 186(18) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 51+226 = 288(277) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 2+6 = 8(8) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 3+18 = 102(21) 1/0 0\n",
      "Epoch 10. Train Loss: 0.32145813, Test Loss : 0.276249\n",
      "Epoch 11. Train Loss: 0.2105019, Test Loss : 0.25283378\n",
      "Epoch 12. Train Loss: 0.14802597, Test Loss : 0.09182515\n",
      "Epoch 13. Train Loss: 0.11160381, Test Loss : 0.072158515\n",
      "Epoch 14. Train Loss: 0.08673496, Test Loss : 0.056795366\n",
      "Epoch 15. Train Loss: 0.07351399, Test Loss : 0.119547926\n",
      "Epoch 16. Train Loss: 0.059506215, Test Loss : 0.03199128\n",
      "Epoch 17. Train Loss: 0.053088177, Test Loss : 0.031351812\n",
      "Epoch 18. Train Loss: 0.04454949, Test Loss : 0.02808522\n",
      "Epoch 19. Train Loss: 0.040302828, Test Loss : 0.02191589\n",
      "\u001b[92m☑\u001b[0m 909+4 = 913(913) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 6+12 = 18(18) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 695+471 = 1166(1166) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 78+318 = 396(396) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 81+6 = 97(87) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 0+6 = 74(6) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 296+27 = 323(323) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 451+372 = 823(823) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 0+2 = 22(2) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 977+74 = 1051(1051) 1/0 1\n",
      "Epoch 20. Train Loss: 0.036595467, Test Loss : 0.02646999\n",
      "Epoch 21. Train Loss: 0.028040845, Test Loss : 0.02081558\n",
      "Epoch 22. Train Loss: 0.028547974, Test Loss : 0.01504544\n",
      "Epoch 23. Train Loss: 0.025004786, Test Loss : 0.014161262\n",
      "Epoch 24. Train Loss: 0.0244262, Test Loss : 0.1096895\n",
      "Epoch 25. Train Loss: 0.021220375, Test Loss : 0.020165624\n",
      "Epoch 26. Train Loss: 0.020838596, Test Loss : 0.043339424\n",
      "Epoch 27. Train Loss: 0.021814404, Test Loss : 0.01304809\n",
      "Epoch 28. Train Loss: 0.0155213475, Test Loss : 0.19518097\n",
      "Epoch 29. Train Loss: 0.01672448, Test Loss : 0.012790201\n",
      "\u001b[92m☑\u001b[0m 6+182 = 188(188) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 39+2 = 51(41) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 7+6 = 14(13) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 8+4 = 12(12) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 82+2 = 94(84) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 6+32 = 38(38) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 840+334 = 1174(1174) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 2+3 = 5(5) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 0+832 = 832(832) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 99+140 = 249(239) 1/0 0\n",
      "Epoch 30. Train Loss: 0.015793722, Test Loss : 0.21434388\n",
      "Epoch 31. Train Loss: 0.014172789, Test Loss : 0.012372465\n",
      "Epoch 32. Train Loss: 0.010743793, Test Loss : 0.011105227\n",
      "Epoch 33. Train Loss: 0.013121113, Test Loss : 0.006957958\n",
      "Epoch 34. Train Loss: 0.011146569, Test Loss : 0.018491134\n",
      "Epoch 35. Train Loss: 0.010509438, Test Loss : 0.009831254\n",
      "Epoch 36. Train Loss: 0.010773481, Test Loss : 0.005975665\n",
      "Epoch 37. Train Loss: 0.0046477537, Test Loss : 0.011084614\n",
      "Epoch 38. Train Loss: 0.00877685, Test Loss : 0.0101273805\n",
      "Epoch 39. Train Loss: 0.009947606, Test Loss : 0.04502961\n",
      "\u001b[92m☑\u001b[0m 1+20 = 21(21) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 43+567 = 610(610) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 471+98 = 569(569) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 54+188 = 242(242) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 46+7 = 53(53) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 2+2 = 4(4) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 4+498 = 502(502) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 46+3 = 49(49) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 84+2 = 96(86) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 128+50 = 178(178) 1/0 1\n",
      "Epoch 40. Train Loss: 0.010341442, Test Loss : 0.009095502\n",
      "Epoch 41. Train Loss: 0.0075376807, Test Loss : 0.0077838087\n",
      "Epoch 42. Train Loss: 0.008151436, Test Loss : 0.006619009\n",
      "Epoch 43. Train Loss: 0.006737669, Test Loss : 0.0093343435\n",
      "Epoch 44. Train Loss: 0.0052585127, Test Loss : 0.0051579713\n",
      "Epoch 45. Train Loss: 0.0042783236, Test Loss : 0.007714069\n",
      "Epoch 46. Train Loss: 0.008753309, Test Loss : 0.006618255\n",
      "Epoch 47. Train Loss: 0.0046313484, Test Loss : 0.012012977\n",
      "Epoch 48. Train Loss: 0.0028676607, Test Loss : 0.010412066\n",
      "Epoch 49. Train Loss: 0.005707996, Test Loss : 0.017250929\n",
      "\u001b[92m☑\u001b[0m 279+99 = 378(378) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 0+882 = 882(882) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 5+2 = 3(7) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 8+957 = 965(965) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 440+350 = 790(790) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 34+4 = 38(38) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 90+65 = 155(155) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 1+4 = 4(5) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 751+1 = 752(752) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 55+3 = 68(58) 1/0 0\n",
      "Epoch 50. Train Loss: 0.0072036516, Test Loss : 0.0049106507\n",
      "Epoch 51. Train Loss: 0.0060872156, Test Loss : 0.006685057\n",
      "Epoch 52. Train Loss: 0.006469795, Test Loss : 0.006213614\n",
      "Epoch 53. Train Loss: 0.0059027197, Test Loss : 0.006792389\n",
      "Epoch 54. Train Loss: 0.0028080647, Test Loss : 0.015705023\n",
      "Epoch 55. Train Loss: 0.003902568, Test Loss : 0.011874745\n",
      "Epoch 56. Train Loss: 0.0048969593, Test Loss : 0.00832155\n",
      "Epoch 57. Train Loss: 0.0031117357, Test Loss : 0.007344426\n",
      "Epoch 58. Train Loss: 0.0039035887, Test Loss : 0.0045780404\n",
      "Epoch 59. Train Loss: 0.006253752, Test Loss : 0.0038328543\n",
      "\u001b[92m☑\u001b[0m 308+71 = 379(379) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 6+44 = 50(50) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 353+10 = 363(363) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 8+37 = 47(45) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 9+20 = 299(29) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 761+4 = 765(765) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 787+94 = 881(881) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 151+2 = 153(153) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 26+66 = 92(92) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 531+532 = 1063(1063) 1/0 1\n",
      "Epoch 60. Train Loss: 0.0036515668, Test Loss : 0.0065562027\n",
      "Epoch 61. Train Loss: 0.003279348, Test Loss : 0.004941901\n",
      "Epoch 62. Train Loss: 0.0027920313, Test Loss : 0.014271423\n",
      "Epoch 63. Train Loss: 0.0019510539, Test Loss : 0.05507656\n",
      "Epoch 64. Train Loss: 0.0033350594, Test Loss : 0.003584051\n",
      "Epoch 65. Train Loss: 0.0040388587, Test Loss : 0.013361459\n",
      "Epoch 66. Train Loss: 0.0031414775, Test Loss : 0.008697226\n",
      "Epoch 67. Train Loss: 0.0023565607, Test Loss : 0.004154808\n",
      "Epoch 68. Train Loss: 0.0021082768, Test Loss : 0.0039232373\n",
      "Epoch 69. Train Loss: 0.0003994629, Test Loss : 0.002924075\n",
      "\u001b[91m☒\u001b[0m 2+8 = 90(10) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 301+1 = 302(302) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 684+42 = 726(726) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 7+7 = 14(14) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 5+18 = 22(23) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 760+107 = 867(867) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 3+5 = 7(8) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 73+93 = 166(166) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 747+952 = 1699(1699) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 12+926 = 938(938) 1/0 1\n",
      "Epoch 70. Train Loss: 0.0011960118, Test Loss : 0.003641719\n",
      "Epoch 71. Train Loss: 0.0018305848, Test Loss : 0.0064855814\n",
      "Epoch 72. Train Loss: 0.0017795955, Test Loss : 0.008605691\n",
      "Epoch 73. Train Loss: 0.0036349795, Test Loss : 0.0050041177\n",
      "Epoch 74. Train Loss: 0.00036342486, Test Loss : 0.006363895\n",
      "Epoch 75. Train Loss: 0.004402109, Test Loss : 0.0030145044\n",
      "Epoch 76. Train Loss: 0.00084292336, Test Loss : 0.005423364\n",
      "Epoch 77. Train Loss: 0.0006365008, Test Loss : 0.0041284245\n",
      "Epoch 78. Train Loss: 0.002232364, Test Loss : 0.0074820346\n",
      "Epoch 79. Train Loss: 0.0046750517, Test Loss : 0.00634712\n",
      "\u001b[92m☑\u001b[0m 813+115 = 928(928) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 19+71 = 80(90) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 11+1 = 12(12) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 46+39 = 85(85) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 430+22 = 452(452) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 102+67 = 169(169) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 65+3 = 79(68) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 63+1 = 64(64) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 55+601 = 656(656) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 1+230 = 231(231) 1/0 1\n",
      "Epoch 80. Train Loss: 6.633228e-05, Test Loss : 0.002109668\n",
      "Epoch 81. Train Loss: 2.466824e-05, Test Loss : 0.0022275476\n",
      "Epoch 82. Train Loss: 1.8668688e-05, Test Loss : 0.0019989377\n",
      "Epoch 83. Train Loss: 1.6374126e-05, Test Loss : 0.0020452566\n",
      "Epoch 84. Train Loss: 1.438946e-05, Test Loss : 0.0019398484\n",
      "Epoch 85. Train Loss: 1.3268003e-05, Test Loss : 0.0018961336\n",
      "Epoch 86. Train Loss: 1.20229615e-05, Test Loss : 0.001868058\n",
      "Epoch 87. Train Loss: 1.1224011e-05, Test Loss : 0.0018421864\n",
      "Epoch 88. Train Loss: 1.0574315e-05, Test Loss : 0.0018118105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89. Train Loss: 1.0144741e-05, Test Loss : 0.0018088237\n",
      "\u001b[92m☑\u001b[0m 4+27 = 31(31) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 9+3 = 12(12) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 8+2 = 11(10) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 7+12 = 29(19) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 267+5 = 272(272) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 5+4 = 99(9) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 6+5 = 110(11) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 296+0 = 296(296) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 9+299 = 308(308) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 9+847 = 856(856) 1/0 1\n",
      "Epoch 90. Train Loss: 9.426972e-06, Test Loss : 0.0018367034\n",
      "Epoch 91. Train Loss: 8.945159e-06, Test Loss : 0.0018501902\n",
      "Epoch 92. Train Loss: 8.552154e-06, Test Loss : 0.0017373872\n",
      "Epoch 93. Train Loss: 8.099047e-06, Test Loss : 0.0017897595\n",
      "Epoch 94. Train Loss: 7.698823e-06, Test Loss : 0.0017154213\n",
      "Epoch 95. Train Loss: 7.620215e-06, Test Loss : 0.0017259516\n",
      "Epoch 96. Train Loss: 7.047277e-06, Test Loss : 0.0018039042\n",
      "Epoch 97. Train Loss: 7.0666183e-06, Test Loss : 0.0017195757\n",
      "Epoch 98. Train Loss: 6.7186443e-06, Test Loss : 0.0018472739\n",
      "Epoch 99. Train Loss: 6.448164e-06, Test Loss : 0.0017108137\n",
      "\u001b[92m☑\u001b[0m 435+99 = 534(534) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 771+98 = 869(869) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 92+8 = 100(100) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 937+8 = 1445(945) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 59+76 = 135(135) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 4+19 = 22(23) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 49+9 = 588(58) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 4+6 = 80(10) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 113+2 = 115(115) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 11+886 = 897(897) 1/0 1\n",
      "Epoch 100. Train Loss: 6.2707895e-06, Test Loss : 0.0016868198\n",
      "Epoch 101. Train Loss: 6.1614683e-06, Test Loss : 0.0018904952\n",
      "Epoch 102. Train Loss: 5.870027e-06, Test Loss : 0.0017426513\n",
      "Epoch 103. Train Loss: 5.7656275e-06, Test Loss : 0.0017423304\n",
      "Epoch 104. Train Loss: 5.573838e-06, Test Loss : 0.001794162\n",
      "Epoch 105. Train Loss: 5.40657e-06, Test Loss : 0.0017656751\n",
      "Epoch 106. Train Loss: 5.2432065e-06, Test Loss : 0.001717704\n",
      "Epoch 107. Train Loss: 5.2053215e-06, Test Loss : 0.0018088354\n",
      "Epoch 108. Train Loss: 5.0632907e-06, Test Loss : 0.0017358294\n",
      "Epoch 109. Train Loss: 4.8259844e-06, Test Loss : 0.0017156923\n",
      "\u001b[92m☑\u001b[0m 82+656 = 738(738) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 4+36 = 40(40) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 21+441 = 462(462) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 827+9 = 836(836) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 93+634 = 727(727) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 8+50 = 58(58) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 788+68 = 856(856) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 4+391 = 395(395) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 29+92 = 121(121) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 85+5 = 100(90) 1/0 0\n",
      "Epoch 110. Train Loss: 4.827789e-06, Test Loss : 0.0016713577\n",
      "Epoch 111. Train Loss: 4.766883e-06, Test Loss : 0.0017643964\n",
      "Epoch 112. Train Loss: 4.5899255e-06, Test Loss : 0.001703555\n",
      "Epoch 113. Train Loss: 4.4984595e-06, Test Loss : 0.00168269\n",
      "Epoch 114. Train Loss: 4.385852e-06, Test Loss : 0.0017081304\n",
      "Epoch 115. Train Loss: 4.3697755e-06, Test Loss : 0.0016728835\n",
      "Epoch 116. Train Loss: 4.1657913e-06, Test Loss : 0.0017786408\n",
      "Epoch 117. Train Loss: 4.1867675e-06, Test Loss : 0.0016821312\n",
      "Epoch 118. Train Loss: 4.043256e-06, Test Loss : 0.0016830212\n",
      "Epoch 119. Train Loss: 3.987338e-06, Test Loss : 0.0016953725\n",
      "\u001b[92m☑\u001b[0m 78+6 = 84(84) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 0+4 = 43(4) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 2+8 = 9(10) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 717+13 = 730(730) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 5+629 = 634(634) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 868+7 = 875(875) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 5+3 = 8(8) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 6+1 = 3(7) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 96+3 = 100(99) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 585+716 = 1301(1301) 1/0 1\n",
      "Epoch 120. Train Loss: 3.9385095e-06, Test Loss : 0.0017524523\n",
      "Epoch 121. Train Loss: 3.8004582e-06, Test Loss : 0.0016607571\n",
      "Epoch 122. Train Loss: 3.7455684e-06, Test Loss : 0.001677741\n",
      "Epoch 123. Train Loss: 3.7786683e-06, Test Loss : 0.0016763138\n",
      "Epoch 124. Train Loss: 3.646835e-06, Test Loss : 0.0017381457\n",
      "Epoch 125. Train Loss: 3.531239e-06, Test Loss : 0.0017581045\n",
      "Epoch 126. Train Loss: 3.5310177e-06, Test Loss : 0.0017405147\n",
      "Epoch 127. Train Loss: 3.5286378e-06, Test Loss : 0.0017160043\n",
      "Epoch 128. Train Loss: 3.4565555e-06, Test Loss : 0.0016657064\n",
      "Epoch 129. Train Loss: 3.352815e-06, Test Loss : 0.0017176538\n",
      "\u001b[92m☑\u001b[0m 885+810 = 1695(1695) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 8+96 = 104(104) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 904+7 = 911(911) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 5+99 = 104(104) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 38+50 = 88(88) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 601+944 = 1545(1545) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 74+5 = 80(79) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 0+5 = 54(5) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 90+19 = 109(109) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 7+42 = 49(49) 1/0 1\n",
      "Epoch 130. Train Loss: 3.3648057e-06, Test Loss : 0.0016835963\n",
      "Epoch 131. Train Loss: 3.3245497e-06, Test Loss : 0.0016711984\n",
      "Epoch 132. Train Loss: 3.2041896e-06, Test Loss : 0.0016533004\n",
      "Epoch 133. Train Loss: 3.145815e-06, Test Loss : 0.0016743226\n",
      "Epoch 134. Train Loss: 3.153793e-06, Test Loss : 0.001665274\n",
      "Epoch 135. Train Loss: 3.04336e-06, Test Loss : 0.0016718863\n",
      "Epoch 136. Train Loss: 3.0101755e-06, Test Loss : 0.0016626905\n",
      "Epoch 137. Train Loss: 2.9543933e-06, Test Loss : 0.0017602628\n",
      "Epoch 138. Train Loss: 2.9222886e-06, Test Loss : 0.0017070081\n",
      "Epoch 139. Train Loss: 2.9257665e-06, Test Loss : 0.0017094959\n",
      "\u001b[92m☑\u001b[0m 47+280 = 327(327) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 3+70 = 72(73) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 17+4 = 21(21) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 351+56 = 407(407) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 274+0 = 274(274) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 25+4 = 39(29) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 1+370 = 371(371) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 4+68 = 72(72) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 7+615 = 622(622) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 8+96 = 104(104) 1/0 1\n",
      "Epoch 140. Train Loss: 2.9103949e-06, Test Loss : 0.0018222705\n",
      "Epoch 141. Train Loss: 2.8705554e-06, Test Loss : 0.0018118644\n",
      "Epoch 142. Train Loss: 2.875956e-06, Test Loss : 0.0017081324\n",
      "Epoch 143. Train Loss: 2.7553278e-06, Test Loss : 0.0016396759\n",
      "Epoch 144. Train Loss: 2.7441854e-06, Test Loss : 0.0016328084\n",
      "Epoch 145. Train Loss: 2.7006784e-06, Test Loss : 0.0018391504\n",
      "Epoch 146. Train Loss: 2.7205092e-06, Test Loss : 0.0016217001\n",
      "Epoch 147. Train Loss: 2.684982e-06, Test Loss : 0.0016084152\n",
      "Epoch 148. Train Loss: 2.632867e-06, Test Loss : 0.0016573655\n",
      "Epoch 149. Train Loss: 2.5928214e-06, Test Loss : 0.0017278077\n",
      "\u001b[92m☑\u001b[0m 30+18 = 48(48) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 831+980 = 1811(1811) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 12+5 = 17(17) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 538+432 = 970(970) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 9+983 = 992(992) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 87+263 = 350(350) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 92+740 = 832(832) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 0+27 = 28(27) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 16+2 = 28(18) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 1+5 = 5(6) 1/0 0\n",
      "Epoch 150. Train Loss: 2.5070897e-06, Test Loss : 0.0016800169\n",
      "Epoch 151. Train Loss: 2.5057377e-06, Test Loss : 0.0016117254\n",
      "Epoch 152. Train Loss: 2.4892659e-06, Test Loss : 0.0016752742\n",
      "Epoch 153. Train Loss: 2.4697645e-06, Test Loss : 0.0017204409\n",
      "Epoch 154. Train Loss: 2.4097335e-06, Test Loss : 0.0016259144\n",
      "Epoch 155. Train Loss: 2.3764685e-06, Test Loss : 0.0017750257\n",
      "Epoch 156. Train Loss: 2.3788023e-06, Test Loss : 0.0016271714\n",
      "Epoch 157. Train Loss: 2.3411585e-06, Test Loss : 0.0016611567\n",
      "Epoch 158. Train Loss: 2.3238902e-06, Test Loss : 0.0017997816\n",
      "Epoch 159. Train Loss: 2.3106227e-06, Test Loss : 0.0016048815\n",
      "\u001b[91m☒\u001b[0m 8+0 = 2(8) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 80+415 = 495(495) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 15+5 = 20(20) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 656+53 = 709(709) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 212+3 = 215(215) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 48+3 = 50(51) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 2+4 = 5(6) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 3+22 = 25(25) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 599+6 = 605(605) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 2+747 = 749(749) 1/0 1\n",
      "Epoch 160. Train Loss: 2.267655e-06, Test Loss : 0.001570998\n",
      "Epoch 161. Train Loss: 2.3055022e-06, Test Loss : 0.0016288506\n",
      "Epoch 162. Train Loss: 2.2583745e-06, Test Loss : 0.0016363475\n",
      "Epoch 163. Train Loss: 2.2284135e-06, Test Loss : 0.0016236076\n",
      "Epoch 164. Train Loss: 2.191982e-06, Test Loss : 0.0015891766\n",
      "Epoch 165. Train Loss: 2.132965e-06, Test Loss : 0.0015675953\n",
      "Epoch 166. Train Loss: 2.1426674e-06, Test Loss : 0.0015771482\n",
      "Epoch 167. Train Loss: 2.1499718e-06, Test Loss : 0.0015986059\n",
      "Epoch 168. Train Loss: 2.0993418e-06, Test Loss : 0.0016690089\n",
      "Epoch 169. Train Loss: 2.0779448e-06, Test Loss : 0.0016249504\n",
      "\u001b[91m☒\u001b[0m 1+1 = 22(2) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 8+196 = 204(204) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 441+439 = 880(880) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 688+479 = 1167(1167) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 74+639 = 713(713) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 17+596 = 613(613) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 3+654 = 657(657) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 44+77 = 121(121) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 269+5 = 274(274) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 494+3 = 507(497) 1/0 0\n",
      "Epoch 170. Train Loss: 2.0853206e-06, Test Loss : 0.0015912149\n",
      "Epoch 171. Train Loss: 2.032032e-06, Test Loss : 0.0016377654\n",
      "Epoch 172. Train Loss: 2.0434645e-06, Test Loss : 0.0016104052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173. Train Loss: 2.0216246e-06, Test Loss : 0.0016217118\n",
      "Epoch 174. Train Loss: 1.984745e-06, Test Loss : 0.0016715692\n",
      "Epoch 175. Train Loss: 1.965978e-06, Test Loss : 0.0015754653\n",
      "Epoch 176. Train Loss: 1.9408772e-06, Test Loss : 0.0016290244\n",
      "Epoch 177. Train Loss: 1.911637e-06, Test Loss : 0.0016166505\n",
      "Epoch 178. Train Loss: 1.927981e-06, Test Loss : 0.0017307382\n",
      "Epoch 179. Train Loss: 1.919613e-06, Test Loss : 0.0016378465\n",
      "\u001b[92m☑\u001b[0m 8+8 = 16(16) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 473+837 = 1310(1310) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 3+91 = 104(94) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 88+72 = 160(160) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 6+3 = 10(9) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 577+340 = 917(917) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 12+3 = 15(15) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 528+385 = 913(913) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 214+81 = 295(295) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 45+665 = 710(710) 1/0 1\n",
      "Epoch 180. Train Loss: 1.9401277e-06, Test Loss : 0.0015946736\n",
      "Epoch 181. Train Loss: 1.9302247e-06, Test Loss : 0.0015744527\n",
      "Epoch 182. Train Loss: 1.8725927e-06, Test Loss : 0.0017087928\n",
      "Epoch 183. Train Loss: 1.8481159e-06, Test Loss : 0.0016380952\n",
      "Epoch 184. Train Loss: 1.8116777e-06, Test Loss : 0.0015646319\n",
      "Epoch 185. Train Loss: 1.7912081e-06, Test Loss : 0.0018082341\n",
      "Epoch 186. Train Loss: 1.8051495e-06, Test Loss : 0.0015773571\n",
      "Epoch 187. Train Loss: 1.7722384e-06, Test Loss : 0.0016339226\n",
      "Epoch 188. Train Loss: 1.7681624e-06, Test Loss : 0.0017194562\n",
      "Epoch 189. Train Loss: 1.7529753e-06, Test Loss : 0.0015878351\n",
      "\u001b[92m☑\u001b[0m 93+104 = 197(197) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 987+5 = 992(992) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 0+519 = 519(519) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 87+66 = 153(153) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 52+587 = 639(639) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 6+56 = 62(62) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 46+9 = 546(55) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 417+11 = 428(428) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 6+56 = 62(62) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 5+784 = 789(789) 1/0 1\n",
      "Epoch 190. Train Loss: 1.7676213e-06, Test Loss : 0.0015841691\n",
      "Epoch 191. Train Loss: 1.7379934e-06, Test Loss : 0.001609168\n",
      "Epoch 192. Train Loss: 1.6753928e-06, Test Loss : 0.0016182342\n",
      "Epoch 193. Train Loss: 1.7165186e-06, Test Loss : 0.0016188441\n",
      "Epoch 194. Train Loss: 1.6619762e-06, Test Loss : 0.0016465686\n",
      "Epoch 195. Train Loss: 1.6892816e-06, Test Loss : 0.0016207226\n",
      "Epoch 196. Train Loss: 1.6887094e-06, Test Loss : 0.0015895793\n",
      "Epoch 197. Train Loss: 1.690575e-06, Test Loss : 0.0016563034\n",
      "Epoch 198. Train Loss: 1.637821e-06, Test Loss : 0.0016235949\n",
      "Epoch 199. Train Loss: 1.6111563e-06, Test Loss : 0.0016925673\n",
      "\u001b[92m☑\u001b[0m 16+7 = 23(23) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 622+419 = 1041(1041) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 44+109 = 153(153) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 148+0 = 148(148) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 975+295 = 1270(1270) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 19+569 = 588(588) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 56+622 = 678(678) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 334+8 = 342(342) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 68+71 = 139(139) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 125+696 = 821(821) 1/0 1\n",
      "Epoch 200. Train Loss: 1.6263567e-06, Test Loss : 0.0017036574\n"
     ]
    }
   ],
   "source": [
    "epochs = 201\n",
    "\n",
    "tot_test_loss = []\n",
    "tot_train_loss = []\n",
    "for e in range(epochs):\n",
    "    train_loss = []\n",
    "    for i, (x_data, y_data, z_data) in enumerate(tr_data_iterator):\n",
    "        x_data = x_data.as_in_context(ctx).astype('float32')\n",
    "        y_data = y_data.as_in_context(ctx).astype('float32')\n",
    "        z_data = z_data.as_in_context(ctx).astype('float32')\n",
    "        \n",
    "        with autograd.record():\n",
    "            z_output = model(x_data, y_data)\n",
    "            loss_ = loss(z_output, z_data)\n",
    "        loss_.backward()\n",
    "        trainer.step(x_data.shape[0])\n",
    "        curr_loss = nd.mean(loss_).asscalar()\n",
    "        train_loss.append(curr_loss)\n",
    "        \n",
    "    if e % 10 == 0:\n",
    "        q, y = gen_n_test(10)\n",
    "        for i in range(10):\n",
    "            with autograd.predict_mode():\n",
    "                p = model.calculation(q[i], char_indices, indices_char).strip()\n",
    "                iscorr = 1 if p == y[i] else 0\n",
    "                if iscorr == 1:\n",
    "                    print(colors.ok + '☑' + colors.close, end=' ')\n",
    "                else:\n",
    "                    print(colors.fail + '☒' + colors.close, end=' ')\n",
    "                print(\"{} = {}({}) 1/0 {}\".format(q[i], p, y[i], str(iscorr) ))\n",
    "    #caculate test loss\n",
    "    test_loss = calculate_loss(model, te_data_iterator, loss_obj = loss, ctx=ctx) \n",
    "\n",
    "    print(\"Epoch %s. Train Loss: %s, Test Loss : %s\" % (e, np.mean(train_loss), test_loss))    \n",
    "    tot_test_loss.append(test_loss)\n",
    "    tot_train_loss.append(np.mean(train_loss))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
