{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from mxnet import nd, autograd, gluon\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import nn, rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def n(digits =3):\n",
    "    number = ''\n",
    "    for i in range(np.random.randint(1, digits + 1)):\n",
    "        number += np.random.choice(list('0123456789'))\n",
    "    return int(number)\n",
    "\n",
    "def padding(chars, maxlen):\n",
    "    return chars + ' ' * (maxlen - len(chars))\n",
    "\n",
    "N = 50000\n",
    "N_train = int(N * .9)\n",
    "N_validation = N - N_train\n",
    "\n",
    "digits = 3\n",
    "input_digits = digits * 2 + 3\n",
    "output_digits = digits + 3\n",
    "\n",
    "added = set()\n",
    "questions = []\n",
    "answers = []\n",
    "answers_y = []\n",
    "\n",
    "while len(questions) < N:\n",
    "    a, b = n(), n()\n",
    "    pair = tuple(sorted((a, b)))\n",
    "    if pair in added:\n",
    "        continue\n",
    "        \n",
    "    question = 'S{}+{}E'.format(a, b)\n",
    "    question = padding(question, input_digits)\n",
    "    answer = 'S' + str(a + b) + 'E'\n",
    "    answer = padding(answer, output_digits)\n",
    "    answer_y = str(a + b) + 'E'\n",
    "    answer_y = padding(answer_y, output_digits)\n",
    "    \n",
    "    added.add(pair)\n",
    "    questions.append(question)\n",
    "    answers.append(answer)\n",
    "    answers_y.append(answer_y)\n",
    "    \n",
    "chars = '0123456789+SE '\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "X = np.zeros((len(questions), input_digits, len(chars)), dtype=np.integer)\n",
    "Y = np.zeros((len(questions), digits + 3, len(chars)), dtype=np.integer)\n",
    "Z = np.zeros((len(questions), digits + 3, len(chars)), dtype=np.integer)\n",
    "\n",
    "for i in range(N):\n",
    "    for t, char in enumerate(questions[i]):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    for t, char in enumerate(answers[i]):\n",
    "        Y[i, t, char_indices[char]] = 1\n",
    "    for t, char in enumerate(answers_y[i]):\n",
    "        Z[i, t, char_indices[char]] = 1\n",
    "    \n",
    "X_train, X_validation, Y_train, Y_validation, Z_train, Z_validation = \\\n",
    "    train_test_split(X, Y, Z, train_size=N_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_n_test(N):\n",
    "    q = []\n",
    "    y = []\n",
    "    for i in range(N):\n",
    "        a, b = n(), n() \n",
    "        question = '{}+{}'.format(a, b)\n",
    "        answer_y = str(a + b)\n",
    "        q.append(question)\n",
    "        y.append(answer_y)\n",
    "    return(q,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class calculator(gluon.Block):\n",
    "    def __init__(self, n_hidden, in_seq_len, out_seq_len, vocab_size, enc_layer, dec_layer = 1, **kwargs):\n",
    "        super(calculator, self).__init__(**kwargs)\n",
    "        self.in_seq_len = in_seq_len\n",
    "        self.out_seq_len = out_seq_len\n",
    "        self.n_hidden = n_hidden\n",
    "        self.vocab_size = vocab_size\n",
    "        self.enc_layer = enc_layer\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.encoder = rnn.LSTM(hidden_size = n_hidden, num_layers = enc_layer, layout = 'NTC')\n",
    "            self.decoder_0 = rnn.LSTMCell(hidden_size = n_hidden)\n",
    "            self.decoder_1 = rnn.LSTMCell(hidden_size = n_hidden)\n",
    "            self.batchnorm = nn.BatchNorm(axis = 2)\n",
    "            self.dense = nn.Dense(self.vocab_size, flatten = False)\n",
    "            \n",
    "    def forward(self, inputs, outputs):\n",
    "        # API says: num_layers, batch_size, num_hidden\n",
    "        self.batch_size = inputs.shape[0]\n",
    "        begin_state = self.encoder.begin_state(batch_size = self.batch_size, ctx = ctx)\n",
    "        enout, (h, c) = self.encoder(inputs, begin_state) # h, c: n_layer * batch_size * n_hidden\n",
    "        # Pick the hidden states and cell states at the last time step in the second layer\n",
    "        next_h_0 = h[0] # batch_size * n_hidden\n",
    "        next_c_0 = c[0] # batch_size * n_hidden\n",
    "        next_h_1 = h[1] # batch_size * n_hidden\n",
    "        next_c_1 = c[1] # batch_size * n_hidden\n",
    "        for i in range(self.out_seq_len):\n",
    "            deout, (next_h_0, next_c_0) = self.decoder_0(outputs[:, i, :], [next_h_0, next_c_0],)\n",
    "            deout, (next_h_1, next_c_1) = self.decoder_1(deout, [next_h_1, next_c_1],)\n",
    "            if i == 0:\n",
    "                deouts = deout\n",
    "            else:\n",
    "                deouts = nd.concat(deouts, deout, dim = 1)   \n",
    "        deouts = nd.reshape(deouts, (-1, self.out_seq_len, self.n_hidden))\n",
    "        deouts = self.batchnorm(deouts)\n",
    "        deouts_fc = self.dense(deouts)\n",
    "        return deouts_fc\n",
    "    \n",
    "    def calculation(self, input_str, char_indices, indices_char, input_digits = 9, lchars = 14, ctx = mx.gpu()):\n",
    "        input_str = 'S' + input_str + 'E'\n",
    "        X = nd.zeros((1, input_digits, lchars), ctx = ctx)\n",
    "        for t, char in enumerate(input_str):\n",
    "            X[0, t, char_indices[char]] = 1\n",
    "        Y_init = nd.zeros((1, lchars), ctx = ctx)\n",
    "        Y_init[0, char_indices['S']] = 1\n",
    "        begin_state = self.encoder.begin_state(batch_size = 1, ctx = ctx)\n",
    "        enout, (h, c) = self.encoder(X, begin_state)\n",
    "        next_h_0 = h[0] # batch_size * n_hidden\n",
    "        next_c_0 = c[0] # batch_size * n_hidden\n",
    "        next_h_1 = h[1] # batch_size * n_hidden\n",
    "        next_c_1 = c[1] # batch_size * n_hidden\n",
    "        deout = Y_init\n",
    "        \n",
    "        for i in range(self.out_seq_len):\n",
    "            deout, (next_h_0, next_c_0) = self.decoder_0(deout, [next_h_0, next_c_0],)\n",
    "            deout, (next_h_1, next_c_1) = self.decoder_1(deout, [next_h_1, next_c_1],)\n",
    "            deout = nd.expand_dims(deout, axis = 1)\n",
    "            deout = self.batchnorm(deout)\n",
    "            deout = deout[:, 0, :]\n",
    "            deout_sm = self.dense(deout)\n",
    "            deout = nd.one_hot(nd.argmax(nd.softmax(deout_sm, axis = 1), axis = 1), depth = self.vocab_size)\n",
    "            if i == 0:\n",
    "                ret_seq = indices_char[nd.argmax(deout_sm, axis = 1).asnumpy()[0].astype('int')]\n",
    "            else:\n",
    "                ret_seq += indices_char[nd.argmax(deout_sm, axis = 1).asnumpy()[0].astype('int')]\n",
    "\n",
    "            if ret_seq[-1] == ' ' or ret_seq[-1] == 'E':\n",
    "                break\n",
    "        return ret_seq.strip('E').strip()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_set = gluon.data.ArrayDataset(X_train, Y_train, Z_train)\n",
    "tr_data_iterator = gluon.data.DataLoader(tr_set, batch_size=256, shuffle=True)\n",
    "\n",
    "te_set =gluon.data.ArrayDataset(X_validation, Y_validation, Z_validation)\n",
    "te_data_iterator = gluon.data.DataLoader(te_set, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu()\n",
    "model = calculator(300, 9, 6, 14, 2)\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx = ctx)\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), 'rmsprop')\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss(axis = 2, sparse_label = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculator(\n",
      "  (encoder): LSTM(None -> 300, NTC, num_layers=2)\n",
      "  (decoder_0): LSTMCell(None -> 1200)\n",
      "  (decoder_1): LSTMCell(None -> 1200)\n",
      "  (batchnorm): BatchNorm(axis=2, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "  (dense): Dense(None -> 14, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(model, data_iter, loss_obj, ctx = ctx):\n",
    "    test_loss = []\n",
    "    for i, (x_data, y_data, z_data) in enumerate(data_iter):\n",
    "        x_data = x_data.as_in_context(ctx).astype('float32')\n",
    "        y_data = y_data.as_in_context(ctx).astype('float32')\n",
    "        z_data = z_data.as_in_context(ctx).astype('float32')\n",
    "        with autograd.predict_mode():\n",
    "            z_output = model(x_data, y_data)\n",
    "            loss_te = loss_obj(z_output, z_data)\n",
    "        curr_loss = nd.mean(loss_te).asscalar()\n",
    "        test_loss.append(curr_loss)\n",
    "    return np.mean(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m☒\u001b[0m 29+798 = 1005(827) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 5+0 = 105(5) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 92+74 = 1005(166) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 2+474 = 143(476) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 486+848 = 1005(1334) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 7+8 = 1005(15) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 29+0 = 1005(29) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 383+5 = 1305(388) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 621+53 = 135(674) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 44+58 = 550(102) 1/0 0\n",
      "Epoch 0. Train Loss: 1.1944661, Test Loss : 1.1354314\n",
      "Epoch 1. Train Loss: 1.1253184, Test Loss : 1.113188\n",
      "Epoch 2. Train Loss: 1.099548, Test Loss : 1.0727321\n",
      "Epoch 3. Train Loss: 1.034573, Test Loss : 0.9679411\n",
      "Epoch 4. Train Loss: 0.91749996, Test Loss : 0.8765491\n",
      "Epoch 5. Train Loss: 0.8379429, Test Loss : 0.8202093\n",
      "Epoch 6. Train Loss: 0.77298045, Test Loss : 0.76619637\n",
      "Epoch 7. Train Loss: 0.68192005, Test Loss : 0.65772104\n",
      "Epoch 8. Train Loss: 0.5897682, Test Loss : 0.5635521\n",
      "Epoch 9. Train Loss: 0.5030496, Test Loss : 0.47183222\n",
      "\u001b[91m☒\u001b[0m 6+42 = 496(48) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 471+52 = 523(523) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 1+321 = 332(322) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 779+18 = 806(797) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 2+1 = 36(3) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 91+54 = 145(145) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 753+93 = 947(846) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 361+37 = 408(398) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 245+7 = 342(252) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 8+228 = 236(236) 1/0 1\n",
      "Epoch 10. Train Loss: 0.37291843, Test Loss : 0.3257751\n",
      "Epoch 11. Train Loss: 0.23776656, Test Loss : 0.23003726\n",
      "Epoch 12. Train Loss: 0.15778524, Test Loss : 0.11969088\n",
      "Epoch 13. Train Loss: 0.11376262, Test Loss : 0.20606215\n",
      "Epoch 14. Train Loss: 0.0848567, Test Loss : 0.12332134\n",
      "Epoch 15. Train Loss: 0.07170666, Test Loss : 0.03713853\n",
      "Epoch 16. Train Loss: 0.056608893, Test Loss : 0.031132177\n",
      "Epoch 17. Train Loss: 0.051580384, Test Loss : 0.025897492\n",
      "Epoch 18. Train Loss: 0.046765435, Test Loss : 0.021238687\n",
      "Epoch 19. Train Loss: 0.03695642, Test Loss : 0.02377807\n",
      "\u001b[92m☑\u001b[0m 39+87 = 126(126) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 0+39 = 499(39) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 57+56 = 113(113) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 67+5 = 72(72) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 94+63 = 147(157) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 74+970 = 1044(1044) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 8+5 = 13(13) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 5+6 = 10(11) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 0+67 = 67(67) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 278+9 = 287(287) 1/0 1\n",
      "Epoch 20. Train Loss: 0.036288902, Test Loss : 0.015619214\n",
      "Epoch 21. Train Loss: 0.030791342, Test Loss : 0.015314447\n",
      "Epoch 22. Train Loss: 0.027674329, Test Loss : 0.012125755\n",
      "Epoch 23. Train Loss: 0.024996378, Test Loss : 0.4019832\n",
      "Epoch 24. Train Loss: 0.024966098, Test Loss : 0.14442676\n",
      "Epoch 25. Train Loss: 0.020657636, Test Loss : 0.0116739515\n",
      "Epoch 26. Train Loss: 0.02159369, Test Loss : 0.12259531\n",
      "Epoch 27. Train Loss: 0.017873468, Test Loss : 0.006819238\n",
      "Epoch 28. Train Loss: 0.017286064, Test Loss : 0.011700867\n",
      "Epoch 29. Train Loss: 0.015232116, Test Loss : 0.010920562\n",
      "\u001b[92m☑\u001b[0m 291+571 = 862(862) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 1+6 = 77(7) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 71+15 = 86(86) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 699+73 = 772(772) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 86+200 = 286(286) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 76+41 = 117(117) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 71+36 = 108(107) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 1+591 = 592(592) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 8+141 = 149(149) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 4+1 = 5(5) 1/0 1\n",
      "Epoch 30. Train Loss: 0.011633762, Test Loss : 0.009136015\n",
      "Epoch 31. Train Loss: 0.01515631, Test Loss : 0.0068577603\n",
      "Epoch 32. Train Loss: 0.012165937, Test Loss : 0.22926565\n",
      "Epoch 33. Train Loss: 0.010525457, Test Loss : 0.009910726\n",
      "Epoch 34. Train Loss: 0.012965718, Test Loss : 0.009296147\n",
      "Epoch 35. Train Loss: 0.01113597, Test Loss : 0.005622601\n",
      "Epoch 36. Train Loss: 0.0074655577, Test Loss : 0.009472831\n",
      "Epoch 37. Train Loss: 0.009538874, Test Loss : 0.0057685683\n",
      "Epoch 38. Train Loss: 0.007092014, Test Loss : 0.006938804\n",
      "Epoch 39. Train Loss: 0.00901016, Test Loss : 0.010298977\n",
      "\u001b[92m☑\u001b[0m 6+38 = 44(44) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 564+11 = 575(575) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 0+113 = 113(113) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 3+3 = 6(6) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 737+846 = 1583(1583) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 27+77 = 104(104) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 9+811 = 820(820) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 54+66 = 120(120) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 520+76 = 596(596) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 2+29 = 31(31) 1/0 1\n",
      "Epoch 40. Train Loss: 0.011691556, Test Loss : 0.014606474\n",
      "Epoch 41. Train Loss: 0.0071044522, Test Loss : 0.006238588\n",
      "Epoch 42. Train Loss: 0.006671469, Test Loss : 0.0710121\n",
      "Epoch 43. Train Loss: 0.009758559, Test Loss : 0.0047880285\n",
      "Epoch 44. Train Loss: 0.0062516797, Test Loss : 0.0145190265\n",
      "Epoch 45. Train Loss: 0.006874664, Test Loss : 0.0055145836\n",
      "Epoch 46. Train Loss: 0.00665851, Test Loss : 0.005472023\n",
      "Epoch 47. Train Loss: 0.0059344503, Test Loss : 0.0037837785\n",
      "Epoch 48. Train Loss: 0.0072530746, Test Loss : 0.005465571\n",
      "Epoch 49. Train Loss: 0.0039769243, Test Loss : 0.008127658\n",
      "\u001b[92m☑\u001b[0m 0+556 = 556(556) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 4+820 = 824(824) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 921+410 = 1341(1331) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 9+78 = 88(87) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 4+5 = 87(9) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 0+299 = 399(299) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 8+7 = 15(15) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 19+21 = 40(40) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 9+73 = 82(82) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 653+62 = 715(715) 1/0 1\n",
      "Epoch 50. Train Loss: 0.0048462716, Test Loss : 0.0045419973\n",
      "Epoch 51. Train Loss: 0.008817336, Test Loss : 0.0052057933\n",
      "Epoch 52. Train Loss: 0.0033400736, Test Loss : 0.004240523\n",
      "Epoch 53. Train Loss: 0.00017952466, Test Loss : 0.003200187\n",
      "Epoch 54. Train Loss: 6.965642e-05, Test Loss : 0.002798703\n",
      "Epoch 55. Train Loss: 5.088176e-05, Test Loss : 0.0027421904\n",
      "Epoch 56. Train Loss: 4.122693e-05, Test Loss : 0.0026579127\n",
      "Epoch 57. Train Loss: 3.5376717e-05, Test Loss : 0.00264167\n",
      "Epoch 58. Train Loss: 3.066418e-05, Test Loss : 0.0025983015\n",
      "Epoch 59. Train Loss: 2.7596807e-05, Test Loss : 0.0025926693\n",
      "\u001b[92m☑\u001b[0m 1+385 = 386(386) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 580+86 = 666(666) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 59+86 = 145(145) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 20+179 = 199(199) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 1+328 = 329(329) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 515+92 = 607(607) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 87+82 = 169(169) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 3+0 = 33(3) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 9+918 = 927(927) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 687+412 = 1099(1099) 1/0 1\n",
      "Epoch 60. Train Loss: 2.4799898e-05, Test Loss : 0.002570063\n",
      "Epoch 61. Train Loss: 2.277682e-05, Test Loss : 0.0025079348\n",
      "Epoch 62. Train Loss: 2.1169015e-05, Test Loss : 0.0025760727\n",
      "Epoch 63. Train Loss: 1.987256e-05, Test Loss : 0.0025383928\n",
      "Epoch 64. Train Loss: 1.798547e-05, Test Loss : 0.002459459\n",
      "Epoch 65. Train Loss: 1.6873173e-05, Test Loss : 0.0025779419\n",
      "Epoch 66. Train Loss: 1.5872605e-05, Test Loss : 0.0024708998\n",
      "Epoch 67. Train Loss: 1.4952408e-05, Test Loss : 0.0025901173\n",
      "Epoch 68. Train Loss: 1.4252559e-05, Test Loss : 0.0024902928\n",
      "Epoch 69. Train Loss: 1.35078235e-05, Test Loss : 0.0026987395\n",
      "\u001b[92m☑\u001b[0m 60+381 = 441(441) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 0+7 = 7(7) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 698+20 = 718(718) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 7+5 = 12(12) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 215+77 = 292(292) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 83+7 = 99(90) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 500+763 = 1263(1263) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 34+884 = 918(918) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 813+775 = 1588(1588) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 4+11 = 15(15) 1/0 1\n",
      "Epoch 70. Train Loss: 1.3072404e-05, Test Loss : 0.0024554897\n",
      "Epoch 71. Train Loss: 1.2350606e-05, Test Loss : 0.002488595\n",
      "Epoch 72. Train Loss: 1.216125e-05, Test Loss : 0.0024519388\n",
      "Epoch 73. Train Loss: 1.1377118e-05, Test Loss : 0.0024435641\n",
      "Epoch 74. Train Loss: 1.1082953e-05, Test Loss : 0.0023973628\n",
      "Epoch 75. Train Loss: 1.0429987e-05, Test Loss : 0.0023850657\n",
      "Epoch 76. Train Loss: 1.0177552e-05, Test Loss : 0.0024205237\n",
      "Epoch 77. Train Loss: 9.795484e-06, Test Loss : 0.0025147106\n",
      "Epoch 78. Train Loss: 9.438308e-06, Test Loss : 0.002653714\n",
      "Epoch 79. Train Loss: 9.174959e-06, Test Loss : 0.0023735894\n",
      "\u001b[92m☑\u001b[0m 6+5 = 11(11) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 304+8 = 312(312) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 9+978 = 987(987) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 80+0 = 80(80) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 91+672 = 763(763) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 4+6 = 10(10) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 355+35 = 390(390) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 0+38 = 388(38) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 583+3 = 586(586) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 1+58 = 59(59) 1/0 1\n",
      "Epoch 80. Train Loss: 8.82025e-06, Test Loss : 0.002366281\n",
      "Epoch 81. Train Loss: 8.594746e-06, Test Loss : 0.0024062265\n",
      "Epoch 82. Train Loss: 8.237101e-06, Test Loss : 0.0024325075\n",
      "Epoch 83. Train Loss: 8.078354e-06, Test Loss : 0.0024697916\n",
      "Epoch 84. Train Loss: 7.907601e-06, Test Loss : 0.0025467116\n",
      "Epoch 85. Train Loss: 7.6184e-06, Test Loss : 0.002390237\n",
      "Epoch 86. Train Loss: 7.4280583e-06, Test Loss : 0.002401237\n",
      "Epoch 87. Train Loss: 7.253115e-06, Test Loss : 0.0025275326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88. Train Loss: 7.1180707e-06, Test Loss : 0.0024517984\n",
      "Epoch 89. Train Loss: 6.8402123e-06, Test Loss : 0.0025251582\n",
      "\u001b[92m☑\u001b[0m 97+507 = 604(604) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 432+856 = 1288(1288) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 228+63 = 291(291) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 9+2 = 11(11) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 704+0 = 704(704) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 4+718 = 722(722) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 44+9 = 53(53) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 86+931 = 1017(1017) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 1+5 = 6(6) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 27+65 = 92(92) 1/0 1\n",
      "Epoch 90. Train Loss: 6.8194176e-06, Test Loss : 0.0023782158\n",
      "Epoch 91. Train Loss: 6.7371e-06, Test Loss : 0.0024871244\n",
      "Epoch 92. Train Loss: 6.340904e-06, Test Loss : 0.0023339286\n",
      "Epoch 93. Train Loss: 6.2266013e-06, Test Loss : 0.0026296931\n",
      "Epoch 94. Train Loss: 6.1020314e-06, Test Loss : 0.0025065304\n",
      "Epoch 95. Train Loss: 5.94201e-06, Test Loss : 0.0026804241\n",
      "Epoch 96. Train Loss: 5.890624e-06, Test Loss : 0.0023863213\n",
      "Epoch 97. Train Loss: 5.68883e-06, Test Loss : 0.0023624052\n",
      "Epoch 98. Train Loss: 5.4881425e-06, Test Loss : 0.0024435357\n",
      "Epoch 99. Train Loss: 5.4214183e-06, Test Loss : 0.0023423072\n",
      "\u001b[92m☑\u001b[0m 9+380 = 389(389) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 1+377 = 378(378) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 0+664 = 664(664) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 974+763 = 1737(1737) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 2+807 = 809(809) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 22+42 = 64(64) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 4+16 = 20(20) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 51+781 = 832(832) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 7+595 = 602(602) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 1+4 = 4(5) 1/0 0\n",
      "Epoch 100. Train Loss: 5.417224e-06, Test Loss : 0.0027147362\n",
      "Epoch 101. Train Loss: 5.281452e-06, Test Loss : 0.0023502975\n",
      "Epoch 102. Train Loss: 5.1925276e-06, Test Loss : 0.0023771366\n",
      "Epoch 103. Train Loss: 4.994083e-06, Test Loss : 0.002343514\n",
      "Epoch 104. Train Loss: 4.9968326e-06, Test Loss : 0.0023694048\n",
      "Epoch 105. Train Loss: 4.982155e-06, Test Loss : 0.0024030067\n",
      "Epoch 106. Train Loss: 4.766848e-06, Test Loss : 0.0023466602\n",
      "Epoch 107. Train Loss: 4.7335257e-06, Test Loss : 0.0024015266\n",
      "Epoch 108. Train Loss: 4.710144e-06, Test Loss : 0.0022994536\n",
      "Epoch 109. Train Loss: 4.5712627e-06, Test Loss : 0.0023938078\n",
      "\u001b[92m☑\u001b[0m 419+908 = 1327(1327) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 78+2 = 90(80) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 9+6 = 156(15) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 8+59 = 687(67) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 817+80 = 897(897) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 19+337 = 356(356) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 897+2 = 899(899) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 41+6 = 47(47) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 86+3 = 90(89) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 7+368 = 375(375) 1/0 1\n",
      "Epoch 110. Train Loss: 4.475056e-06, Test Loss : 0.0024314453\n",
      "Epoch 111. Train Loss: 4.477728e-06, Test Loss : 0.0023978795\n",
      "Epoch 112. Train Loss: 4.3955547e-06, Test Loss : 0.0023725815\n",
      "Epoch 113. Train Loss: 4.279037e-06, Test Loss : 0.0024816857\n",
      "Epoch 114. Train Loss: 4.253932e-06, Test Loss : 0.0022917935\n",
      "Epoch 115. Train Loss: 4.2187935e-06, Test Loss : 0.0023418763\n",
      "Epoch 116. Train Loss: 4.141414e-06, Test Loss : 0.002334469\n",
      "Epoch 117. Train Loss: 4.056916e-06, Test Loss : 0.0023918261\n",
      "Epoch 118. Train Loss: 3.9683637e-06, Test Loss : 0.002336132\n",
      "Epoch 119. Train Loss: 3.962954e-06, Test Loss : 0.0023412877\n",
      "\u001b[92m☑\u001b[0m 8+45 = 53(53) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 625+30 = 655(655) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 6+1 = 8(7) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 66+29 = 95(95) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 126+7 = 133(133) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 942+1 = 943(943) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 68+903 = 971(971) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 8+46 = 55(54) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 2+7 = 8(9) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 6+176 = 182(182) 1/0 1\n",
      "Epoch 120. Train Loss: 3.910675e-06, Test Loss : 0.0023060758\n",
      "Epoch 121. Train Loss: 3.8288617e-06, Test Loss : 0.0023509725\n",
      "Epoch 122. Train Loss: 3.755056e-06, Test Loss : 0.0023996443\n",
      "Epoch 123. Train Loss: 3.7109837e-06, Test Loss : 0.0023530067\n",
      "Epoch 124. Train Loss: 3.7149575e-06, Test Loss : 0.0024759762\n",
      "Epoch 125. Train Loss: 3.6237352e-06, Test Loss : 0.0023120078\n",
      "Epoch 126. Train Loss: 3.5837977e-06, Test Loss : 0.0023411328\n",
      "Epoch 127. Train Loss: 3.548173e-06, Test Loss : 0.0023263856\n",
      "Epoch 128. Train Loss: 3.4341554e-06, Test Loss : 0.0023325305\n",
      "Epoch 129. Train Loss: 3.39692e-06, Test Loss : 0.002316527\n",
      "\u001b[92m☑\u001b[0m 577+1 = 578(578) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 0+8 = 8(8) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 93+8 = 181(101) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 20+30 = 50(50) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 189+85 = 274(274) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 2+81 = 82(83) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 86+2 = 89(88) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 63+113 = 176(176) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 744+617 = 1361(1361) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 290+8 = 298(298) 1/0 1\n",
      "Epoch 130. Train Loss: 3.3438039e-06, Test Loss : 0.0023082183\n",
      "Epoch 131. Train Loss: 3.340291e-06, Test Loss : 0.0023569304\n",
      "Epoch 132. Train Loss: 3.336094e-06, Test Loss : 0.0024603708\n",
      "Epoch 133. Train Loss: 3.251302e-06, Test Loss : 0.0023419017\n",
      "Epoch 134. Train Loss: 3.2205942e-06, Test Loss : 0.002616123\n",
      "Epoch 135. Train Loss: 3.2096286e-06, Test Loss : 0.0023399943\n",
      "Epoch 136. Train Loss: 3.2074545e-06, Test Loss : 0.0023061861\n",
      "Epoch 137. Train Loss: 3.1709008e-06, Test Loss : 0.002308968\n",
      "Epoch 138. Train Loss: 3.1632517e-06, Test Loss : 0.0023867218\n",
      "Epoch 139. Train Loss: 3.0495264e-06, Test Loss : 0.0023146095\n",
      "\u001b[92m☑\u001b[0m 46+813 = 859(859) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 11+8 = 19(19) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 685+14 = 699(699) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 9+5 = 134(14) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 59+63 = 122(122) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 62+1 = 63(63) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 23+19 = 42(42) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 17+23 = 40(40) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 5+593 = 598(598) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 133+41 = 174(174) 1/0 1\n",
      "Epoch 140. Train Loss: 3.019155e-06, Test Loss : 0.0022769603\n",
      "Epoch 141. Train Loss: 2.9779062e-06, Test Loss : 0.002321989\n",
      "Epoch 142. Train Loss: 2.9734997e-06, Test Loss : 0.002584661\n",
      "Epoch 143. Train Loss: 2.9227442e-06, Test Loss : 0.0023097799\n",
      "Epoch 144. Train Loss: 2.852839e-06, Test Loss : 0.0024077664\n",
      "Epoch 145. Train Loss: 2.8883978e-06, Test Loss : 0.002305796\n",
      "Epoch 146. Train Loss: 2.779954e-06, Test Loss : 0.0026836789\n",
      "Epoch 147. Train Loss: 2.834384e-06, Test Loss : 0.0023663144\n",
      "Epoch 148. Train Loss: 2.758384e-06, Test Loss : 0.0026344508\n",
      "Epoch 149. Train Loss: 2.7730791e-06, Test Loss : 0.0022728553\n",
      "\u001b[92m☑\u001b[0m 570+9 = 579(579) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 9+1 = 10(10) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 40+669 = 709(709) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 86+0 = 96(86) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 6+953 = 959(959) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 159+8 = 167(167) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 59+883 = 942(942) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 6+55 = 61(61) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 7+74 = 810(81) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 89+59 = 1488(148) 1/0 0\n",
      "Epoch 150. Train Loss: 2.6986693e-06, Test Loss : 0.002604495\n",
      "Epoch 151. Train Loss: 2.6783944e-06, Test Loss : 0.0022417598\n",
      "Epoch 152. Train Loss: 2.632586e-06, Test Loss : 0.0023082746\n",
      "Epoch 153. Train Loss: 2.5821957e-06, Test Loss : 0.0022902398\n",
      "Epoch 154. Train Loss: 2.5912623e-06, Test Loss : 0.0023404416\n",
      "Epoch 155. Train Loss: 2.5444178e-06, Test Loss : 0.0023926888\n",
      "Epoch 156. Train Loss: 2.5511017e-06, Test Loss : 0.002456902\n",
      "Epoch 157. Train Loss: 2.5340576e-06, Test Loss : 0.0022613981\n",
      "Epoch 158. Train Loss: 2.527169e-06, Test Loss : 0.0022777005\n",
      "Epoch 159. Train Loss: 2.5476386e-06, Test Loss : 0.0025646505\n",
      "\u001b[92m☑\u001b[0m 11+3 = 14(14) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 52+3 = 65(55) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 49+4 = 53(53) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 19+28 = 47(47) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 2+7 = 8(9) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 26+951 = 977(977) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 84+31 = 115(115) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 589+387 = 976(976) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 90+304 = 394(394) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 7+963 = 960(970) 1/0 0\n",
      "Epoch 160. Train Loss: 2.4810563e-06, Test Loss : 0.0022978082\n",
      "Epoch 161. Train Loss: 2.4720318e-06, Test Loss : 0.002321119\n",
      "Epoch 162. Train Loss: 2.4433496e-06, Test Loss : 0.0025469062\n",
      "Epoch 163. Train Loss: 2.4280232e-06, Test Loss : 0.002378197\n",
      "Epoch 164. Train Loss: 2.3512423e-06, Test Loss : 0.0023581425\n",
      "Epoch 165. Train Loss: 2.3050698e-06, Test Loss : 0.0023148556\n",
      "Epoch 166. Train Loss: 2.3143527e-06, Test Loss : 0.002371539\n",
      "Epoch 167. Train Loss: 2.3689236e-06, Test Loss : 0.0023405543\n",
      "Epoch 168. Train Loss: 2.2548024e-06, Test Loss : 0.002408815\n",
      "Epoch 169. Train Loss: 2.305956e-06, Test Loss : 0.002317919\n",
      "\u001b[92m☑\u001b[0m 1+117 = 118(118) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 46+129 = 175(175) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 3+6 = 87(9) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 60+376 = 436(436) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 8+133 = 141(141) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 833+0 = 833(833) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 5+12 = 17(17) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 1+2 = 33(3) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 9+6 = 156(15) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 6+71 = 78(77) 1/0 0\n",
      "Epoch 170. Train Loss: 2.2365368e-06, Test Loss : 0.0022959542\n",
      "Epoch 171. Train Loss: 2.272691e-06, Test Loss : 0.0023232463\n",
      "Epoch 172. Train Loss: 2.2115776e-06, Test Loss : 0.0022597706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173. Train Loss: 2.1838339e-06, Test Loss : 0.0023485278\n",
      "Epoch 174. Train Loss: 2.1711367e-06, Test Loss : 0.0023103869\n",
      "Epoch 175. Train Loss: 2.1425649e-06, Test Loss : 0.0022776288\n",
      "Epoch 176. Train Loss: 2.1329097e-06, Test Loss : 0.0023074658\n",
      "Epoch 177. Train Loss: 2.1482826e-06, Test Loss : 0.0024132668\n",
      "Epoch 178. Train Loss: 2.1081619e-06, Test Loss : 0.0023862284\n",
      "Epoch 179. Train Loss: 2.092818e-06, Test Loss : 0.002298784\n",
      "\u001b[92m☑\u001b[0m 9+821 = 830(830) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 981+7 = 988(988) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 3+5 = 87(8) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 519+6 = 525(525) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 86+50 = 136(136) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 13+61 = 74(74) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 8+544 = 552(552) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 195+83 = 278(278) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 5+41 = 46(46) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 89+4 = 133(93) 1/0 0\n",
      "Epoch 180. Train Loss: 2.0457828e-06, Test Loss : 0.0023088977\n",
      "Epoch 181. Train Loss: 2.0648488e-06, Test Loss : 0.002313013\n",
      "Epoch 182. Train Loss: 2.057208e-06, Test Loss : 0.0023098933\n",
      "Epoch 183. Train Loss: 2.008566e-06, Test Loss : 0.002311127\n",
      "Epoch 184. Train Loss: 2.0075609e-06, Test Loss : 0.0022626827\n",
      "Epoch 185. Train Loss: 1.9916909e-06, Test Loss : 0.0025640146\n",
      "Epoch 186. Train Loss: 1.992591e-06, Test Loss : 0.0022945225\n",
      "Epoch 187. Train Loss: 1.96232e-06, Test Loss : 0.0022855327\n",
      "Epoch 188. Train Loss: 1.9519168e-06, Test Loss : 0.0023104178\n",
      "Epoch 189. Train Loss: 1.9588874e-06, Test Loss : 0.0022949749\n",
      "\u001b[92m☑\u001b[0m 6+664 = 670(670) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 54+57 = 112(111) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 2+26 = 28(28) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 4+631 = 635(635) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 705+9 = 714(714) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 40+481 = 521(521) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 87+10 = 97(97) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 4+2 = 6(6) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 656+8 = 664(664) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 695+953 = 1648(1648) 1/0 1\n",
      "Epoch 190. Train Loss: 1.9154477e-06, Test Loss : 0.0023130018\n",
      "Epoch 191. Train Loss: 1.8978964e-06, Test Loss : 0.00263997\n",
      "Epoch 192. Train Loss: 1.9627305e-06, Test Loss : 0.0023107147\n",
      "Epoch 193. Train Loss: 1.8852453e-06, Test Loss : 0.0024310104\n",
      "Epoch 194. Train Loss: 1.8607047e-06, Test Loss : 0.0022856216\n",
      "Epoch 195. Train Loss: 1.8508338e-06, Test Loss : 0.002284558\n",
      "Epoch 196. Train Loss: 1.8826752e-06, Test Loss : 0.002313412\n",
      "Epoch 197. Train Loss: 1.8475887e-06, Test Loss : 0.002281352\n",
      "Epoch 198. Train Loss: 1.8321608e-06, Test Loss : 0.002247051\n",
      "Epoch 199. Train Loss: 1.8044452e-06, Test Loss : 0.0023398625\n",
      "\u001b[92m☑\u001b[0m 4+847 = 851(851) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 928+724 = 1652(1652) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 71+2 = 83(73) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 38+77 = 115(115) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 469+5 = 474(474) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 261+6 = 267(267) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 6+151 = 157(157) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 46+610 = 656(656) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 8+59 = 687(67) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 25+84 = 109(109) 1/0 1\n",
      "Epoch 200. Train Loss: 1.8045001e-06, Test Loss : 0.0023207664\n"
     ]
    }
   ],
   "source": [
    "epochs = 201\n",
    "\n",
    "tot_test_loss = []\n",
    "tot_train_loss = []\n",
    "for e in range(epochs):\n",
    "    train_loss = []\n",
    "    for i, (x_data, y_data, z_data) in enumerate(tr_data_iterator):\n",
    "        x_data = x_data.as_in_context(ctx).astype('float32')\n",
    "        y_data = y_data.as_in_context(ctx).astype('float32')\n",
    "        z_data = z_data.as_in_context(ctx).astype('float32')\n",
    "        \n",
    "        with autograd.record():\n",
    "            z_output = model(x_data, y_data)\n",
    "            loss_ = loss(z_output, z_data)\n",
    "        loss_.backward()\n",
    "        trainer.step(x_data.shape[0])\n",
    "        curr_loss = nd.mean(loss_).asscalar()\n",
    "        train_loss.append(curr_loss)\n",
    "        \n",
    "    if e % 10 == 0:\n",
    "        q, y = gen_n_test(10)\n",
    "        for i in range(10):\n",
    "            with autograd.predict_mode():\n",
    "                p = model.calculation(q[i], char_indices, indices_char).strip()\n",
    "                iscorr = 1 if p == y[i] else 0\n",
    "                if iscorr == 1:\n",
    "                    print(colors.ok + '☑' + colors.close, end=' ')\n",
    "                else:\n",
    "                    print(colors.fail + '☒' + colors.close, end=' ')\n",
    "                print(\"{} = {}({}) 1/0 {}\".format(q[i], p, y[i], str(iscorr) ))\n",
    "    #caculate test loss\n",
    "    test_loss = calculate_loss(model, te_data_iterator, loss_obj = loss, ctx=ctx) \n",
    "\n",
    "    print(\"Epoch %s. Train Loss: %s, Test Loss : %s\" % (e, np.mean(train_loss), test_loss))    \n",
    "    tot_test_loss.append(test_loss)\n",
    "    tot_train_loss.append(np.mean(train_loss))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
