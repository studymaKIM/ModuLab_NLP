{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from mxnet import nd, autograd, gluon\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import nn, rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def n(digits =3):\n",
    "    number = ''\n",
    "    for i in range(np.random.randint(1, digits + 1)):\n",
    "        number += np.random.choice(list('0123456789'))\n",
    "    return int(number)\n",
    "\n",
    "def padding(chars, maxlen):\n",
    "    return chars + ' ' * (maxlen - len(chars))\n",
    "\n",
    "N = 50000\n",
    "N_train = int(N * .9)\n",
    "N_validation = N - N_train\n",
    "\n",
    "digits = 3\n",
    "input_digits = digits * 2 + 3\n",
    "output_digits = digits + 3\n",
    "\n",
    "added = set()\n",
    "questions = []\n",
    "answers = []\n",
    "answers_y = []\n",
    "\n",
    "while len(questions) < N:\n",
    "    a, b = n(), n()\n",
    "    pair = tuple(sorted((a, b)))\n",
    "    if pair in added:\n",
    "        continue\n",
    "        \n",
    "    question = 'S{}+{}E'.format(a, b)\n",
    "    question = padding(question, input_digits)\n",
    "    answer = 'S' + str(a + b) + 'E'\n",
    "    answer = padding(answer, output_digits)\n",
    "    answer_y = str(a + b) + 'E'\n",
    "    answer_y = padding(answer_y, output_digits)\n",
    "    \n",
    "    added.add(pair)\n",
    "    questions.append(question)\n",
    "    answers.append(answer)\n",
    "    answers_y.append(answer_y)\n",
    "    \n",
    "chars = '0123456789+SE '\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "X = np.zeros((len(questions), input_digits, len(chars)), dtype=np.integer)\n",
    "Y = np.zeros((len(questions), digits + 3, len(chars)), dtype=np.integer)\n",
    "Z = np.zeros((len(questions), digits + 3, len(chars)), dtype=np.integer)\n",
    "\n",
    "for i in range(N):\n",
    "    for t, char in enumerate(questions[i]):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    for t, char in enumerate(answers[i]):\n",
    "        Y[i, t, char_indices[char]] = 1\n",
    "    for t, char in enumerate(answers_y[i]):\n",
    "        Z[i, t, char_indices[char]] = 1\n",
    "    \n",
    "X_train, X_validation, Y_train, Y_validation, Z_train, Z_validation = \\\n",
    "    train_test_split(X, Y, Z, train_size=N_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_n_test(N):\n",
    "    q = []\n",
    "    y = []\n",
    "    for i in range(N):\n",
    "        a, b = n(), n() \n",
    "        question = '{}+{}'.format(a, b)\n",
    "        answer_y = str(a + b)\n",
    "        q.append(question)\n",
    "        y.append(answer_y)\n",
    "    return(q,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class calculator(gluon.Block):\n",
    "    def __init__(self, n_hidden, in_seq_len, out_seq_len, vocab_size, **kwargs):\n",
    "        super(calculator, self).__init__(**kwargs)\n",
    "        self.in_seq_len = in_seq_len\n",
    "        self.out_seq_len = out_seq_len\n",
    "        self.n_hidden = n_hidden\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.encoder = rnn.LSTMCell(hidden_size = n_hidden)\n",
    "            self.decoder = rnn.LSTMCell(hidden_size = n_hidden)\n",
    "            self.batchnorm = nn.BatchNorm(axis = 2)\n",
    "            self.dense = nn.Dense(self.vocab_size, flatten = False)\n",
    "            \n",
    "    def forward(self, inputs, outputs):\n",
    "        # Since we don't use intermediate states for 'though vector', we don't need to unroll it.\n",
    "        # In the later examples, we will use LSTM class rather than LSTMCell class.\n",
    "        enout, (next_h, next_c) = self.encoder.unroll(inputs = inputs\n",
    "                                                    , length = self.in_seq_len\n",
    "                                                    , merge_outputs = True)\n",
    "        for i in range(self.out_seq_len):\n",
    "            deout, (next_h, next_c) = self.decoder(outputs[:, i, :], [next_h, next_c],)\n",
    "            if i == 0:\n",
    "                deouts = deout\n",
    "            else:\n",
    "                deouts = nd.concat(deouts, deout, dim = 1)\n",
    "            #print('i= {}, deouts= {}'.format(i, deouts.shape))\n",
    "        \n",
    "        deouts = nd.reshape(deouts, (-1, self.out_seq_len, self.n_hidden))\n",
    "        deouts = self.batchnorm(deouts)\n",
    "        deouts_fc = self.dense(deouts)\n",
    "        return deouts_fc\n",
    "    \n",
    "    def calculation(self, input_str, char_indices, indices_char, input_digits = 9, lchars = 14, ctx = mx.gpu()):\n",
    "        input_str = 'S' + input_str + 'E'\n",
    "        X = nd.zeros((1, input_digits, lchars), ctx = ctx)\n",
    "        for t, char in enumerate(input_str):\n",
    "            X[0, t, char_indices[char]] = 1\n",
    "        Y_init = nd.zeros((1, lchars), ctx = ctx)\n",
    "        Y_init[0, char_indices['S']] = 1\n",
    "        enout, (next_h, next_c) = self.encoder.unroll(inputs = X, length = self.in_seq_len, merge_outputs = True)\n",
    "        deout = Y_init\n",
    "        \n",
    "        for i in range(self.out_seq_len):\n",
    "            deout, (next_h, next_c) = self.decoder(deout, [next_h, next_c])\n",
    "            #print('dim deout = {}'.format(deout.shape))\n",
    "            deout = nd.expand_dims(deout, axis = 1)\n",
    "            #print('dim deout = {}'.format(deout.shape))\n",
    "            deout = self.batchnorm(deout)\n",
    "            deout = deout[:, 0, :]\n",
    "            #print('dim deout = {}'.format(deout.shape))\n",
    "\n",
    "            deout_sm = self.dense(deout)\n",
    "            deout = nd.one_hot(nd.argmax(nd.softmax(deout_sm, axis = 1), axis = 1), depth = self.vocab_size)\n",
    "            if i == 0:\n",
    "                ret_seq = indices_char[nd.argmax(deout_sm, axis = 1).asnumpy()[0].astype('int')]\n",
    "            else:\n",
    "                ret_seq += indices_char[nd.argmax(deout_sm, axis = 1).asnumpy()[0].astype('int')]\n",
    "\n",
    "            if ret_seq[-1] == ' ' or ret_seq[-1] == 'E':\n",
    "                break\n",
    "        return ret_seq.strip('E').strip()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_set = gluon.data.ArrayDataset(X_train, Y_train, Z_train)\n",
    "tr_data_iterator = gluon.data.DataLoader(tr_set, batch_size=256, shuffle=True)\n",
    "\n",
    "te_set =gluon.data.ArrayDataset(X_validation, Y_validation, Z_validation)\n",
    "te_data_iterator = gluon.data.DataLoader(te_set, batch_size=256, shuffle=True)\n",
    "\n",
    "ctx = mx.gpu()\n",
    "model = calculator(300, 9, 6, 14)\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx = ctx)\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), 'rmsprop')\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss(axis = 2, sparse_label = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculator(\n",
      "  (encoder): LSTMCell(None -> 1200)\n",
      "  (decoder): LSTMCell(None -> 1200)\n",
      "  (batchnorm): BatchNorm(axis=2, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "  (dense): Dense(None -> 14, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(model, data_iter, loss_obj, ctx = ctx):\n",
    "    test_loss = []\n",
    "    for i, (x_data, y_data, z_data) in enumerate(data_iter):\n",
    "        x_data = x_data.as_in_context(ctx).astype('float32')\n",
    "        y_data = y_data.as_in_context(ctx).astype('float32')\n",
    "        z_data = z_data.as_in_context(ctx).astype('float32')\n",
    "        with autograd.predict_mode():\n",
    "            z_output = model(x_data, y_data)\n",
    "            loss_te = loss_obj(z_output, z_data)\n",
    "        curr_loss = nd.mean(loss_te).asscalar()\n",
    "        test_loss.append(curr_loss)\n",
    "    return np.mean(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m☒\u001b[0m 252+26 = 222(278) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 89+84 = 1032(173) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 14+66 = 144(80) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 1+0 = 122(1) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 161+175 = 1122(336) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 0+270 = 122(270) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 1+85 = 122(86) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 572+661 = 1144(1233) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 19+905 = 1003(924) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 0+13 = 122(13) 1/0 0\n",
      "Epoch 0. Train Loss: 1.1944102, Test Loss : 1.1342056\n",
      "Epoch 1. Train Loss: 1.1213632, Test Loss : 1.1067674\n",
      "Epoch 2. Train Loss: 1.0915885, Test Loss : 1.0877845\n",
      "Epoch 3. Train Loss: 1.0276933, Test Loss : 0.96420777\n",
      "Epoch 4. Train Loss: 0.92164785, Test Loss : 0.87567604\n",
      "Epoch 5. Train Loss: 0.83364654, Test Loss : 0.830315\n",
      "Epoch 6. Train Loss: 0.76330084, Test Loss : 0.73404014\n",
      "Epoch 7. Train Loss: 0.69588923, Test Loss : 0.6664765\n",
      "Epoch 8. Train Loss: 0.6245909, Test Loss : 0.5847117\n",
      "Epoch 9. Train Loss: 0.54041433, Test Loss : 0.51308846\n",
      "\u001b[91m☒\u001b[0m 4+28 = 212(32) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 1+62 = 632(63) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 0+952 = 942(952) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 5+1 = 69(6) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 298+482 = 889(780) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 5+741 = 745(746) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 8+4 = 123(12) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 56+13 = 79(69) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 58+20 = 78(78) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 97+643 = 731(740) 1/0 0\n",
      "Epoch 10. Train Loss: 0.4313195, Test Loss : 0.38500667\n",
      "Epoch 11. Train Loss: 0.32246515, Test Loss : 0.26972848\n",
      "Epoch 12. Train Loss: 0.24044943, Test Loss : 0.26540518\n",
      "Epoch 13. Train Loss: 0.18183765, Test Loss : 0.23185591\n",
      "Epoch 14. Train Loss: 0.14079529, Test Loss : 0.13731876\n",
      "Epoch 15. Train Loss: 0.11226056, Test Loss : 0.14742437\n",
      "Epoch 16. Train Loss: 0.09189934, Test Loss : 0.07572783\n",
      "Epoch 17. Train Loss: 0.07704744, Test Loss : 0.112596\n",
      "Epoch 18. Train Loss: 0.06508983, Test Loss : 0.047631364\n",
      "Epoch 19. Train Loss: 0.056473136, Test Loss : 0.047988404\n",
      "\u001b[92m☑\u001b[0m 907+4 = 911(911) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 75+18 = 93(93) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 284+641 = 925(925) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 861+682 = 1543(1543) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 144+6 = 149(150) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 0+49 = 49(49) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 244+0 = 344(244) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 93+542 = 635(635) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 36+1 = 38(37) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 13+15 = 28(28) 1/0 1\n",
      "Epoch 20. Train Loss: 0.053242538, Test Loss : 0.053634327\n",
      "Epoch 21. Train Loss: 0.0462114, Test Loss : 0.031108674\n",
      "Epoch 22. Train Loss: 0.039406657, Test Loss : 0.12305516\n",
      "Epoch 23. Train Loss: 0.037634037, Test Loss : 0.027889108\n",
      "Epoch 24. Train Loss: 0.034139086, Test Loss : 0.060063757\n",
      "Epoch 25. Train Loss: 0.029986106, Test Loss : 0.026660442\n",
      "Epoch 26. Train Loss: 0.027471127, Test Loss : 0.021096582\n",
      "Epoch 27. Train Loss: 0.026798872, Test Loss : 0.04278791\n",
      "Epoch 28. Train Loss: 0.025528599, Test Loss : 0.01961552\n",
      "Epoch 29. Train Loss: 0.021055184, Test Loss : 0.05604862\n",
      "\u001b[91m☒\u001b[0m 18+8 = 25(26) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 6+2 = 9(8) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 1+47 = 48(48) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 545+8 = 553(553) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 85+47 = 132(132) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 289+3 = 292(292) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 10+664 = 674(674) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 3+212 = 215(215) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 230+158 = 388(388) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 159+7 = 166(166) 1/0 1\n",
      "Epoch 30. Train Loss: 0.021416523, Test Loss : 0.01655003\n",
      "Epoch 31. Train Loss: 0.019042842, Test Loss : 0.030978963\n",
      "Epoch 32. Train Loss: 0.019858742, Test Loss : 0.08129202\n",
      "Epoch 33. Train Loss: 0.017950475, Test Loss : 0.018673236\n",
      "Epoch 34. Train Loss: 0.014248032, Test Loss : 0.02834635\n",
      "Epoch 35. Train Loss: 0.018587666, Test Loss : 0.014292779\n",
      "Epoch 36. Train Loss: 0.012783655, Test Loss : 0.028279847\n",
      "Epoch 37. Train Loss: 0.014434679, Test Loss : 0.02143659\n",
      "Epoch 38. Train Loss: 0.013459987, Test Loss : 0.011096481\n",
      "Epoch 39. Train Loss: 0.011790967, Test Loss : 0.030777996\n",
      "\u001b[92m☑\u001b[0m 5+9 = 14(14) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 24+314 = 338(338) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 8+6 = 14(14) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 776+39 = 815(815) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 5+56 = 61(61) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 48+990 = 1038(1038) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 9+8 = 17(17) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 2+833 = 835(835) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 0+5 = 5(5) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 703+80 = 783(783) 1/0 1\n",
      "Epoch 40. Train Loss: 0.012887654, Test Loss : 0.02225393\n",
      "Epoch 41. Train Loss: 0.009929806, Test Loss : 0.015733635\n",
      "Epoch 42. Train Loss: 0.010350447, Test Loss : 0.020115495\n",
      "Epoch 43. Train Loss: 0.009908658, Test Loss : 0.01395365\n",
      "Epoch 44. Train Loss: 0.010698033, Test Loss : 0.009549437\n",
      "Epoch 45. Train Loss: 0.009783694, Test Loss : 0.012526812\n",
      "Epoch 46. Train Loss: 0.008416576, Test Loss : 0.0098410705\n",
      "Epoch 47. Train Loss: 0.008149586, Test Loss : 0.012096362\n",
      "Epoch 48. Train Loss: 0.0097493855, Test Loss : 0.009924805\n",
      "Epoch 49. Train Loss: 0.0072962777, Test Loss : 0.014599289\n",
      "\u001b[92m☑\u001b[0m 0+1 = 1(1) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 75+6 = 131(81) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 151+6 = 157(157) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 9+14 = 23(23) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 85+5 = 109(90) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 122+108 = 229(230) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 91+62 = 153(153) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 296+5 = 301(301) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 5+54 = 59(59) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 425+28 = 453(453) 1/0 1\n",
      "Epoch 50. Train Loss: 0.0075735557, Test Loss : 0.013070713\n",
      "Epoch 51. Train Loss: 0.0070978147, Test Loss : 0.010023954\n",
      "Epoch 52. Train Loss: 0.0063752625, Test Loss : 0.009391213\n",
      "Epoch 53. Train Loss: 0.0066671222, Test Loss : 0.0091220215\n",
      "Epoch 54. Train Loss: 0.0059375796, Test Loss : 0.009149132\n",
      "Epoch 55. Train Loss: 0.0057610343, Test Loss : 0.010100809\n",
      "Epoch 56. Train Loss: 0.0060359957, Test Loss : 0.01021214\n",
      "Epoch 57. Train Loss: 0.007157758, Test Loss : 0.00913977\n",
      "Epoch 58. Train Loss: 0.00482377, Test Loss : 0.016008954\n",
      "Epoch 59. Train Loss: 0.0061260206, Test Loss : 0.012745082\n",
      "\u001b[92m☑\u001b[0m 2+1 = 3(3) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 2+24 = 36(26) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 26+77 = 103(103) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 535+15 = 550(550) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 7+17 = 24(24) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 918+0 = 918(918) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 426+508 = 934(934) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 26+20 = 46(46) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 314+48 = 362(362) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 4+30 = 33(34) 1/0 0\n",
      "Epoch 60. Train Loss: 0.0048935725, Test Loss : 0.019845497\n",
      "Epoch 61. Train Loss: 0.0043146806, Test Loss : 0.013273135\n",
      "Epoch 62. Train Loss: 0.0044514653, Test Loss : 0.008024013\n",
      "Epoch 63. Train Loss: 0.005103178, Test Loss : 0.038366415\n",
      "Epoch 64. Train Loss: 0.0056975367, Test Loss : 0.018700154\n",
      "Epoch 65. Train Loss: 0.006086072, Test Loss : 0.009267914\n",
      "Epoch 66. Train Loss: 0.0038668953, Test Loss : 0.00986959\n",
      "Epoch 67. Train Loss: 0.0048916447, Test Loss : 0.008247509\n",
      "Epoch 68. Train Loss: 0.004418556, Test Loss : 0.008464279\n",
      "Epoch 69. Train Loss: 0.0038475299, Test Loss : 0.007675326\n",
      "\u001b[92m☑\u001b[0m 7+8 = 15(15) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 37+601 = 638(638) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 498+5 = 503(503) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 7+695 = 702(702) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 525+137 = 662(662) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 1+8 = 9(9) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 99+22 = 121(121) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 1+3 = 4(4) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 36+563 = 699(599) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 6+234 = 240(240) 1/0 1\n",
      "Epoch 70. Train Loss: 0.003911817, Test Loss : 0.009360841\n",
      "Epoch 71. Train Loss: 0.0039350493, Test Loss : 0.009379811\n",
      "Epoch 72. Train Loss: 0.0044763084, Test Loss : 0.0074048503\n",
      "Epoch 73. Train Loss: 0.0033047795, Test Loss : 0.02730472\n",
      "Epoch 74. Train Loss: 0.0041614133, Test Loss : 0.00879924\n",
      "Epoch 75. Train Loss: 0.0038268643, Test Loss : 0.010827142\n",
      "Epoch 76. Train Loss: 0.0022544863, Test Loss : 0.022041015\n",
      "Epoch 77. Train Loss: 0.0037373507, Test Loss : 0.009612419\n",
      "Epoch 78. Train Loss: 0.0041429815, Test Loss : 0.009284036\n",
      "Epoch 79. Train Loss: 0.0023332874, Test Loss : 0.0063751927\n",
      "\u001b[92m☑\u001b[0m 6+4 = 10(10) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 52+238 = 290(290) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 693+1 = 694(694) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 5+9 = 14(14) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 2+610 = 612(612) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 583+785 = 1368(1368) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 309+570 = 879(879) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 41+2 = 44(43) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 5+52 = 57(57) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 978+98 = 1076(1076) 1/0 1\n",
      "Epoch 80. Train Loss: 0.0030816607, Test Loss : 0.008754716\n",
      "Epoch 81. Train Loss: 0.0026128283, Test Loss : 0.021057185\n",
      "Epoch 82. Train Loss: 0.0027206694, Test Loss : 0.019167613\n",
      "Epoch 83. Train Loss: 0.0022235827, Test Loss : 0.009504593\n",
      "Epoch 84. Train Loss: 0.003926437, Test Loss : 0.024011772\n",
      "Epoch 85. Train Loss: 0.0022253708, Test Loss : 0.007218299\n",
      "Epoch 86. Train Loss: 0.0025833899, Test Loss : 0.004775987\n",
      "Epoch 87. Train Loss: 0.0033225894, Test Loss : 0.007829699\n",
      "Epoch 88. Train Loss: 0.0027820794, Test Loss : 0.010777939\n",
      "Epoch 89. Train Loss: 0.003273481, Test Loss : 0.009083693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m☑\u001b[0m 5+55 = 60(60) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 2+9 = 11(11) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 38+9 = 47(47) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 136+950 = 1086(1086) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 13+7 = 19(20) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 6+5 = 11(11) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 2+8 = 110(10) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 5+97 = 101(102) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 159+288 = 437(447) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 76+8 = 94(84) 1/0 0\n",
      "Epoch 90. Train Loss: 0.0024783742, Test Loss : 0.048588507\n",
      "Epoch 91. Train Loss: 0.0018179704, Test Loss : 0.0076959417\n",
      "Epoch 92. Train Loss: 0.0016809956, Test Loss : 0.0071055098\n",
      "Epoch 93. Train Loss: 0.001687593, Test Loss : 0.0075776307\n",
      "Epoch 94. Train Loss: 0.002192769, Test Loss : 0.0075569926\n",
      "Epoch 95. Train Loss: 0.0028593973, Test Loss : 0.0055113\n",
      "Epoch 96. Train Loss: 0.0019556386, Test Loss : 0.016315576\n",
      "Epoch 97. Train Loss: 0.0018179577, Test Loss : 0.008407067\n",
      "Epoch 98. Train Loss: 0.0018593405, Test Loss : 0.006684537\n",
      "Epoch 99. Train Loss: 0.0026867779, Test Loss : 0.008272896\n",
      "\u001b[92m☑\u001b[0m 22+415 = 437(437) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 700+35 = 735(735) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 50+563 = 613(613) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 7+471 = 478(478) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 0+169 = 169(169) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 3+104 = 107(107) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 2+658 = 660(660) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 0+200 = 200(200) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 223+709 = 932(932) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 197+34 = 231(231) 1/0 1\n",
      "Epoch 100. Train Loss: 0.0020205968, Test Loss : 0.005145566\n",
      "Epoch 101. Train Loss: 0.0026295753, Test Loss : 0.0048882943\n",
      "Epoch 102. Train Loss: 0.0030048287, Test Loss : 0.006449866\n",
      "Epoch 103. Train Loss: 0.0018275094, Test Loss : 0.010540764\n",
      "Epoch 104. Train Loss: 0.002390934, Test Loss : 0.018068735\n",
      "Epoch 105. Train Loss: 0.0022031092, Test Loss : 0.007247071\n",
      "Epoch 106. Train Loss: 0.0010099993, Test Loss : 0.0036454848\n",
      "Epoch 107. Train Loss: 0.0008857379, Test Loss : 0.00654452\n",
      "Epoch 108. Train Loss: 0.0002904636, Test Loss : 0.0029559876\n",
      "Epoch 109. Train Loss: 2.374486e-05, Test Loss : 0.0026200626\n",
      "\u001b[92m☑\u001b[0m 59+1 = 60(60) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 7+1 = 9(8) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 6+2 = 8(8) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 258+0 = 258(258) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 5+93 = 98(98) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 67+9 = 75(76) 1/0 0\n",
      "\u001b[91m☒\u001b[0m 73+1 = 85(74) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 28+66 = 94(94) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 74+9 = 95(83) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 87+640 = 727(727) 1/0 1\n",
      "Epoch 110. Train Loss: 1.5835707e-05, Test Loss : 0.0025544153\n",
      "Epoch 111. Train Loss: 1.3304099e-05, Test Loss : 0.0025057183\n",
      "Epoch 112. Train Loss: 1.1721528e-05, Test Loss : 0.0024134347\n",
      "Epoch 113. Train Loss: 1.1005383e-05, Test Loss : 0.0022918025\n",
      "Epoch 114. Train Loss: 1.0015835e-05, Test Loss : 0.002258482\n",
      "Epoch 115. Train Loss: 9.182794e-06, Test Loss : 0.0023296312\n",
      "Epoch 116. Train Loss: 8.831391e-06, Test Loss : 0.002268041\n",
      "Epoch 117. Train Loss: 8.317523e-06, Test Loss : 0.002372062\n",
      "Epoch 118. Train Loss: 8.0869495e-06, Test Loss : 0.0023213655\n",
      "Epoch 119. Train Loss: 7.636606e-06, Test Loss : 0.0023102383\n",
      "\u001b[92m☑\u001b[0m 49+11 = 60(60) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 639+938 = 1577(1577) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 910+6 = 916(916) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 6+849 = 855(855) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 6+9 = 15(15) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 8+8 = 16(16) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 394+2 = 396(396) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 390+7 = 397(397) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 7+6 = 13(13) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 4+61 = 65(65) 1/0 1\n",
      "Epoch 120. Train Loss: 7.2033527e-06, Test Loss : 0.0022668624\n",
      "Epoch 121. Train Loss: 7.220417e-06, Test Loss : 0.0023183571\n",
      "Epoch 122. Train Loss: 6.9340067e-06, Test Loss : 0.0023202333\n",
      "Epoch 123. Train Loss: 6.5902354e-06, Test Loss : 0.0023658653\n",
      "Epoch 124. Train Loss: 6.5477634e-06, Test Loss : 0.0022242053\n",
      "Epoch 125. Train Loss: 6.2808376e-06, Test Loss : 0.0021923569\n",
      "Epoch 126. Train Loss: 6.0700445e-06, Test Loss : 0.0022180602\n",
      "Epoch 127. Train Loss: 5.898955e-06, Test Loss : 0.0022960599\n",
      "Epoch 128. Train Loss: 5.7074826e-06, Test Loss : 0.002368057\n",
      "Epoch 129. Train Loss: 5.4653906e-06, Test Loss : 0.002322154\n",
      "\u001b[92m☑\u001b[0m 3+62 = 65(65) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 6+32 = 38(38) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 19+32 = 51(51) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 1+1 = 2(2) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 328+91 = 419(419) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 96+3 = 100(99) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 1+1 = 2(2) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 550+50 = 600(600) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 4+8 = 12(12) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 923+0 = 923(923) 1/0 1\n",
      "Epoch 130. Train Loss: 5.473869e-06, Test Loss : 0.002289685\n",
      "Epoch 131. Train Loss: 5.419971e-06, Test Loss : 0.0021970987\n",
      "Epoch 132. Train Loss: 5.2108767e-06, Test Loss : 0.002391225\n",
      "Epoch 133. Train Loss: 5.053021e-06, Test Loss : 0.002312398\n",
      "Epoch 134. Train Loss: 4.9089017e-06, Test Loss : 0.0022450401\n",
      "Epoch 135. Train Loss: 4.9356713e-06, Test Loss : 0.0022362093\n",
      "Epoch 136. Train Loss: 4.8174984e-06, Test Loss : 0.0022566773\n",
      "Epoch 137. Train Loss: 4.647999e-06, Test Loss : 0.0022004112\n",
      "Epoch 138. Train Loss: 4.6140367e-06, Test Loss : 0.0024353974\n",
      "Epoch 139. Train Loss: 4.5367965e-06, Test Loss : 0.0022079148\n",
      "\u001b[92m☑\u001b[0m 5+5 = 10(10) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 23+966 = 989(989) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 34+8 = 42(42) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 406+2 = 408(408) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 0+7 = 7(7) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 955+36 = 991(991) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 19+38 = 57(57) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 993+554 = 1547(1547) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 4+0 = 5(4) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 594+713 = 1307(1307) 1/0 1\n",
      "Epoch 140. Train Loss: 4.488869e-06, Test Loss : 0.0021804094\n",
      "Epoch 141. Train Loss: 4.348905e-06, Test Loss : 0.0021833968\n",
      "Epoch 142. Train Loss: 4.174474e-06, Test Loss : 0.0022225818\n",
      "Epoch 143. Train Loss: 4.3009277e-06, Test Loss : 0.00222896\n",
      "Epoch 144. Train Loss: 4.0650893e-06, Test Loss : 0.0021985015\n",
      "Epoch 145. Train Loss: 4.078342e-06, Test Loss : 0.002191837\n",
      "Epoch 146. Train Loss: 3.9004585e-06, Test Loss : 0.0022128378\n",
      "Epoch 147. Train Loss: 3.9179668e-06, Test Loss : 0.0021713194\n",
      "Epoch 148. Train Loss: 3.7978482e-06, Test Loss : 0.0023136074\n",
      "Epoch 149. Train Loss: 3.7817285e-06, Test Loss : 0.002171353\n",
      "\u001b[92m☑\u001b[0m 37+9 = 46(46) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 2+954 = 956(956) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 49+6 = 54(55) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 7+121 = 128(128) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 50+6 = 56(56) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 8+893 = 901(901) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 739+9 = 748(748) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 64+383 = 447(447) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 61+1 = 63(62) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 5+265 = 270(270) 1/0 1\n",
      "Epoch 150. Train Loss: 3.7316292e-06, Test Loss : 0.0022567543\n",
      "Epoch 151. Train Loss: 3.6990373e-06, Test Loss : 0.0022214628\n",
      "Epoch 152. Train Loss: 3.6166982e-06, Test Loss : 0.0022333325\n",
      "Epoch 153. Train Loss: 3.6577878e-06, Test Loss : 0.0022471303\n",
      "Epoch 154. Train Loss: 3.5305247e-06, Test Loss : 0.0022075917\n",
      "Epoch 155. Train Loss: 3.5350204e-06, Test Loss : 0.002205614\n",
      "Epoch 156. Train Loss: 3.4849843e-06, Test Loss : 0.0023276154\n",
      "Epoch 157. Train Loss: 3.3643294e-06, Test Loss : 0.0021653452\n",
      "Epoch 158. Train Loss: 3.2749908e-06, Test Loss : 0.002402566\n",
      "Epoch 159. Train Loss: 3.3504048e-06, Test Loss : 0.0021740538\n",
      "\u001b[92m☑\u001b[0m 10+311 = 321(321) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 84+265 = 349(349) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 902+6 = 908(908) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 91+443 = 534(534) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 43+2 = 45(45) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 70+858 = 928(928) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 52+4 = 57(56) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 177+961 = 1138(1138) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 178+91 = 269(269) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 420+99 = 519(519) 1/0 1\n",
      "Epoch 160. Train Loss: 3.3654578e-06, Test Loss : 0.0022026063\n",
      "Epoch 161. Train Loss: 3.2461123e-06, Test Loss : 0.0021795265\n",
      "Epoch 162. Train Loss: 3.1932664e-06, Test Loss : 0.0021923312\n",
      "Epoch 163. Train Loss: 3.1513491e-06, Test Loss : 0.0022301606\n",
      "Epoch 164. Train Loss: 3.0570102e-06, Test Loss : 0.0021461903\n",
      "Epoch 165. Train Loss: 3.0939666e-06, Test Loss : 0.0021988228\n",
      "Epoch 166. Train Loss: 3.0932608e-06, Test Loss : 0.0021872749\n",
      "Epoch 167. Train Loss: 3.0040696e-06, Test Loss : 0.0023036576\n",
      "Epoch 168. Train Loss: 2.9569737e-06, Test Loss : 0.0023900755\n",
      "Epoch 169. Train Loss: 2.934813e-06, Test Loss : 0.0022339355\n",
      "\u001b[92m☑\u001b[0m 55+187 = 242(242) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 34+563 = 597(597) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 3+2 = 5(5) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 77+127 = 204(204) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 41+374 = 415(415) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 641+45 = 686(686) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 3+54 = 57(57) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 688+919 = 1607(1607) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 762+2 = 764(764) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 7+87 = 94(94) 1/0 1\n",
      "Epoch 170. Train Loss: 2.9257592e-06, Test Loss : 0.0022810926\n",
      "Epoch 171. Train Loss: 2.8509169e-06, Test Loss : 0.002196847\n",
      "Epoch 172. Train Loss: 2.842751e-06, Test Loss : 0.0021901461\n",
      "Epoch 173. Train Loss: 2.8415088e-06, Test Loss : 0.002177908\n",
      "Epoch 174. Train Loss: 2.854431e-06, Test Loss : 0.0021548136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175. Train Loss: 2.749527e-06, Test Loss : 0.002226091\n",
      "Epoch 176. Train Loss: 2.7499957e-06, Test Loss : 0.0022506018\n",
      "Epoch 177. Train Loss: 2.6849543e-06, Test Loss : 0.0021794266\n",
      "Epoch 178. Train Loss: 2.6770176e-06, Test Loss : 0.0021291904\n",
      "Epoch 179. Train Loss: 2.646597e-06, Test Loss : 0.0022371626\n",
      "\u001b[91m☒\u001b[0m 103+7 = 100(110) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 9+1 = 10(10) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 933+85 = 1018(1018) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 7+721 = 728(728) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 8+0 = 9(8) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 67+3 = 70(70) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 65+9 = 74(74) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 137+449 = 586(586) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 0+566 = 566(566) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 769+12 = 781(781) 1/0 1\n",
      "Epoch 180. Train Loss: 2.6078012e-06, Test Loss : 0.0021729602\n",
      "Epoch 181. Train Loss: 2.5343752e-06, Test Loss : 0.0025367003\n",
      "Epoch 182. Train Loss: 2.5487607e-06, Test Loss : 0.0022060536\n",
      "Epoch 183. Train Loss: 2.5458949e-06, Test Loss : 0.0022452483\n",
      "Epoch 184. Train Loss: 2.495283e-06, Test Loss : 0.0021666258\n",
      "Epoch 185. Train Loss: 2.4933092e-06, Test Loss : 0.0021778943\n",
      "Epoch 186. Train Loss: 2.4691324e-06, Test Loss : 0.0022189503\n",
      "Epoch 187. Train Loss: 2.3982773e-06, Test Loss : 0.0023041128\n",
      "Epoch 188. Train Loss: 2.422763e-06, Test Loss : 0.002252769\n",
      "Epoch 189. Train Loss: 2.4607214e-06, Test Loss : 0.002178511\n",
      "\u001b[91m☒\u001b[0m 4+0 = 5(4) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 63+554 = 617(617) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 523+8 = 531(531) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 10+25 = 35(35) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 803+3 = 806(806) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 24+0 = 24(24) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 84+842 = 926(926) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 81+7 = 97(88) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 77+21 = 98(98) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 4+4 = 8(8) 1/0 1\n",
      "Epoch 190. Train Loss: 2.3781258e-06, Test Loss : 0.0022455864\n",
      "Epoch 191. Train Loss: 2.3354683e-06, Test Loss : 0.0021850667\n",
      "Epoch 192. Train Loss: 2.4391563e-06, Test Loss : 0.0023787548\n",
      "Epoch 193. Train Loss: 2.348065e-06, Test Loss : 0.0022201403\n",
      "Epoch 194. Train Loss: 2.2948645e-06, Test Loss : 0.002181473\n",
      "Epoch 195. Train Loss: 2.2144786e-06, Test Loss : 0.0021898986\n",
      "Epoch 196. Train Loss: 2.2572528e-06, Test Loss : 0.0022315388\n",
      "Epoch 197. Train Loss: 2.2462511e-06, Test Loss : 0.0022075635\n",
      "Epoch 198. Train Loss: 2.2763666e-06, Test Loss : 0.002225996\n",
      "Epoch 199. Train Loss: 2.1625658e-06, Test Loss : 0.0022486246\n",
      "\u001b[92m☑\u001b[0m 9+6 = 15(15) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 399+706 = 1105(1105) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 84+53 = 137(137) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 32+166 = 198(198) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 588+70 = 658(658) 1/0 1\n",
      "\u001b[91m☒\u001b[0m 76+33 = 119(109) 1/0 0\n",
      "\u001b[92m☑\u001b[0m 94+4 = 98(98) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 773+91 = 864(864) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 17+88 = 105(105) 1/0 1\n",
      "\u001b[92m☑\u001b[0m 3+20 = 23(23) 1/0 1\n",
      "Epoch 200. Train Loss: 2.1855346e-06, Test Loss : 0.0023719282\n"
     ]
    }
   ],
   "source": [
    "epochs = 201\n",
    "\n",
    "tot_test_loss = []\n",
    "tot_train_loss = []\n",
    "for e in range(epochs):\n",
    "    train_loss = []\n",
    "    for i, (x_data, y_data, z_data) in enumerate(tr_data_iterator):\n",
    "        x_data = x_data.as_in_context(ctx).astype('float32')\n",
    "        y_data = y_data.as_in_context(ctx).astype('float32')\n",
    "        z_data = z_data.as_in_context(ctx).astype('float32')\n",
    "        \n",
    "        with autograd.record():\n",
    "            z_output = model(x_data, y_data)\n",
    "            loss_ = loss(z_output, z_data)\n",
    "        loss_.backward()\n",
    "        trainer.step(x_data.shape[0])\n",
    "        curr_loss = nd.mean(loss_).asscalar()\n",
    "        train_loss.append(curr_loss)\n",
    "        \n",
    "    if e % 10 == 0:\n",
    "        q, y = gen_n_test(10)\n",
    "        for i in range(10):\n",
    "            with autograd.predict_mode():\n",
    "                p = model.calculation(q[i], char_indices, indices_char).strip()\n",
    "                iscorr = 1 if p == y[i] else 0\n",
    "                if iscorr == 1:\n",
    "                    print(colors.ok + '☑' + colors.close, end=' ')\n",
    "                else:\n",
    "                    print(colors.fail + '☒' + colors.close, end=' ')\n",
    "                print(\"{} = {}({}) 1/0 {}\".format(q[i], p, y[i], str(iscorr) ))\n",
    "    #caculate test loss\n",
    "    test_loss = calculate_loss(model, te_data_iterator, loss_obj = loss, ctx=ctx) \n",
    "\n",
    "    print(\"Epoch %s. Train Loss: %s, Test Loss : %s\" % (e, np.mean(train_loss), test_loss))    \n",
    "    tot_test_loss.append(test_loss)\n",
    "    tot_train_loss.append(np.mean(train_loss))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
