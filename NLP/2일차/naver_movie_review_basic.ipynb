{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네이버 무비 리뷰 분류 모형 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "약 2만건의 네이버 무비 리뷰 데이터를 활용해 Sentiment Classification을 하는 모형을 만들어 본다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tagger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a3142fc7c49d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmecab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMecab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/konlpy/tag/_mecab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dicpath)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdicpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/usr/local/lib/mecab/dic/mecab-ko-dic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-d %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdicpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s/data/tagset/mecab.json'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstallpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tagger' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from konlpy.tag import Mecab\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet import gluon, autograd\n",
    "import gluonnlp as nlp\n",
    "from mxnet import nd \n",
    "import mxnet as mx\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab 생성 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습셋 전체의 문장을 이용해 전처리를 한 뒤, Vocab을 생성한다. `Mecab` 형태소 분석기로 형태소만으로 Vocab을 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating = pd.read_csv(\"ratings.txt\",sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
       "2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
       "3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
       "4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [(d, l) for d,l in zip(rating['document'], rating['label'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_clip = nlp.data.PadSequence(seq_len, pad_val=\"<pad>\")\n",
    "\n",
    "def preprocess(data):\n",
    "    comment, label = data\n",
    "    morphs = mecab.morphs(str(comment).strip())\n",
    "    return(length_clip(morphs), label)\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    start = time.time()\n",
    "    with mp.Pool() as pool:\n",
    "        dataset = gluon.data.SimpleDataset(pool.map(preprocess, dataset))\n",
    "    end = time.time()\n",
    "    print('Done! Tokenizing Time={:.2f}s, #Sentences={}'\n",
    "          .format(end - start, len(dataset)))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Tokenizing Time=9.78s, #Sentences=200000\n"
     ]
    }
   ],
   "source": [
    "preprocessed  = preprocess_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫번째 문장의 첫 11개 토큰 출력  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['어릴', '때', '보', '고', '지금', '다시', '봐도', '재밌', '어요', 'ㅋㅋ', '<pad>']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed[0][0][:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습셋 전체로 토큰 빈도를 생성 `counter`를 만들고, `vocab`을 생성. \n",
    "문장 생성이나 seq2seq가 아니기 때문에 `bos_token`, `eos_token` 표현은 생략 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = nlp.data.count_tokens(itertools.chain.from_iterable([c for c, _ in preprocessed]))\n",
    "\n",
    "vocab = nlp.Vocab(counter,bos_token=None, eos_token=None, min_freq=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습셋 생성 \n",
    "\n",
    "토큰을 `index`로 변환 하여 학습을 위한 데이터로 변환 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_encoded  = [(vocab[data], label)  for data, label in preprocessed ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = nlp.data.train_valid_split(preprocessed_encoded, valid_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Stack(),\n",
    "                                      nlp.data.batchify.Stack('float32'))\n",
    "\n",
    "train_dataloader  = gluon.data.DataLoader(train, batch_size=60, batchify_fn=batchify_fn, shuffle=True, last_batch='discard')\n",
    "test_dataloader  = gluon.data.DataLoader(test, batch_size=60, batchify_fn=batchify_fn, shuffle=True, last_batch='discard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 정의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentClassificationModel(gluon.HybridBlock):\n",
    "    def __init__(self, vocab_size, num_embed, **kwargs):\n",
    "        super(SentClassificationModel, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.embed = nn.Embedding(input_dim=vocab_size, output_dim=num_embed)\n",
    "            self.drop = nn.Dropout(0.3)\n",
    "            self.fc = nn.Dense(100, activation='relu')\n",
    "            self.out = nn.Dense(2)  \n",
    "    def hybrid_forward(self, F ,inputs): # F == ndarray ... symbol\n",
    "        em_out = self.drop(self.embed(inputs))\n",
    "        fc_out = self.fc(em_out) \n",
    "        return(self.out(fc_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu()\n",
    "\n",
    "#모형 인스턴스 생성 및 트래이너, loss 정의 \n",
    "model = SentClassificationModel(vocab_size = len(vocab.idx_to_token), num_embed=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.initialize(mx.init.Xavier(),ctx=ctx)\n",
    "model.hybridize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________________________________________________\n",
      "Layer (type)                                        Output Shape            Param #     Previous Layer                  \n",
      "========================================================================================================================\n",
      "data(null)                                          30                      0                                           \n",
      "________________________________________________________________________________________________________________________\n",
      "sentclassificationmodel0_embedding0_fwd(Embedding)  30x50                   0           data                            \n",
      "________________________________________________________________________________________________________________________\n",
      "sentclassificationmodel0_dropout0_fwd(Dropout)      30x50                   0           sentclassificationmodel0_embeddi\n",
      "________________________________________________________________________________________________________________________\n",
      "sentclassificationmodel0_dense0_fwd(FullyConnected) 100                     3100        sentclassificationmodel0_dropout\n",
      "________________________________________________________________________________________________________________________\n",
      "sentclassificationmodel0_dense0_relu_fwd(Activation)100                     0           sentclassificationmodel0_dense0_\n",
      "________________________________________________________________________________________________________________________\n",
      "sentclassificationmodel0_dense1_fwd(FullyConnected) 2                       202         sentclassificationmodel0_dense0_\n",
      "========================================================================================================================\n",
      "Total params: 3302\n",
      "________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mx.viz.print_summary(\n",
    "    model(mx.sym.var('data')), \n",
    "    shape={'data':(1,30)}, #set your shape here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(model.collect_params(), 'Rmsprop')\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, data_iter, ctx=ctx):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iter):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = model(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return(acc.get()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(model, data_iter, loss_obj, ctx=ctx):\n",
    "    test_loss = []\n",
    "    for i, (te_data, te_label) in enumerate(data_iter):\n",
    "        te_data = te_data.as_in_context(ctx)\n",
    "        te_label = te_label.as_in_context(ctx)\n",
    "        te_output = model(te_data)\n",
    "        loss_te = loss_obj(te_output, te_label)\n",
    "        curr_loss = nd.mean(loss_te).asscalar()\n",
    "        test_loss.append(curr_loss)\n",
    "    return(np.mean(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:12<00:00, 233.85it/s]\n",
      "  1%|          | 23/3000 [00:00<00:13, 228.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train Loss: 0.40141717, Test Loss : 0.3687849, Test Accuracy : 0.8377877877877878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:12<00:00, 237.82it/s]\n",
      "  1%|          | 24/3000 [00:00<00:12, 238.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Train Loss: 0.35516727, Test Loss : 0.3646571, Test Accuracy : 0.8402902902902903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:12<00:00, 237.84it/s]\n",
      "  1%|          | 24/3000 [00:00<00:12, 231.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2. Train Loss: 0.33922815, Test Loss : 0.35330832, Test Accuracy : 0.8455955955955956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:12<00:00, 238.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3. Train Loss: 0.3261278, Test Loss : 0.35240707, Test Accuracy : 0.8482982982982983\n"
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "\n",
    "\n",
    "tot_test_loss = []\n",
    "tot_test_accu = []\n",
    "tot_train_loss = []\n",
    "for e in range(epochs):\n",
    "    train_loss = []\n",
    "    #batch training \n",
    "    for i, (data, label) in enumerate(tqdm(train_dataloader)):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = model(data)\n",
    "            loss_ = loss(output, label)\n",
    "            loss_.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "        curr_loss = nd.mean(loss_).asscalar()\n",
    "        train_loss.append(curr_loss)\n",
    "\n",
    "    #caculate test loss\n",
    "    test_loss = calculate_loss(model, test_dataloader, loss_obj = loss, ctx=ctx) \n",
    "    test_accu = evaluate_accuracy(model, test_dataloader,  ctx=ctx)\n",
    "\n",
    "    print(\"Epoch %s. Train Loss: %s, Test Loss : %s, Test Accuracy : %s\" % (e, np.mean(train_loss), test_loss, test_accu))    \n",
    "    tot_test_loss.append(test_loss)\n",
    "    tot_train_loss.append(np.mean(train_loss))\n",
    "    tot_test_accu.append(test_accu)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO \n",
    "\n",
    "- 테스트 정확도를 87% 이상 올려본다.(Optimizer, RNN, Convolution, 데이터 전처리 방식 변경(명사만 사용?), ...) \n",
    "- 학습된 임베딩 레이어를 기반으로 단어간의 유사도를 구해본다. \n",
    "- 토큰이 아닌 char 기반으로 학습하면 어떨까? 성능이 좋아지나? \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
