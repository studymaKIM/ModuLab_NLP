{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity and Intent Classification \n",
    "\n",
    "- 인텐트와 엔티티의 멀티테스크 러닝 학습을 수행한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet import gluon, autograd\n",
    "import gluonnlp as nlp\n",
    "from mxnet import nd \n",
    "import mxnet as mx\n",
    "import time\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(\"data/trainset.txt\",names=['intent', 'entity', 'sentence'], sep='\\t')\n",
    "validation_raw = pd.read_csv(\"data/test_hidden.txt\",names=['intent', 'entity', 'sentence'], sep='\\t')\n",
    "#validation_raw = pd.read_csv(\"data/validation.txt\",names=['intent', 'entity', 'sentence'], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>entity</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>area</td>\n",
       "      <td>EECCCCCCCCCCCCCCCCCCC</td>\n",
       "      <td>자강의 면적은 얼마 정도되는지 알려줄래</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>birth_date</td>\n",
       "      <td>CCCCCCCCCCCCEEECCCCCCCCCCCC</td>\n",
       "      <td>WIKI PEDIA로 변재일 생년월일을 알고 싶어</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age</td>\n",
       "      <td>EEEEEEEEEEECCCCCCCCCCCCCCCCC</td>\n",
       "      <td>남쪽 물고기자리 알파 나이가 위키백과사전으로 얼마야</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>length</td>\n",
       "      <td>EEEECCCCCCCCCCCCCCCCCC</td>\n",
       "      <td>삼양터널의 총 길이 위키백과사전에서 뭐야</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>birth_place</td>\n",
       "      <td>EEEEEECCCCCCCCCCC</td>\n",
       "      <td>코니 윌리스의 태어난 곳은 뭐지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>weight</td>\n",
       "      <td>CCCCCCCCCCCCEEEECCCCCCCCCCCCC</td>\n",
       "      <td>WIKI백과사전 검색 AA12의 무게가 얼만지 찾아봐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>definition</td>\n",
       "      <td>CCCCCCCCCCCCCEEECCCCCCCC</td>\n",
       "      <td>WIKIPEDIA백과로 라이프 찾아서 말해줘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>height</td>\n",
       "      <td>EEEEEEEECCCCCCCCCCCCCCCCCCC</td>\n",
       "      <td>송파 헬리오시티 구조물 높이 위키 피디아에서 뭐야</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>birth_date</td>\n",
       "      <td>CCCEEEEEECCCCCCCCCCCCCCC</td>\n",
       "      <td>검색 HLKVAM 언제 출생했는지를 검색해라</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>height</td>\n",
       "      <td>CCCCCCCCEEEEEECCCCCCCC</td>\n",
       "      <td>위키 피디아에 푸조 508 전고가 몇이야</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        intent                         entity                       sentence\n",
       "0         area          EECCCCCCCCCCCCCCCCCCC          자강의 면적은 얼마 정도되는지 알려줄래\n",
       "1   birth_date    CCCCCCCCCCCCEEECCCCCCCCCCCC    WIKI PEDIA로 변재일 생년월일을 알고 싶어\n",
       "2          age   EEEEEEEEEEECCCCCCCCCCCCCCCCC   남쪽 물고기자리 알파 나이가 위키백과사전으로 얼마야\n",
       "3       length         EEEECCCCCCCCCCCCCCCCCC         삼양터널의 총 길이 위키백과사전에서 뭐야\n",
       "4  birth_place              EEEEEECCCCCCCCCCC              코니 윌리스의 태어난 곳은 뭐지\n",
       "5       weight  CCCCCCCCCCCCEEEECCCCCCCCCCCCC  WIKI백과사전 검색 AA12의 무게가 얼만지 찾아봐\n",
       "6   definition       CCCCCCCCCCCCCEEECCCCCCCC       WIKIPEDIA백과로 라이프 찾아서 말해줘\n",
       "7       height    EEEEEEEECCCCCCCCCCCCCCCCCCC    송파 헬리오시티 구조물 높이 위키 피디아에서 뭐야\n",
       "8   birth_date       CCCEEEEEECCCCCCCCCCCCCCC       검색 HLKVAM 언제 출생했는지를 검색해라\n",
       "9       height         CCCCCCCCEEEEEECCCCCCCC         위키 피디아에 푸조 508 전고가 몇이야"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intent Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = [(s,i,e) for i,e,s in zip(train_raw['intent'], train_raw['entity'], train_raw['sentence'])]\n",
    "valid_dataset = [(s,i,e) for i,e,s in zip(validation_raw['intent'], validation_raw['entity'], validation_raw['sentence'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 32\n",
    "\n",
    "length_clip = nlp.data.PadSequence(seq_len, pad_val=\"<pad>\")\n",
    "\n",
    "def preprocess(data):\n",
    "    sent, intent, entity = data\n",
    "    char_sent = list(str(sent))\n",
    "    char_entity = list(str(entity))\n",
    "    char_intent = str(intent)\n",
    "    sent_len = len(sent) if len(sent) < seq_len else seq_len\n",
    "    return(length_clip(char_sent), sent_len, char_intent, length_clip(char_entity))\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    start = time.time()\n",
    "    with mp.Pool() as pool:\n",
    "        dataset = gluon.data.SimpleDataset(pool.map(preprocess, dataset))\n",
    "    end = time.time()\n",
    "    print('Done! Tokenizing Time={:.2f}s, #Sentences={}'\n",
    "          .format(end - start, len(dataset)))\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Tokenizing Time=0.15s, #Sentences=9000\n",
      "Done! Tokenizing Time=0.13s, #Sentences=1000\n"
     ]
    }
   ],
   "source": [
    "train_preprocessed  = preprocess_dataset(train_dataset)\n",
    "valid_preprocessed  = preprocess_dataset(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_sent   = nlp.data.count_tokens(itertools.chain.from_iterable([c for c,_,_,_ in train_preprocessed]))\n",
    "counter_intent = nlp.data.count_tokens([c for _,_,c,_ in train_preprocessed])\n",
    "counter_entity = nlp.data.count_tokens(itertools.chain.from_iterable([c for _,_,_,c in train_preprocessed]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'age': 900,\n",
       "         'area': 900,\n",
       "         'belong_to': 900,\n",
       "         'birth_date': 900,\n",
       "         'birth_place': 900,\n",
       "         'definition': 900,\n",
       "         'height': 900,\n",
       "         'length': 900,\n",
       "         'weight': 900,\n",
       "         'width': 900})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sent = nlp.Vocab(counter_sent, bos_token=None, eos_token=None, min_freq=15)\n",
    "vocab_intent = nlp.Vocab(counter_intent, bos_token=None, eos_token=None, unknown_token=None, padding_token=None)\n",
    "vocab_entity = nlp.Vocab(counter_entity, bos_token=None, eos_token=None, unknown_token=None, padding_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<unk>', '<pad>', ' ', 'I', '이', '색', '검', '의', '지', '아'],\n",
       " ['age',\n",
       "  'area',\n",
       "  'belong_to',\n",
       "  'birth_date',\n",
       "  'birth_place',\n",
       "  'definition',\n",
       "  'height',\n",
       "  'length',\n",
       "  'weight',\n",
       "  'width'],\n",
       " ['C', '<pad>', 'E'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_sent.idx_to_token[:10], vocab_intent.idx_to_token[:10], vocab_entity.idx_to_token[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessed_encoded  = [(vocab_sent[sent], length ,vocab_intent[intent], vocab_entity[entity])  \n",
    "                               for sent, length, intent, entity  in train_preprocessed ]\n",
    "valid  = [(vocab_sent[sent], length ,vocab_intent[intent], vocab_entity[entity])  for sent, length, intent, entity in valid_preprocessed ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = nlp.data.train_valid_split(train_preprocessed_encoded, valid_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbatch = 30\n",
    "batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Stack(),\n",
    "                                      nlp.data.batchify.Stack('float32'),\n",
    "                                      nlp.data.batchify.Stack(),\n",
    "                                      nlp.data.batchify.Stack())\n",
    "\n",
    "train_dataloader  = gluon.data.DataLoader(train, batch_size=nbatch, batchify_fn=batchify_fn, shuffle=True)\n",
    "test_dataloader  = gluon.data.DataLoader(test, batch_size=nbatch, batchify_fn=batchify_fn, shuffle=True)\n",
    "valid_dataloader  = gluon.data.DataLoader(valid, batch_size=nbatch, batchify_fn=batchify_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델링 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentEntityMultiTask(gluon.HybridBlock):\n",
    "    def __init__(self, vocab_size, intent_class_size, entity_class_size, num_embed, seq_len, hidden_size, **kwargs):\n",
    "        super(IntentEntityMultiTask, self).__init__(**kwargs)\n",
    "        self.seq_len = seq_len\n",
    "        self.hidden_size = hidden_size \n",
    "        self.intent_class_size = intent_class_size\n",
    "        self.entity_class_size = entity_class_size\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.embed = nn.Embedding(input_dim=vocab_size, output_dim=num_embed)\n",
    "            self.dropout = nn.Dropout(0.3)\n",
    "            self.bilstm = rnn.LSTM(self.hidden_size, dropout=0.3)\n",
    "            self.out_intent = nn.Dense(self.intent_class_size)\n",
    "            self.bilstm_last = rnn.LSTM(self.hidden_size, dropout=0.3, bidirectional=True)\n",
    "            \n",
    "            self.dense_en = nn.Dense(50, flatten=False)\n",
    "            self.out_entity = nn.Dense(self.entity_class_size, flatten=False)\n",
    "            \n",
    "    def hybrid_forward(self, F ,inputs, length):\n",
    "        em_out = self.dropout(self.embed(inputs))\n",
    "        bilstm = self.bilstm(em_out)\n",
    "        masked_encoded_intent = F.SequenceMask(bilstm,\n",
    "                                sequence_length=length,\n",
    "                                use_sequence_length=True)\n",
    "        agg_intent = F.broadcast_div(F.sum(masked_encoded_intent, axis=0), \n",
    "                            F.expand_dims(length, axis=1))\n",
    "        intent = self.out_intent(agg_intent) \n",
    "        \n",
    "        #start entity\n",
    "        bilstm_last = self.bilstm_last(em_out)\n",
    "        \n",
    "        masked_encoded_entity = F.SequenceMask(bilstm_last,\n",
    "                                sequence_length=length,\n",
    "                                use_sequence_length=True).transpose((1,0,2))\n",
    "        fc_entity = self.dense_en(masked_encoded_entity)\n",
    "        entity = self.out_entity(fc_entity) \n",
    "        return(intent, entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](model-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu()\n",
    "\n",
    "model = IntentEntityMultiTask(vocab_size = len(vocab_sent.idx_to_token), \n",
    "                              intent_class_size=len(vocab_intent.idx_to_token), \n",
    "                              entity_class_size=len(vocab_entity.idx_to_token), num_embed=50, seq_len=seq_len, hidden_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.initialize(mx.initializer.Xavier(), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(model.collect_params(),\"Adam\")\n",
    "loss = gluon.loss.SoftmaxCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.hybridize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IntentEntityMultiTask(\n",
      "  (out_entity): Dense(None -> 3, linear)\n",
      "  (out_intent): Dense(None -> 10, linear)\n",
      "  (embed): Embedding(481 -> 50, float32)\n",
      "  (dropout): Dropout(p = 0.3, axes=())\n",
      "  (dense_en): Dense(None -> 50, linear)\n",
      "  (bilstm): LSTM(None -> 30, TNC, dropout=0.3)\n",
      "  (bilstm_last): LSTM(None -> 30, TNC, dropout=0.3, bidirectional)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, data_iter, ctx=ctx):\n",
    "    acc_intent = mx.metric.Accuracy()\n",
    "    corrected = 0\n",
    "    n = 0\n",
    "    for i, (data, length, intent, entity) in enumerate(data_iter):\n",
    "        data = data.as_in_context(ctx)\n",
    "        intent = intent.as_in_context(ctx)\n",
    "        entity = entity.as_in_context(ctx)\n",
    "        length = length.as_in_context(ctx)\n",
    "        intent_output, entity_output = model(data.T, length)\n",
    "        intent_pred = nd.argmax(intent_output, axis=1)\n",
    "        acc_intent.update(preds=intent_pred, labels=intent)\n",
    "        entity_pred = nd.argmax(entity_output, axis=2)\n",
    "        tf = entity_pred.astype('int64') == entity\n",
    "        for i in range(length.shape[0]):\n",
    "            l = int(length[i].asscalar())\n",
    "            corrected += nd.sum(tf[i][:l]).asscalar() == l\n",
    "            n += 1\n",
    "    return(acc_intent.get()[1], corrected/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(model, data_iter, loss_obj, ctx=ctx):\n",
    "    intent_loss = []\n",
    "    entity_loss = []\n",
    "    for i, (te_data, te_length, te_intent, te_entity) in enumerate(data_iter):\n",
    "        te_data = te_data.as_in_context(ctx)\n",
    "        te_entity = te_entity.as_in_context(ctx)\n",
    "        te_intent = te_intent.as_in_context(ctx)\n",
    "        te_length = te_length.as_in_context(ctx)\n",
    "        intent_output, entity_output = model(te_data.T, te_length)\n",
    "        loss_int = loss_obj(intent_output, te_intent)\n",
    "        curr_loss_int = nd.mean(loss_int).asscalar()\n",
    "        intent_loss.append(curr_loss_int)\n",
    "        loss_ent = loss_obj(entity_output, te_entity)\n",
    "        curr_loss_ent = nd.mean(loss_ent).asscalar()\n",
    "        entity_loss.append(curr_loss_ent)\n",
    "    return(np.mean(intent_loss), np.mean(entity_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:01<00:00, 188.26it/s]\n",
      "  6%|▋         | 17/270 [00:00<00:01, 163.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train Loss: (1.1306442, 0.045056757), Test Loss : (1.1324457, 0.04624194), Test Accuracy : (0.7777777777777778, 0.7711111111111111), Train Accuracy : (0.7785185185185185, 0.7685185185185185) : Valid Accuracy : (0.676, 0.641)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:01<00:00, 179.87it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 193.02it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 190.53it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 191.45it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 185.98it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 188.84it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 187.27it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 188.98it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 179.83it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 191.66it/s]\n",
      "  7%|▋         | 20/270 [00:00<00:01, 197.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10. Train Loss: (0.03729426, 0.0034014084), Test Loss : (0.06959214, 0.005860069), Test Accuracy : (0.9877777777777778, 0.9677777777777777), Train Accuracy : (0.9974074074074074, 0.977037037037037) : Valid Accuracy : (0.846, 0.924)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:01<00:00, 190.61it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 189.35it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 187.37it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 190.48it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 190.43it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 181.15it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 184.80it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 191.92it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 185.29it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 190.29it/s]\n",
      "  6%|▌         | 16/270 [00:00<00:01, 159.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20. Train Loss: (0.011029284, 0.0017870525), Test Loss : (0.03375423, 0.0058157165), Test Accuracy : (0.9922222222222222, 0.9733333333333334), Train Accuracy : (0.9991358024691358, 0.9854320987654321) : Valid Accuracy : (0.839, 0.922)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:01<00:00, 186.84it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 182.28it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 188.72it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 184.81it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 189.76it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 184.54it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 189.72it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 186.03it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 187.88it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 191.77it/s]\n",
      "  7%|▋         | 18/270 [00:00<00:01, 172.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30. Train Loss: (0.0037759773, 0.00064132095), Test Loss : (0.023643184, 0.004492641), Test Accuracy : (0.9955555555555555, 0.9766666666666667), Train Accuracy : (1.0, 0.9950617283950617) : Valid Accuracy : (0.852, 0.935)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:01<00:00, 183.02it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 189.03it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 185.15it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 187.53it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 186.19it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 184.28it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 186.70it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 185.89it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 185.35it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 184.85it/s]\n",
      "  7%|▋         | 18/270 [00:00<00:01, 176.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40. Train Loss: (0.0017887043, 0.00026370824), Test Loss : (0.018721739, 0.0044044647), Test Accuracy : (0.9955555555555555, 0.9811111111111112), Train Accuracy : (1.0, 0.9979012345679013) : Valid Accuracy : (0.844, 0.929)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:01<00:00, 183.79it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 184.49it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 181.98it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 188.84it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 182.81it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 184.42it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 190.78it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 188.87it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 189.25it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 187.68it/s]\n",
      "  7%|▋         | 20/270 [00:00<00:01, 195.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50. Train Loss: (0.00090198487, 0.00013875663), Test Loss : (0.020827971, 0.003547297), Test Accuracy : (0.9966666666666667, 0.9822222222222222), Train Accuracy : (1.0, 0.9992592592592593) : Valid Accuracy : (0.845, 0.914)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:01<00:00, 182.18it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 186.77it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 189.63it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 187.83it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 190.59it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 187.32it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 187.42it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 189.50it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 186.80it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 186.94it/s]\n",
      "  7%|▋         | 20/270 [00:00<00:01, 195.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60. Train Loss: (0.000828967, 9.077938e-05), Test Loss : (0.008679804, 0.0045302208), Test Accuracy : (0.9977777777777778, 0.9822222222222222), Train Accuracy : (1.0, 0.9993827160493827) : Valid Accuracy : (0.854, 0.94)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:01<00:00, 185.46it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 186.97it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 190.38it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 187.48it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 187.26it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 188.51it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 184.71it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 189.23it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 187.00it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 185.16it/s]\n",
      "  7%|▋         | 20/270 [00:00<00:01, 192.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70. Train Loss: (0.00036503948, 2.7286307e-05), Test Loss : (0.0069011147, 0.002193475), Test Accuracy : (0.9977777777777778, 0.9877777777777778), Train Accuracy : (1.0, 0.9998765432098765) : Valid Accuracy : (0.862, 0.915)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:01<00:00, 185.39it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 186.09it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 185.57it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 189.87it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 182.50it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 186.34it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 188.67it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 183.65it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 186.69it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 186.38it/s]\n",
      "  6%|▋         | 17/270 [00:00<00:01, 166.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80. Train Loss: (0.0003008305, 1.7000959e-05), Test Loss : (0.015655918, 0.0034545886), Test Accuracy : (0.9966666666666667, 0.9822222222222222), Train Accuracy : (1.0, 0.9998765432098765) : Valid Accuracy : (0.862, 0.943)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:01<00:00, 185.59it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 183.07it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 185.73it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 188.95it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 187.92it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 187.04it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 186.51it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 187.14it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 188.63it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 187.24it/s]\n",
      "  7%|▋         | 20/270 [00:00<00:01, 194.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90. Train Loss: (0.00017531229, 8.08034e-06), Test Loss : (0.012422423, 0.0017884199), Test Accuracy : (0.9977777777777778, 0.9933333333333333), Train Accuracy : (1.0, 1.0) : Valid Accuracy : (0.846, 0.946)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 230/270 [00:01<00:00, 185.55it/s]"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "\n",
    "\n",
    "tot_test_loss = []\n",
    "tot_test_accu = []\n",
    "tot_train_loss = []\n",
    "tot_train_accu = []\n",
    "tot_valid_accu = [] \n",
    "for e in range(epochs):\n",
    "    #batch training \n",
    "    for i, (data, length, intent, entity) in enumerate(tqdm(train_dataloader)):\n",
    "        data = data.as_in_context(ctx)\n",
    "        intent = intent.as_in_context(ctx)\n",
    "        entity = entity.as_in_context(ctx)\n",
    "        length = length.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            out_intent, out_entity = model(data.T, length)\n",
    "            loss_intent = loss(out_intent, intent)\n",
    "            loss_eitity = loss(out_entity, entity)\n",
    "            loss_ = loss_intent * 0.4 + loss_eitity * 0.6\n",
    "            loss_.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "    #caculate test loss\n",
    "    if e % 10 == 0: \n",
    "        test_loss = calculate_loss(model, test_dataloader, loss_obj = loss, ctx=ctx) \n",
    "        train_loss = calculate_loss(model, train_dataloader, loss_obj = loss, ctx=ctx) \n",
    "        test_accu = evaluate_accuracy(model, test_dataloader,  ctx=ctx)\n",
    "        train_accu = evaluate_accuracy(model, train_dataloader,  ctx=ctx)\n",
    "        valid_accu = evaluate_accuracy(model, valid_dataloader,  ctx=ctx)\n",
    "\n",
    "        print(\"Epoch %s. Train Loss: %s, Test Loss : %s,\" \\\n",
    "        \" Test Accuracy : %s,\" \\\n",
    "        \" Train Accuracy : %s : Valid Accuracy : %s\" % (e, train_loss, test_loss, test_accu, train_accu, valid_accu))    \n",
    "        tot_test_loss.append(test_loss)\n",
    "        tot_train_loss.append(train_loss)\n",
    "        tot_test_accu.append(test_accu)\n",
    "        tot_train_accu.append(train_accu)\n",
    "        tot_valid_accu.append(valid_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.676, 0.641),\n",
       " (0.846, 0.924),\n",
       " (0.839, 0.922),\n",
       " (0.852, 0.935),\n",
       " (0.844, 0.929),\n",
       " (0.845, 0.914),\n",
       " (0.854, 0.94),\n",
       " (0.862, 0.915),\n",
       " (0.862, 0.943),\n",
       " (0.846, 0.946),\n",
       " (0.845, 0.95),\n",
       " (0.844, 0.942),\n",
       " (0.822, 0.913),\n",
       " (0.858, 0.942),\n",
       " (0.838, 0.935),\n",
       " (0.844, 0.957),\n",
       " (0.855, 0.953),\n",
       " (0.852, 0.955),\n",
       " (0.858, 0.947),\n",
       " (0.851, 0.955)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_valid_accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model export and Visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export(\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Netron으로 네트워크 시각화 \n",
    "\n",
    "- https://lutzroeder.github.io/netron/\n",
    "- 저장된 `model-symbol.json`을 입력해 시각화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = gluon.nn.SymbolBlock.imports(\"model-symbol.json\", ['data0', 'data1'], \n",
    "                                          \"model-0000.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entitytag(sent):\n",
    "    sent_len = len(sent)\n",
    "    coded_sent = vocab_sent[length_clip(list(sent))]\n",
    "    co = nd.array(coded_sent).expand_dims(axis=1)\n",
    "    _, ret_code = load_model(co, nd.array([sent_len,]))\n",
    "    ret_seq = vocab_entity.to_tokens(ret_code.argmax(axis=2)[0].asnumpy().astype('int').tolist())\n",
    "    return(''.join(ret_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EEECCCCCCCCCC<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_entitytag(\"아이유가 신곡을 낸 이유\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intent(sent):\n",
    "    sent_len = len(sent)\n",
    "    coded_sent = vocab_sent[length_clip(list(sent))]\n",
    "    co = nd.array(coded_sent).expand_dims(axis=1)\n",
    "    ret_code,_ = load_model(co, nd.array([sent_len,]))\n",
    "    ret_seq = vocab_intent.to_tokens(ret_code.argmax(axis=1).asnumpy().astype('int').tolist())\n",
    "    return(''.join(ret_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'definition'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_intent(\"모두의 연구소에 대해서 찾아줘\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "- 개별 Intent와 Entity 모형을 하나의 모형으로 구축해본다. (Multi-Task Learning) \n",
    "  - 분류 성능이 좋아지는가? 학습 수렴 속도는 어떠한가?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
