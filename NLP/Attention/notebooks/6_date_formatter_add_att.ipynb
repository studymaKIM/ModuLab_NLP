{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from mxnet import nd, autograd, gluon\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet.ndarray.linalg import gemm2\n",
    "import datetime\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n"
     ]
    }
   ],
   "source": [
    "def padding(chars, maxlen):\n",
    "    if len(chars) < maxlen:\n",
    "        return chars + ' ' * (maxlen - len(chars))\n",
    "    else:\n",
    "        return chars[:maxlen]\n",
    "\n",
    "def gen_date():\n",
    "    rnd = int(np.random.uniform(low = 1000000000, high = 1350000000))\n",
    "    timestamp = datetime.fromtimestamp(rnd)\n",
    "    return str(timestamp.strftime('%Y-%B-%d %a')) # '%Y-%B-%d %H:%M:%S'\n",
    "\n",
    "def format_date(x):\n",
    "    return str(datetime.strptime(x, '%Y-%B-%d %a').strftime('%m/%d/%Y, %A')).lower() #'%H%M%S:%Y%m%d'\n",
    "\n",
    "N = 1000\n",
    "N_train = int(N * .9)\n",
    "N_validation = N - N_train\n",
    "\n",
    "in_seq_len = 32\n",
    "out_seq_len = 32\n",
    "\n",
    "added = set()\n",
    "questions = []\n",
    "answers = []\n",
    "answers_y = []\n",
    "\n",
    "while len(questions) < N:\n",
    "    if len(questions) % 1000 == 0:\n",
    "        print('i = {}'.format(len(questions)))\n",
    "    a = gen_date()\n",
    "    if a in added:\n",
    "        continue\n",
    "    question = '[{}]'.format(a)\n",
    "    answer = '[' + str(format_date(a)) + ']'\n",
    "    answer = padding(answer, out_seq_len)\n",
    "    answer_y = str(format_date(a)) + ']'\n",
    "    answer_y = padding(answer_y, out_seq_len)\n",
    "    \n",
    "    added.add(a)\n",
    "    questions.append(question)\n",
    "    answers.append(answer)\n",
    "    answers_y.append(answer_y)\n",
    "\n",
    "chars = list(set(''.join(questions[:20000])))\n",
    "chars.extend(['[', ']']) # Start and End of Expression\n",
    "chars.extend(list(set(''.join(answers[:20000]))))\n",
    "chars = np.sort(list(set(chars)))\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((len(questions), in_seq_len, len(chars)), dtype=np.integer)\n",
    "Y = np.zeros((len(questions), out_seq_len, len(chars)), dtype=np.integer)\n",
    "Z = np.zeros((len(questions), out_seq_len, len(chars)), dtype=np.integer)\n",
    "\n",
    "for i in range(N):\n",
    "    for t, char in enumerate(questions[i]):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    for t, char in enumerate(answers[i]):\n",
    "        Y[i, t, char_indices[char]] = 1\n",
    "    for t, char in enumerate(answers_y[i]):\n",
    "        Z[i, t, char_indices[char]] = 1\n",
    "    \n",
    "X_train, X_validation, Y_train, Y_validation, Z_train, Z_validation = \\\n",
    "    train_test_split(X, Y, Z, train_size=N_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_test(N):\n",
    "    q = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        question = gen_date()\n",
    "        answer_y = format_date(question)\n",
    "        q.append(question)\n",
    "        y.append(answer_y)\n",
    "    return(q,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['2006-October-01 Sun', '2006-March-05 Sun'],\n",
       " ['10/01/2006, sunday', '03/05/2006, sunday'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_test(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pattern_matcher(gluon.Block):\n",
    "    def __init__(self, n_hidden, in_seq_len, out_seq_len, vocab_size, ctx, **kwargs):\n",
    "        super(pattern_matcher, self).__init__(**kwargs)\n",
    "        self.in_seq_len = in_seq_len\n",
    "        self.out_seq_len = out_seq_len\n",
    "        self.n_hidden = n_hidden\n",
    "        self.vocab_size = vocab_size\n",
    "        self.ctx = ctx\n",
    "        with self.name_scope():\n",
    "            self.encoder = rnn.LSTMCell(hidden_size = n_hidden)\n",
    "            self.decoder = rnn.LSTMCell(hidden_size = n_hidden)\n",
    "            self.attn_w = nn.Dense(self.n_hidden)\n",
    "            self.attn_v = nn.Dense(self.in_seq_len)\n",
    "            self.batchnorm = nn.BatchNorm(axis = 2)\n",
    "            self.dense = nn.Dense(self.vocab_size, flatten = False)\n",
    "            \n",
    "    def forward(self, inputs, outputs):\n",
    "        self.batch_size = inputs.shape[0]\n",
    "        enout, (next_h, next_c) = self.encoder.unroll(inputs = inputs, length = self.in_seq_len, merge_outputs = True)\n",
    "        for i in range(self.out_seq_len):\n",
    "            _n_h = next_h.expand_dims(axis = 1)\n",
    "            ####### Attention part: To get context vector at jth point of output sequence\n",
    "            _n_h_s = nd.repeat(_n_h, repeats = enout.shape[1], axis = 1)\n",
    "            _in_attn = nd.concat(enout, _n_h_s, dim = 2)\n",
    "            _align = self.attn_w(_in_attn)\n",
    "            _align = nd.tanh(_align)\n",
    "            score_i = self.attn_v(_align)\n",
    "            alpha_i = nd.softmax(score_i) # (n_batch * in_seq_len)\n",
    "            alpha_expand = alpha_i.expand_dims(2) # (n_batch * in_seq_len * n_hidden)\n",
    "            alpha_expand = nd.repeat(alpha_expand,repeats= self.n_hidden, axis=2) # n_batch * time step * n_hidden\n",
    "            context = nd.multiply(alpha_expand, enout)\n",
    "            context = nd.sum(context, axis = 1) # n_batch * n_hidden\n",
    "            _in = nd.concat(outputs[:, i, :], context)\n",
    "            deout, (next_h, next_c) = self.decoder(_in, [next_h, next_c],)\n",
    "            if i == 0:\n",
    "                deouts = deout\n",
    "            else:\n",
    "                deouts = nd.concat(deouts, deout, dim = 1)   \n",
    "        deouts = nd.reshape(deouts, (-1, self.out_seq_len, self.n_hidden))\n",
    "        deouts = self.batchnorm(deouts)\n",
    "        deouts_fc = self.dense(deouts)\n",
    "        return deouts_fc\n",
    "    \n",
    "    def predict(self, input_str, char_indices, indices_char, input_digits = 9, lchars = 14, ctx = mx.gpu()):\n",
    "        # No label when evaluating new example. So try to put the result of the previous time step\n",
    "        alpha = []\n",
    "        input_str = '[' + input_str + ']'\n",
    "        X = nd.zeros((1, input_digits, lchars), ctx = ctx)\n",
    "        for t, char in enumerate(input_str):\n",
    "            X[0, t, char_indices[char]] = 1\n",
    "        Y_init = nd.zeros((1, lchars), ctx = ctx)\n",
    "        Y_init[0, char_indices['[']] = 1\n",
    "        enout, (next_h, next_c) = self.encoder.unroll(inputs = X, length = self.in_seq_len, merge_outputs = True)\n",
    "        deout = Y_init\n",
    "\n",
    "        for i in range(self.out_seq_len):\n",
    "            _n_h = next_h.expand_dims(axis = 1)\n",
    "            ####### Attention part: To get context vector at jth point of output sequence\n",
    "            _n_h_s = nd.repeat(_n_h, repeats = enout.shape[1], axis = 1)\n",
    "            _in_attn = nd.concat(enout, _n_h_s, dim = 2)\n",
    "            _align = self.attn_w(_in_attn)\n",
    "            _align = nd.tanh(_align)\n",
    "            score_i = self.attn_v(_align)\n",
    "            alpha_i = nd.softmax(score_i)\n",
    "            alpha_expand = alpha_i.expand_dims(2)\n",
    "            alpha_expand = nd.repeat(alpha_expand,repeats= self.n_hidden, axis=2) # n_batch * time step * n_hidden\n",
    "            context = nd.multiply(alpha_expand, enout)\n",
    "            context = nd.sum(context, axis = 1) # n_batch * n_hidden\n",
    "            _in = nd.concat(deout, context)\n",
    "            deout, (next_h, next_c) = self.decoder(_in, [next_h, next_c],)\n",
    "            deout = nd.expand_dims(deout, axis = 1)\n",
    "            deout = self.batchnorm(deout)\n",
    "            deout = deout[:, 0, :]\n",
    "            deout_sm = self.dense(deout)\n",
    "            deout = nd.one_hot(nd.argmax(nd.softmax(deout_sm, axis = 1), axis = 1), depth = self.vocab_size)\n",
    "            if i == 0:\n",
    "                ret_seq = indices_char[nd.argmax(deout_sm, axis = 1).asnumpy()[0].astype('int')]\n",
    "            else:\n",
    "                ret_seq += indices_char[nd.argmax(deout_sm, axis = 1).asnumpy()[0].astype('int')]\n",
    "            if ret_seq[-1] == ']':\n",
    "                break\n",
    "            alpha.append(alpha_i.asnumpy())\n",
    "        return ret_seq.strip(']').strip(), np.squeeze(np.array(alpha), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_set = gluon.data.ArrayDataset(X_train, Y_train, Z_train)\n",
    "tr_data_iterator = gluon.data.DataLoader(tr_set, batch_size=256, shuffle=True)\n",
    "\n",
    "te_set =gluon.data.ArrayDataset(X_validation, Y_validation, Z_validation)\n",
    "te_data_iterator = gluon.data.DataLoader(te_set, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu()\n",
    "model = pattern_matcher(300, in_seq_len, out_seq_len, len(chars), ctx)\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx = ctx)\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), 'rmsprop')\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss(axis = 2, sparse_label = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pattern_matcher(\n",
      "  (encoder): LSTMCell(None -> 1200)\n",
      "  (decoder): LSTMCell(None -> 1200)\n",
      "  (attn_w): Dense(None -> 300, linear)\n",
      "  (attn_v): Dense(None -> 32, linear)\n",
      "  (batchnorm): BatchNorm(axis=2, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "  (dense): Dense(None -> 47, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(model, data_iter, loss_obj, ctx = ctx):\n",
    "    test_loss = []\n",
    "    for i, (x_data, y_data, z_data) in enumerate(data_iter):\n",
    "        x_data = x_data.as_in_context(ctx).astype('float32')\n",
    "        y_data = y_data.as_in_context(ctx).astype('float32')\n",
    "        z_data = z_data.as_in_context(ctx).astype('float32')\n",
    "        with autograd.predict_mode():\n",
    "            z_output = model(x_data, y_data)\n",
    "            loss_te = loss_obj(z_output, z_data)\n",
    "        curr_loss = nd.mean(loss_te).asscalar()\n",
    "        test_loss.append(curr_loss)\n",
    "    return np.mean(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m☒\u001b[0m 2005-March-28 Mon = 0(03/28/2005, monday) 0, attention (32, 32)\n",
      "\u001b[91m☒\u001b[0m 2009-February-09 Mon = 0(02/09/2009, monday) 0, attention (32, 32)\n",
      "\u001b[91m☒\u001b[0m 2009-October-02 Fri = 0(10/02/2009, friday) 0, attention (32, 32)\n",
      "\u001b[91m☒\u001b[0m 2011-March-23 Wed = 0(03/23/2011, wednesday) 0, attention (32, 32)\n",
      "\u001b[91m☒\u001b[0m 2004-July-17 Sat = 0(07/17/2004, saturday) 0, attention (32, 32)\n",
      "\u001b[91m☒\u001b[0m 2012-September-29 Sat = 0(09/29/2012, saturday) 0, attention (32, 32)\n",
      "\u001b[91m☒\u001b[0m 2008-July-02 Wed = 0(07/02/2008, wednesday) 0, attention (32, 32)\n",
      "\u001b[91m☒\u001b[0m 2010-August-20 Fri = 0(08/20/2010, friday) 0, attention (32, 32)\n",
      "\u001b[91m☒\u001b[0m 2002-January-08 Tue = 0(01/08/2002, tuesday) 0, attention (32, 32)\n",
      "\u001b[91m☒\u001b[0m 2011-January-19 Wed = 0(01/19/2011, wednesday) 0, attention (32, 32)\n",
      "Epoch 0. Train Loss: 2.3006501, Test Loss : 3.6794803\n",
      "Epoch 1. Train Loss: 0.788105, Test Loss : 3.6252139\n",
      "Epoch 2. Train Loss: 0.54211414, Test Loss : 3.5828173\n",
      "Epoch 3. Train Loss: 0.44723988, Test Loss : 3.5441458\n",
      "Epoch 4. Train Loss: 0.3476764, Test Loss : 3.4572513\n",
      "Epoch 5. Train Loss: 0.31054038, Test Loss : 3.4213924\n",
      "Epoch 6. Train Loss: 0.21523722, Test Loss : 3.2769024\n",
      "Epoch 7. Train Loss: 0.28949422, Test Loss : 3.228482\n",
      "Epoch 8. Train Loss: 0.24047504, Test Loss : 3.1288002\n",
      "Epoch 9. Train Loss: 0.116208166, Test Loss : 2.9014163\n",
      "\u001b[91m☒\u001b[0m 2004-March-30 Tue = 03/030204, tuesday(03/30/2004, tuesday) 0, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-April-15 Thu = 04/15/2004, thursday(04/15/2004, thursday) 1, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2008-April-26 Sat = 04/2008, saturday(04/26/2008, saturday) 0, attention (17, 32)\n",
      "\u001b[91m☒\u001b[0m 2006-March-22 Wed = 03/22006, wednesday(03/22/2006, wednesday) 0, attention (19, 32)\n",
      "\u001b[91m☒\u001b[0m 2011-August-12 Fri = 08/211201, friday(08/12/2011, friday) 0, attention (17, 32)\n",
      "\u001b[91m☒\u001b[0m 2005-January-26 Wed = 016/2005, wednesday(01/26/2005, wednesday) 0, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2002-November-18 Mon = 11/18/2002, monday(11/18/2002, monday) 1, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2011-December-24 Sat = 12/2011, saturday(12/24/2011, saturday) 0, attention (17, 32)\n",
      "\u001b[91m☒\u001b[0m 2010-January-24 Sun = 01/2001, sunday(01/24/2010, sunday) 0, attention (15, 32)\n",
      "\u001b[91m☒\u001b[0m 2010-September-09 Thu = 09/2001, thursday(09/09/2010, thursday) 0, attention (17, 32)\n",
      "Epoch 10. Train Loss: 0.118119925, Test Loss : 2.6708808\n",
      "Epoch 11. Train Loss: 0.08962155, Test Loss : 2.3205314\n",
      "Epoch 12. Train Loss: 0.0925828, Test Loss : 1.9914299\n",
      "Epoch 13. Train Loss: 0.042310864, Test Loss : 1.5869882\n",
      "Epoch 14. Train Loss: 0.031762794, Test Loss : 1.1828934\n",
      "Epoch 15. Train Loss: 0.034902956, Test Loss : 0.9063013\n",
      "Epoch 16. Train Loss: 0.094869345, Test Loss : 0.5345628\n",
      "Epoch 17. Train Loss: 0.06465038, Test Loss : 0.31608018\n",
      "Epoch 18. Train Loss: 0.018691722, Test Loss : 0.1701478\n",
      "Epoch 19. Train Loss: 0.014200231, Test Loss : 0.091149464\n",
      "\u001b[92m☑\u001b[0m 2001-October-23 Tue = 10/23/2001, tuesday(10/23/2001, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-October-13 Mon = 10/13/2008, monday(10/13/2008, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-June-19 Sat = 06/19/2010, saturday(06/19/2010, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-February-13 Fri = 02/13/2004, friday(02/13/2004, friday) 1, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2005-January-11 Tue = 101/11/2005, tuesday(01/11/2005, tuesday) 0, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-March-09 Sun = 03/09/2003, sunday(03/09/2003, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-September-11 Sat = 09/11/2010, saturday(09/11/2010, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-August-22 Wed = 08/22/2007, wednesday(08/22/2007, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2001-October-10 Wed = 10/10/2001, wednesday(10/10/2001, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2005-July-05 Tue = 07/05/2005, tuesday(07/05/2005, tuesday) 1, attention (19, 32)\n",
      "Epoch 20. Train Loss: 0.016530618, Test Loss : 0.06465939\n",
      "Epoch 21. Train Loss: 0.027808353, Test Loss : 0.036069967\n",
      "Epoch 22. Train Loss: 0.010784072, Test Loss : 0.019387878\n",
      "Epoch 23. Train Loss: 0.0070920554, Test Loss : 0.014595998\n",
      "Epoch 24. Train Loss: 0.0070867348, Test Loss : 0.009841411\n",
      "Epoch 25. Train Loss: 0.004811085, Test Loss : 0.0071148872\n",
      "Epoch 26. Train Loss: 0.0044911606, Test Loss : 0.0076577966\n",
      "Epoch 27. Train Loss: 0.04128491, Test Loss : 0.16692318\n",
      "Epoch 28. Train Loss: 0.070198715, Test Loss : 0.012229703\n",
      "Epoch 29. Train Loss: 0.006082166, Test Loss : 0.0061051417\n",
      "\u001b[92m☑\u001b[0m 2002-February-09 Sat = 02/09/2002, saturday(02/09/2002, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-August-18 Mon = 08/18/2008, monday(08/18/2008, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2011-March-22 Tue = 03/22/2011, tuesday(03/22/2011, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-April-26 Thu = 04/26/2007, thursday(04/26/2007, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-September-18 Thu = 09/18/2008, thursday(09/18/2008, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2011-December-06 Tue = 12/06/2011, tuesday(12/06/2011, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2011-November-13 Sun = 11/13/2011, sunday(11/13/2011, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-December-13 Mon = 12/13/2010, monday(12/13/2010, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-November-26 Fri = 11/26/2010, friday(11/26/2010, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2011-February-09 Wed = 02/09/2011, wednesday(02/09/2011, wednesday) 1, attention (21, 32)\n",
      "Epoch 30. Train Loss: 0.004096238, Test Loss : 0.004093222\n",
      "Epoch 31. Train Loss: 0.0033161272, Test Loss : 0.0034672143\n",
      "Epoch 32. Train Loss: 0.0028631284, Test Loss : 0.0028240022\n",
      "Epoch 33. Train Loss: 0.0024107557, Test Loss : 0.0025776795\n",
      "Epoch 34. Train Loss: 0.0021204846, Test Loss : 0.00213871\n",
      "Epoch 35. Train Loss: 0.001805635, Test Loss : 0.0018950447\n",
      "Epoch 36. Train Loss: 0.001615017, Test Loss : 0.0021009885\n",
      "Epoch 37. Train Loss: 0.0014327755, Test Loss : 0.0015491227\n",
      "Epoch 38. Train Loss: 0.0012468789, Test Loss : 0.0016503854\n",
      "Epoch 39. Train Loss: 0.0011068721, Test Loss : 0.0012932376\n",
      "\u001b[92m☑\u001b[0m 2002-June-14 Fri = 06/14/2002, friday(06/14/2002, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-November-15 Thu = 11/15/2007, thursday(11/15/2007, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-December-16 Tue = 12/16/2008, tuesday(12/16/2008, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-September-06 Sat = 09/06/2008, saturday(09/06/2008, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-February-14 Thu = 02/14/2008, thursday(02/14/2008, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-October-18 Mon = 10/18/2010, monday(10/18/2010, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-June-22 Tue = 06/22/2004, tuesday(06/22/2004, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-March-10 Wed = 03/10/2004, wednesday(03/10/2004, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-May-21 Wed = 05/21/2003, wednesday(05/21/2003, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-August-22 Wed = 08/22/2007, wednesday(08/22/2007, wednesday) 1, attention (21, 32)\n",
      "Epoch 40. Train Loss: 0.0010111887, Test Loss : 0.0012215623\n",
      "Epoch 41. Train Loss: 0.0008731681, Test Loss : 0.0010486179\n",
      "Epoch 42. Train Loss: 0.0007849899, Test Loss : 0.0008916636\n",
      "Epoch 43. Train Loss: 0.00071377214, Test Loss : 0.0009378326\n",
      "Epoch 44. Train Loss: 0.00064055563, Test Loss : 0.0008899472\n",
      "Epoch 45. Train Loss: 0.0005690153, Test Loss : 0.000854398\n",
      "Epoch 46. Train Loss: 0.00054081925, Test Loss : 0.0009584962\n",
      "Epoch 47. Train Loss: 0.00048113026, Test Loss : 0.0008088484\n",
      "Epoch 48. Train Loss: 0.00046867927, Test Loss : 0.0016647442\n",
      "Epoch 49. Train Loss: 0.0024846154, Test Loss : 0.17204319\n",
      "\u001b[92m☑\u001b[0m 2007-January-03 Wed = 01/03/2007, wednesday(01/03/2007, wednesday) 1, attention (21, 32)\n",
      "\u001b[91m☒\u001b[0m 2007-June-29 Fri = 06/21/2007, friday(06/29/2007, friday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2004-November-01 Mon = 11/01/2001, monday(11/01/2004, monday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2008-March-25 Tue = 03/201/200, tuesday(03/25/2008, tuesday) 0, attention (19, 32)\n",
      "\u001b[91m☒\u001b[0m 2008-July-15 Tue = 07/1tuesday(07/15/2008, tuesday) 0, attention (11, 32)\n",
      "\u001b[91m☒\u001b[0m 2009-September-21 Mon = 09/12/2000, monday(09/21/2009, monday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2002-July-04 Thu = 07/0thursday(07/04/2002, thursday) 0, attention (12, 32)\n",
      "\u001b[91m☒\u001b[0m 2009-July-25 Sat = 07/2saturday(07/25/2009, saturday) 0, attention (12, 32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m☒\u001b[0m 2004-April-24 Sat = 04/2saturday(04/24/2004, saturday) 0, attention (12, 32)\n",
      "\u001b[91m☒\u001b[0m 2009-November-07 Sat = 11/07/200saturday(11/07/2009, saturday) 0, attention (17, 32)\n",
      "Epoch 50. Train Loss: 0.27645665, Test Loss : 0.08654448\n",
      "Epoch 51. Train Loss: 0.013999086, Test Loss : 0.015477049\n",
      "Epoch 52. Train Loss: 0.004767848, Test Loss : 0.00853039\n",
      "Epoch 53. Train Loss: 0.00339458, Test Loss : 0.005481505\n",
      "Epoch 54. Train Loss: 0.0030405317, Test Loss : 0.004452289\n",
      "Epoch 55. Train Loss: 0.0024833116, Test Loss : 0.0036031252\n",
      "Epoch 56. Train Loss: 0.0020356427, Test Loss : 0.0034291637\n",
      "Epoch 57. Train Loss: 0.0015326157, Test Loss : 0.0024375508\n",
      "Epoch 58. Train Loss: 0.001180233, Test Loss : 0.001608073\n",
      "Epoch 59. Train Loss: 0.0009768846, Test Loss : 0.0012816432\n",
      "\u001b[92m☑\u001b[0m 2010-May-24 Mon = 05/24/2010, monday(05/24/2010, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-June-13 Sat = 06/13/2009, saturday(06/13/2009, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2002-January-22 Tue = 01/22/2002, tuesday(01/22/2002, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-November-11 Thu = 11/11/2004, thursday(11/11/2004, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-June-01 Mon = 06/01/2009, monday(06/01/2009, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-August-06 Thu = 08/06/2009, thursday(08/06/2009, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-November-10 Sat = 11/10/2007, saturday(11/10/2007, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-June-07 Mon = 06/07/2004, monday(06/07/2004, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-September-05 Sat = 09/05/2009, saturday(09/05/2009, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-February-28 Sat = 02/28/2009, saturday(02/28/2009, saturday) 1, attention (20, 32)\n",
      "Epoch 60. Train Loss: 0.00084525, Test Loss : 0.0010411267\n",
      "Epoch 61. Train Loss: 0.00073904236, Test Loss : 0.0009202667\n",
      "Epoch 62. Train Loss: 0.00065990747, Test Loss : 0.0007893576\n",
      "Epoch 63. Train Loss: 0.00059353816, Test Loss : 0.00070090266\n",
      "Epoch 64. Train Loss: 0.00053636363, Test Loss : 0.00064014195\n",
      "Epoch 65. Train Loss: 0.00049300166, Test Loss : 0.00059336814\n",
      "Epoch 66. Train Loss: 0.00043658764, Test Loss : 0.00054122205\n",
      "Epoch 67. Train Loss: 0.00041316904, Test Loss : 0.0004977582\n",
      "Epoch 68. Train Loss: 0.0003814593, Test Loss : 0.0004724953\n",
      "Epoch 69. Train Loss: 0.00034477725, Test Loss : 0.00043101143\n",
      "\u001b[92m☑\u001b[0m 2004-August-20 Fri = 08/20/2004, friday(08/20/2004, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2002-September-29 Sun = 09/29/2002, sunday(09/29/2002, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-July-16 Sun = 07/16/2006, sunday(07/16/2006, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2011-November-01 Tue = 11/01/2011, tuesday(11/01/2011, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-August-08 Wed = 08/08/2007, wednesday(08/08/2007, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-April-05 Mon = 04/05/2010, monday(04/05/2010, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2005-August-09 Tue = 08/09/2005, tuesday(08/09/2005, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2005-February-04 Fri = 02/04/2005, friday(02/04/2005, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2011-March-30 Wed = 03/30/2011, wednesday(03/30/2011, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-October-26 Sun = 10/26/2008, sunday(10/26/2008, sunday) 1, attention (18, 32)\n",
      "Epoch 70. Train Loss: 0.0003222975, Test Loss : 0.00039998372\n",
      "Epoch 71. Train Loss: 0.00030147657, Test Loss : 0.00038770403\n",
      "Epoch 72. Train Loss: 0.00028183253, Test Loss : 0.0003635013\n",
      "Epoch 73. Train Loss: 0.00026213352, Test Loss : 0.00034603546\n",
      "Epoch 74. Train Loss: 0.00024238706, Test Loss : 0.00032193624\n",
      "Epoch 75. Train Loss: 0.00022622457, Test Loss : 0.0003325088\n",
      "Epoch 76. Train Loss: 0.00022141136, Test Loss : 0.00030706375\n",
      "Epoch 77. Train Loss: 0.00020125962, Test Loss : 0.00030672632\n",
      "Epoch 78. Train Loss: 0.00019353488, Test Loss : 0.00045348555\n",
      "Epoch 79. Train Loss: 0.00087183947, Test Loss : 0.01448634\n",
      "\u001b[91m☒\u001b[0m 2005-November-06 Sun = 15/26/25/25/25/205, sunday(11/06/2005, sunday) 0, attention (26, 32)\n",
      "\u001b[91m☒\u001b[0m 2008-September-06 Sat = 09/5/208, saturday(09/06/2008, saturday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2006-March-19 Sun = 03/15/206, sunday(03/19/2006, sunday) 0, attention (17, 32)\n",
      "\u001b[91m☒\u001b[0m 2009-August-03 Mon = 08/35/209, monday(08/03/2009, monday) 0, attention (17, 32)\n",
      "\u001b[91m☒\u001b[0m 2003-April-17 Thu = 04/17/203, thursday(04/17/2003, thursday) 0, attention (19, 32)\n",
      "\u001b[91m☒\u001b[0m 2006-June-19 Mon = 06/15/206, monday(06/19/2006, monday) 0, attention (17, 32)\n",
      "\u001b[91m☒\u001b[0m 2004-May-28 Fri = 5/25/28/24/254,(05/28/2004, friday) 0, attention (32, 32)\n",
      "\u001b[91m☒\u001b[0m 2010-August-28 Sat = 08/28/2015, saturday(08/28/2010, saturday) 0, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2001-November-09 Fri = 15/20/205/204/205/204/205/204/20(11/09/2001, friday) 0, attention (32, 32)\n",
      "\u001b[91m☒\u001b[0m 2010-July-19 Mon = 07/15/201/25/201/25/201, monday(07/19/2010, monday) 0, attention (31, 32)\n",
      "Epoch 80. Train Loss: 0.20887735, Test Loss : 0.52677137\n",
      "Epoch 81. Train Loss: 0.0345171, Test Loss : 0.050090775\n",
      "Epoch 82. Train Loss: 0.001376915, Test Loss : 0.009514731\n",
      "Epoch 83. Train Loss: 0.00090469717, Test Loss : 0.002937224\n",
      "Epoch 84. Train Loss: 0.00073676754, Test Loss : 0.001451593\n",
      "Epoch 85. Train Loss: 0.0006088135, Test Loss : 0.0008796586\n",
      "Epoch 86. Train Loss: 0.0005290152, Test Loss : 0.0006740296\n",
      "Epoch 87. Train Loss: 0.00046858247, Test Loss : 0.0005803228\n",
      "Epoch 88. Train Loss: 0.00042195126, Test Loss : 0.0005241777\n",
      "Epoch 89. Train Loss: 0.00038632957, Test Loss : 0.00047436458\n",
      "\u001b[92m☑\u001b[0m 2012-April-22 Sun = 04/22/2012, sunday(04/22/2012, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2012-April-30 Mon = 04/30/2012, monday(04/30/2012, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-December-03 Sun = 12/03/2006, sunday(12/03/2006, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-June-24 Thu = 06/24/2010, thursday(06/24/2010, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-April-02 Fri = 04/02/2010, friday(04/02/2010, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-August-02 Thu = 08/02/2007, thursday(08/02/2007, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2005-September-11 Sun = 09/11/2005, sunday(09/11/2005, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-October-03 Sat = 10/03/2009, saturday(10/03/2009, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-November-10 Mon = 11/10/2003, monday(11/10/2003, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2011-May-03 Tue = 05/03/2011, tuesday(05/03/2011, tuesday) 1, attention (19, 32)\n",
      "Epoch 90. Train Loss: 0.00035212492, Test Loss : 0.00043931341\n",
      "Epoch 91. Train Loss: 0.00032095073, Test Loss : 0.0004120143\n",
      "Epoch 92. Train Loss: 0.00029705078, Test Loss : 0.00038055037\n",
      "Epoch 93. Train Loss: 0.00026998983, Test Loss : 0.00035773718\n",
      "Epoch 94. Train Loss: 0.00025466684, Test Loss : 0.0003388806\n",
      "Epoch 95. Train Loss: 0.00023932797, Test Loss : 0.00031660602\n",
      "Epoch 96. Train Loss: 0.0002198686, Test Loss : 0.00030233833\n",
      "Epoch 97. Train Loss: 0.00020416932, Test Loss : 0.00028688618\n",
      "Epoch 98. Train Loss: 0.00019793081, Test Loss : 0.0002818554\n",
      "Epoch 99. Train Loss: 0.00018277534, Test Loss : 0.00026482105\n",
      "\u001b[92m☑\u001b[0m 2005-November-06 Sun = 11/06/2005, sunday(11/06/2005, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-March-20 Tue = 03/20/2007, tuesday(03/20/2007, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-November-18 Thu = 11/18/2010, thursday(11/18/2010, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-February-26 Thu = 02/26/2004, thursday(02/26/2004, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2005-February-09 Wed = 02/09/2005, wednesday(02/09/2005, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2012-July-23 Mon = 07/23/2012, monday(07/23/2012, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-May-27 Tue = 05/27/2003, tuesday(05/27/2003, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-May-21 Fri = 05/21/2004, friday(05/21/2004, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-June-02 Mon = 06/02/2008, monday(06/02/2008, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-August-11 Mon = 08/11/2008, monday(08/11/2008, monday) 1, attention (18, 32)\n",
      "Epoch 100. Train Loss: 0.0001708467, Test Loss : 0.0002510329\n",
      "Epoch 101. Train Loss: 0.00016226537, Test Loss : 0.00025968766\n",
      "Epoch 102. Train Loss: 0.00015243024, Test Loss : 0.00023898801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103. Train Loss: 0.0001425976, Test Loss : 0.00022764994\n",
      "Epoch 104. Train Loss: 0.00013683144, Test Loss : 0.00022211905\n",
      "Epoch 105. Train Loss: 0.00013033806, Test Loss : 0.00021114142\n",
      "Epoch 106. Train Loss: 0.00012446094, Test Loss : 0.00021163681\n",
      "Epoch 107. Train Loss: 0.00011676793, Test Loss : 0.00019284693\n",
      "Epoch 108. Train Loss: 0.00011269046, Test Loss : 0.00019278782\n",
      "Epoch 109. Train Loss: 0.000108342836, Test Loss : 0.00018136684\n",
      "\u001b[92m☑\u001b[0m 2003-April-24 Thu = 04/24/2003, thursday(04/24/2003, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-August-01 Sun = 08/01/2004, sunday(08/01/2004, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-January-01 Wed = 01/01/2003, wednesday(01/01/2003, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-February-16 Fri = 02/16/2007, friday(02/16/2007, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-January-21 Mon = 01/21/2008, monday(01/21/2008, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2002-March-28 Thu = 03/28/2002, thursday(03/28/2002, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-August-04 Fri = 08/04/2006, friday(08/04/2006, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2011-May-28 Sat = 05/28/2011, saturday(05/28/2011, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-June-13 Sat = 06/13/2009, saturday(06/13/2009, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2002-May-08 Wed = 05/08/2002, wednesday(05/08/2002, wednesday) 1, attention (21, 32)\n",
      "Epoch 110. Train Loss: 0.000104896455, Test Loss : 0.00017901027\n",
      "Epoch 111. Train Loss: 9.8086704e-05, Test Loss : 0.0001727213\n",
      "Epoch 112. Train Loss: 9.454126e-05, Test Loss : 0.00015448302\n",
      "Epoch 113. Train Loss: 9.1162496e-05, Test Loss : 0.00015540642\n",
      "Epoch 114. Train Loss: 8.784482e-05, Test Loss : 0.00015448582\n",
      "Epoch 115. Train Loss: 8.6582535e-05, Test Loss : 0.00014507683\n",
      "Epoch 116. Train Loss: 8.31878e-05, Test Loss : 0.00016078554\n",
      "Epoch 117. Train Loss: 8.0417325e-05, Test Loss : 0.00014015535\n",
      "Epoch 118. Train Loss: 7.713514e-05, Test Loss : 0.00012741888\n",
      "Epoch 119. Train Loss: 7.737182e-05, Test Loss : 0.00013027522\n",
      "\u001b[92m☑\u001b[0m 2009-May-21 Thu = 05/21/2009, thursday(05/21/2009, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2011-March-06 Sun = 03/06/2011, sunday(03/06/2011, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-November-14 Sun = 11/14/2010, sunday(11/14/2010, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2002-October-23 Wed = 10/23/2002, wednesday(10/23/2002, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2005-May-02 Mon = 05/02/2005, monday(05/02/2005, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-November-25 Sun = 11/25/2007, sunday(11/25/2007, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2001-November-24 Sat = 11/24/2001, saturday(11/24/2001, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-May-22 Sat = 05/22/2010, saturday(05/22/2010, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-October-11 Wed = 10/11/2006, wednesday(10/11/2006, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-July-20 Tue = 07/20/2004, tuesday(07/20/2004, tuesday) 1, attention (19, 32)\n",
      "Epoch 120. Train Loss: 7.444622e-05, Test Loss : 0.00012820397\n",
      "Epoch 121. Train Loss: 7.3431656e-05, Test Loss : 0.00012372599\n",
      "Epoch 122. Train Loss: 7.017346e-05, Test Loss : 0.0001230439\n",
      "Epoch 123. Train Loss: 7.088494e-05, Test Loss : 0.00011876645\n",
      "Epoch 124. Train Loss: 6.752653e-05, Test Loss : 0.0001229918\n",
      "Epoch 125. Train Loss: 6.5678374e-05, Test Loss : 0.000118733245\n",
      "Epoch 126. Train Loss: 6.4189124e-05, Test Loss : 0.00010785772\n",
      "Epoch 127. Train Loss: 6.350725e-05, Test Loss : 0.0001104869\n",
      "Epoch 128. Train Loss: 6.206444e-05, Test Loss : 0.000109140936\n",
      "Epoch 129. Train Loss: 6.164987e-05, Test Loss : 0.00010462934\n",
      "\u001b[92m☑\u001b[0m 2011-January-31 Mon = 01/31/2011, monday(01/31/2011, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2005-September-26 Mon = 09/26/2005, monday(09/26/2005, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2002-September-06 Fri = 09/06/2002, friday(09/06/2002, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-September-29 Sat = 09/29/2007, saturday(09/29/2007, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-October-25 Wed = 10/25/2006, wednesday(10/25/2006, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-July-04 Wed = 07/04/2007, wednesday(07/04/2007, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-August-22 Sun = 08/22/2010, sunday(08/22/2010, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-April-14 Mon = 04/14/2008, monday(04/14/2008, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2012-July-08 Sun = 07/08/2012, sunday(07/08/2012, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2001-September-12 Wed = 09/12/2001, wednesday(09/12/2001, wednesday) 1, attention (21, 32)\n",
      "Epoch 130. Train Loss: 5.9978905e-05, Test Loss : 0.000104008905\n",
      "Epoch 131. Train Loss: 5.7703495e-05, Test Loss : 0.00010185272\n",
      "Epoch 132. Train Loss: 5.729601e-05, Test Loss : 0.00010480425\n",
      "Epoch 133. Train Loss: 5.690468e-05, Test Loss : 9.98274e-05\n",
      "Epoch 134. Train Loss: 5.5545395e-05, Test Loss : 9.960749e-05\n",
      "Epoch 135. Train Loss: 5.4309057e-05, Test Loss : 9.974911e-05\n",
      "Epoch 136. Train Loss: 5.3422566e-05, Test Loss : 9.541437e-05\n",
      "Epoch 137. Train Loss: 5.2488555e-05, Test Loss : 9.725227e-05\n",
      "Epoch 138. Train Loss: 5.1767798e-05, Test Loss : 9.3645016e-05\n",
      "Epoch 139. Train Loss: 5.070972e-05, Test Loss : 8.822368e-05\n",
      "\u001b[92m☑\u001b[0m 2009-November-02 Mon = 11/02/2009, monday(11/02/2009, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-April-23 Fri = 04/23/2004, friday(04/23/2004, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-June-21 Mon = 06/21/2010, monday(06/21/2010, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2002-September-07 Sat = 09/07/2002, saturday(09/07/2002, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2005-May-06 Fri = 05/06/2005, friday(05/06/2005, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-February-10 Sat = 02/10/2007, saturday(02/10/2007, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-February-22 Sat = 02/22/2003, saturday(02/22/2003, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-October-13 Mon = 10/13/2003, monday(10/13/2003, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2002-November-09 Sat = 11/09/2002, saturday(11/09/2002, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2011-May-08 Sun = 05/08/2011, sunday(05/08/2011, sunday) 1, attention (18, 32)\n",
      "Epoch 140. Train Loss: 4.936403e-05, Test Loss : 9.032655e-05\n",
      "Epoch 141. Train Loss: 4.973302e-05, Test Loss : 8.775421e-05\n",
      "Epoch 142. Train Loss: 4.834702e-05, Test Loss : 8.6079715e-05\n",
      "Epoch 143. Train Loss: 4.782878e-05, Test Loss : 8.5710686e-05\n",
      "Epoch 144. Train Loss: 4.626854e-05, Test Loss : 8.361894e-05\n",
      "Epoch 145. Train Loss: 4.6556263e-05, Test Loss : 8.3381434e-05\n",
      "Epoch 146. Train Loss: 4.6146888e-05, Test Loss : 8.332294e-05\n",
      "Epoch 147. Train Loss: 4.4769287e-05, Test Loss : 7.9281206e-05\n",
      "Epoch 148. Train Loss: 4.468929e-05, Test Loss : 8.120723e-05\n",
      "Epoch 149. Train Loss: 4.3763204e-05, Test Loss : 8.078376e-05\n",
      "\u001b[92m☑\u001b[0m 2004-August-04 Wed = 08/04/2004, wednesday(08/04/2004, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-October-30 Tue = 10/30/2007, tuesday(10/30/2007, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-May-08 Fri = 05/08/2009, friday(05/08/2009, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2005-November-18 Fri = 11/18/2005, friday(11/18/2005, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-November-04 Thu = 11/04/2004, thursday(11/04/2004, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2002-May-23 Thu = 05/23/2002, thursday(05/23/2002, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2005-July-18 Mon = 07/18/2005, monday(07/18/2005, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-May-22 Fri = 05/22/2009, friday(05/22/2009, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2012-January-16 Mon = 01/16/2012, monday(01/16/2012, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-May-21 Sun = 05/21/2006, sunday(05/21/2006, sunday) 1, attention (18, 32)\n",
      "Epoch 150. Train Loss: 4.3239546e-05, Test Loss : 7.922009e-05\n",
      "Epoch 151. Train Loss: 4.3330147e-05, Test Loss : 8.0204496e-05\n",
      "Epoch 152. Train Loss: 4.212398e-05, Test Loss : 8.009779e-05\n",
      "Epoch 153. Train Loss: 4.1364496e-05, Test Loss : 7.51486e-05\n",
      "Epoch 154. Train Loss: 4.149522e-05, Test Loss : 7.1989794e-05\n",
      "Epoch 155. Train Loss: 4.069528e-05, Test Loss : 7.1227696e-05\n",
      "Epoch 156. Train Loss: 3.990142e-05, Test Loss : 7.431305e-05\n",
      "Epoch 157. Train Loss: 3.9608844e-05, Test Loss : 7.092141e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158. Train Loss: 3.948869e-05, Test Loss : 7.4121424e-05\n",
      "Epoch 159. Train Loss: 3.8723232e-05, Test Loss : 6.864794e-05\n",
      "\u001b[92m☑\u001b[0m 2002-October-26 Sat = 10/26/2002, saturday(10/26/2002, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2005-April-04 Mon = 04/04/2005, monday(04/04/2005, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-February-04 Tue = 02/04/2003, tuesday(02/04/2003, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-November-29 Mon = 11/29/2010, monday(11/29/2010, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-October-18 Mon = 10/18/2010, monday(10/18/2010, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-June-10 Sat = 06/10/2006, saturday(06/10/2006, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-January-23 Mon = 01/23/2006, monday(01/23/2006, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-July-01 Wed = 07/01/2009, wednesday(07/01/2009, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-January-17 Sat = 01/17/2009, saturday(01/17/2009, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-September-11 Thu = 09/11/2008, thursday(09/11/2008, thursday) 1, attention (20, 32)\n",
      "Epoch 160. Train Loss: 3.816215e-05, Test Loss : 6.928665e-05\n",
      "Epoch 161. Train Loss: 3.8588078e-05, Test Loss : 6.955914e-05\n",
      "Epoch 162. Train Loss: 3.766503e-05, Test Loss : 7.055001e-05\n",
      "Epoch 163. Train Loss: 3.7139762e-05, Test Loss : 6.697563e-05\n",
      "Epoch 164. Train Loss: 3.661692e-05, Test Loss : 6.713635e-05\n",
      "Epoch 165. Train Loss: 3.6194127e-05, Test Loss : 6.75522e-05\n",
      "Epoch 166. Train Loss: 3.584844e-05, Test Loss : 6.911863e-05\n",
      "Epoch 167. Train Loss: 3.6545483e-05, Test Loss : 6.600223e-05\n",
      "Epoch 168. Train Loss: 3.498988e-05, Test Loss : 6.73519e-05\n",
      "Epoch 169. Train Loss: 3.4915724e-05, Test Loss : 6.449873e-05\n",
      "\u001b[92m☑\u001b[0m 2005-November-25 Fri = 11/25/2005, friday(11/25/2005, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-June-26 Fri = 06/26/2009, friday(06/26/2009, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2001-October-26 Fri = 10/26/2001, friday(10/26/2001, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2011-June-20 Mon = 06/20/2011, monday(06/20/2011, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-August-01 Sun = 08/01/2010, sunday(08/01/2010, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-July-31 Thu = 07/31/2003, thursday(07/31/2003, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-April-09 Fri = 04/09/2004, friday(04/09/2004, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2011-October-09 Sun = 10/09/2011, sunday(10/09/2011, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2005-July-02 Sat = 07/02/2005, saturday(07/02/2005, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-August-10 Thu = 08/10/2006, thursday(08/10/2006, thursday) 1, attention (20, 32)\n",
      "Epoch 170. Train Loss: 3.4994373e-05, Test Loss : 6.41464e-05\n",
      "Epoch 171. Train Loss: 3.372758e-05, Test Loss : 6.32592e-05\n",
      "Epoch 172. Train Loss: 3.3846947e-05, Test Loss : 6.270491e-05\n",
      "Epoch 173. Train Loss: 3.337775e-05, Test Loss : 6.0903287e-05\n",
      "Epoch 174. Train Loss: 3.3193104e-05, Test Loss : 5.936699e-05\n",
      "Epoch 175. Train Loss: 3.2864675e-05, Test Loss : 5.9154e-05\n",
      "Epoch 176. Train Loss: 3.2085958e-05, Test Loss : 6.1904815e-05\n",
      "Epoch 177. Train Loss: 3.1996962e-05, Test Loss : 5.962485e-05\n",
      "Epoch 178. Train Loss: 3.160638e-05, Test Loss : 6.0478873e-05\n",
      "Epoch 179. Train Loss: 3.1311793e-05, Test Loss : 5.8068083e-05\n",
      "\u001b[92m☑\u001b[0m 2009-June-09 Tue = 06/09/2009, tuesday(06/09/2009, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-July-27 Tue = 07/27/2010, tuesday(07/27/2010, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-September-28 Sun = 09/28/2003, sunday(09/28/2003, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2002-March-14 Thu = 03/14/2002, thursday(03/14/2002, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-May-14 Wed = 05/14/2003, wednesday(05/14/2003, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2001-November-07 Wed = 11/07/2001, wednesday(11/07/2001, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-March-26 Fri = 03/26/2004, friday(03/26/2004, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-December-17 Mon = 12/17/2007, monday(12/17/2007, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-June-01 Sun = 06/01/2008, sunday(06/01/2008, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-January-31 Thu = 01/31/2008, thursday(01/31/2008, thursday) 1, attention (20, 32)\n",
      "Epoch 180. Train Loss: 3.1615866e-05, Test Loss : 5.8145575e-05\n",
      "Epoch 181. Train Loss: 3.084454e-05, Test Loss : 5.745277e-05\n",
      "Epoch 182. Train Loss: 3.0453406e-05, Test Loss : 5.84591e-05\n",
      "Epoch 183. Train Loss: 3.0161751e-05, Test Loss : 5.4177668e-05\n",
      "Epoch 184. Train Loss: 3.0133728e-05, Test Loss : 5.613711e-05\n",
      "Epoch 185. Train Loss: 2.9893163e-05, Test Loss : 5.5896224e-05\n",
      "Epoch 186. Train Loss: 2.9656612e-05, Test Loss : 5.644832e-05\n",
      "Epoch 187. Train Loss: 2.9099603e-05, Test Loss : 5.640168e-05\n",
      "Epoch 188. Train Loss: 2.8979284e-05, Test Loss : 5.611626e-05\n",
      "Epoch 189. Train Loss: 2.9029989e-05, Test Loss : 5.4390403e-05\n",
      "\u001b[92m☑\u001b[0m 2012-April-02 Mon = 04/02/2012, monday(04/02/2012, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2002-September-02 Mon = 09/02/2002, monday(09/02/2002, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2002-February-18 Mon = 02/18/2002, monday(02/18/2002, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-October-28 Thu = 10/28/2004, thursday(10/28/2004, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2002-October-17 Thu = 10/17/2002, thursday(10/17/2002, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-January-19 Sat = 01/19/2008, saturday(01/19/2008, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-June-14 Sat = 06/14/2003, saturday(06/14/2003, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-July-10 Fri = 07/10/2009, friday(07/10/2009, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2005-September-29 Thu = 09/29/2005, thursday(09/29/2005, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-July-02 Mon = 07/02/2007, monday(07/02/2007, monday) 1, attention (18, 32)\n",
      "Epoch 190. Train Loss: 2.841567e-05, Test Loss : 5.3672014e-05\n",
      "Epoch 191. Train Loss: 2.8277871e-05, Test Loss : 5.352888e-05\n",
      "Epoch 192. Train Loss: 2.8276274e-05, Test Loss : 5.33704e-05\n",
      "Epoch 193. Train Loss: 2.7806276e-05, Test Loss : 5.2778978e-05\n",
      "Epoch 194. Train Loss: 2.7861426e-05, Test Loss : 5.238479e-05\n",
      "Epoch 195. Train Loss: 2.7750604e-05, Test Loss : 5.3333868e-05\n",
      "Epoch 196. Train Loss: 2.7158954e-05, Test Loss : 5.1291034e-05\n",
      "Epoch 197. Train Loss: 2.7021137e-05, Test Loss : 5.228333e-05\n",
      "Epoch 198. Train Loss: 2.6611602e-05, Test Loss : 5.2085692e-05\n",
      "Epoch 199. Train Loss: 2.7049442e-05, Test Loss : 5.008127e-05\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "\n",
    "tot_test_loss = []\n",
    "tot_train_loss = []\n",
    "for e in range(epochs):\n",
    "    train_loss = []\n",
    "    for i, (x_data, y_data, z_data) in enumerate(tr_data_iterator):\n",
    "        x_data = x_data.as_in_context(ctx).astype('float32')\n",
    "        y_data = y_data.as_in_context(ctx).astype('float32')\n",
    "        z_data = z_data.as_in_context(ctx).astype('float32')\n",
    "        \n",
    "        with autograd.record():\n",
    "            z_output = model(x_data, y_data)\n",
    "            loss_ = loss(z_output, z_data)\n",
    "        loss_.backward()\n",
    "        trainer.step(x_data.shape[0])\n",
    "        curr_loss = nd.mean(loss_).asscalar()\n",
    "        train_loss.append(curr_loss)\n",
    "        \n",
    "    if e % 10 == 0:\n",
    "        q, y = gen_test(10)\n",
    "        for i in range(10):\n",
    "            with autograd.predict_mode():\n",
    "                p, attn = model.predict(q[i], char_indices, indices_char, input_digits = in_seq_len, lchars = len(chars))\n",
    "                p = p.strip()\n",
    "                iscorr = 1 if p == y[i] else 0\n",
    "                if iscorr == 1:\n",
    "                    print(colors.ok + '☑' + colors.close, end=' ')\n",
    "                else:\n",
    "                    print(colors.fail + '☒' + colors.close, end=' ')\n",
    "                print(\"{} = {}({}) {}, attention {}\".format(q[i], p, y[i], str(iscorr), attn.shape))\n",
    "    #caculate test loss\n",
    "    test_loss = calculate_loss(model, te_data_iterator, loss_obj = loss, ctx=ctx) \n",
    "\n",
    "    print(\"Epoch %s. Train Loss: %s, Test Loss : %s\" % (e, np.mean(train_loss), test_loss))    \n",
    "    tot_test_loss.append(test_loss)\n",
    "    tot_train_loss.append(np.mean(train_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(net, data, ctx = mx.cpu()):\n",
    "    from matplotlib import pyplot as plt\n",
    "    import seaborn as sns\n",
    "    sns.set()\n",
    "    p =[]\n",
    "    attn = []\n",
    "    for i, d  in enumerate(data):\n",
    "        _p, _attn = net.predict(d, char_indices, indices_char, input_digits = in_seq_len, lchars = len(chars))\n",
    "        p.append(_p.strip())\n",
    "        attn.append(_attn)\n",
    "\n",
    "    fig, axes = plt.subplots(np.int(np.ceil(len(data) / 1)), 1, sharex = False, sharey = False)\n",
    "    plt.subplots_adjust(hspace=1)\n",
    "\n",
    "    if len(data) > 1:\n",
    "        fig.set_size_inches(5, 40)\n",
    "    else:\n",
    "        fig.set_size_inches(10, 10)\n",
    "    plt.subplots_adjust(hspace=1)\n",
    "    \n",
    "    for i, (d, p, a) in enumerate(zip(data, p, attn)):\n",
    "        _col = list(d)\n",
    "        _idx = list(p)\n",
    "        _val = a[:len(p), :len(d)]\n",
    "        print('input: {}, length: {}'.format(d,len(d)))\n",
    "        print('prediction: {}, length:{}'.format(p,len(p)))\n",
    "        print('attention shape= {}'.format(a.shape))\n",
    "        print('check attn = {}'.format(np.sum(a, axis = 1)))\n",
    "        print('val shape= {}'.format(_val.shape))\n",
    "        if len(data) > 1:\n",
    "            sns.heatmap(pd.DataFrame(_val, index = _idx, columns = _col), ax = axes.flat[i], cmap = 'RdYlGn', linewidths = .3)\n",
    "        else:\n",
    "            sns.heatmap(pd.DataFrame(_val, index = _idx, columns = _col), cmap = 'RdYlGn', linewidths = .3)\n",
    "        #axes.flat[i].set_title('Label: {}, Pred: {}'.format(_label[i], np.int(_pred[i])))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 2006-December-02 Sat, length: 20\n",
      "prediction: 12/02/2006, saturday, length:20\n",
      "attention shape= (20, 32)\n",
      "check attn = [0.99999994 1.         1.         1.         0.99999994 0.99999994\n",
      " 1.         1.         1.         1.         1.         1.0000001\n",
      " 0.99999994 1.         0.9999999  1.         1.         0.9999999\n",
      " 1.         1.        ]\n",
      "val shape= (20, 20)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example = [gen_date() for _ in range(1)]\n",
    "plot_attention(model, example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
