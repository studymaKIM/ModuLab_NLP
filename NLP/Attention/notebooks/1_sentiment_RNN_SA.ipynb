{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-04T04:17:55.562224Z",
     "start_time": "2018-07-04T04:17:54.161752Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import accuracy_score, auc\n",
    "from mxnet import gluon\n",
    "\n",
    "\n",
    "import time, re\n",
    "import multiprocessing as mp\n",
    "import itertools\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import mxnet as mx\n",
    "import spacy\n",
    "os.environ['MXNET_ENGINE_TYPE'] = 'NaiveEngine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 20\n",
    "MAX_VOCAB = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T03:20:57.338763Z",
     "start_time": "2018-07-20T03:18:07.883505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count words and build vocab...\n",
      "Prepare data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yn</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   yn  index\n",
       "0   0   3091\n",
       "1   1   3995"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "word_freq = collections.Counter()\n",
    "max_len = 0\n",
    "num_rec = 0\n",
    "print('Count words and build vocab...')\n",
    "with open('../data/umich-sentiment-train.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        _lab, _sen = line.decode('utf8').strip().split('\\t')\n",
    "        words = [token.lemma_ for token in nlp(_sen) if token.is_alpha] # Stop word제거 안한 상태 \n",
    "        # 제거를 위해 [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "        if len(words) > max_len:\n",
    "            max_len = len(words)\n",
    "        for word in words:\n",
    "            word_freq[word] += 1\n",
    "        num_rec += 1\n",
    "\n",
    "# most_common output -> list\n",
    "word2idx = {x[0]: i+2 for i, x in enumerate(word_freq.most_common(MAX_VOCAB - 2))}\n",
    "word2idx ['PAD'] = 0\n",
    "word2idx['UNK'] = 1\n",
    "\n",
    "idx2word= {i:v for v, i in word2idx.items()}\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "print('Prepare data...')\n",
    "y = []\n",
    "x = []\n",
    "origin_txt = []\n",
    "with open('../data/umich-sentiment-train.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        _label, _sen = line.decode('utf8').strip().split('\\t')\n",
    "        origin_txt.append(_sen)\n",
    "        y.append(int(_label))\n",
    "        words = [token.lemma_ for token in nlp(_sen) if token.is_alpha] # Stop word제거 안한 상태\n",
    "        words = [x for x in words if x != '-PRON-'] # '-PRON-' 제거\n",
    "        _seq = []\n",
    "        for word in words:\n",
    "            if word in word2idx.keys():\n",
    "                _seq.append(word2idx[word])\n",
    "            else:\n",
    "                _seq.append(word2idx['UNK'])\n",
    "        if len(_seq) < MAX_SENTENCE_LENGTH:\n",
    "            _seq.extend([0] * ((MAX_SENTENCE_LENGTH) - len(_seq)))\n",
    "        else:\n",
    "            _seq = _seq[:MAX_SENTENCE_LENGTH]\n",
    "        x.append(_seq)\n",
    "\n",
    "pd.DataFrame(y, columns = ['yn']).reset_index().groupby('yn').count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T04:16:52.755123Z",
     "start_time": "2018-07-20T04:16:52.645286Z"
    }
   },
   "outputs": [],
   "source": [
    "## Data process - tr/va split and define iterator\n",
    "\n",
    "tr_idx = np.random.choice(range(len(x)), int(len(x) * .8))\n",
    "va_idx = [x for x in range(len(x)) if x not in tr_idx]\n",
    "\n",
    "tr_x = [x[i] for i in tr_idx]\n",
    "tr_y = [y[i] for i in tr_idx]\n",
    "tr_origin = [origin_txt[i] for i in tr_idx]\n",
    "\n",
    "va_x = [x[i] for i in va_idx]\n",
    "va_y = [y[i] for i in va_idx]\n",
    "va_origin = [origin_txt[i] for i in va_idx]\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "learning_rate = .0002\n",
    "log_interval = 100\n",
    "\n",
    "\n",
    "train_data = mx.io.NDArrayIter(data=[tr_x, tr_y], batch_size=batch_size, shuffle = False)\n",
    "valid_data = mx.io.NDArrayIter(data=[va_x, va_y], batch_size=batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T04:16:57.940309Z",
     "start_time": "2018-07-20T04:16:57.935557Z"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon, autograd, nd\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet.ndarray.linalg import gemm2\n",
    "import mxnet as mx\n",
    "context = mx.gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:17:32.970927Z",
     "start_time": "2018-07-21T14:17:32.955877Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sentence_Representation(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Sentence_Representation, self).__init__()\n",
    "        for (k, v) in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.embed = nn.Embedding(self.vocab_size, self.emb_dim)\n",
    "            self.drop = nn.Dropout(.2)\n",
    "            self.bi_rnn = rnn.BidirectionalCell(\n",
    "                 rnn.LSTMCell(hidden_size = self.hidden_dim // 2),\n",
    "                 rnn.LSTMCell(hidden_size = self.hidden_dim // 2)\n",
    "            )\n",
    "            self.w_1 = nn.Dense(self.d, use_bias = False)\n",
    "            self.w_2 = nn.Dense(self.r, use_bias = False)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embeds = self.embed(x) # batch * time_step * emb_dim\n",
    "        h, _ = self.bi_rnn.unroll(length = embeds.shape[1] \\\n",
    "                                       , inputs = embeds \\\n",
    "                                       , layout = 'NTC' \\\n",
    "                                       , merge_outputs = True)\n",
    "        # For understanding\n",
    "        batch_size, time_step, _ = h.shape\n",
    "        # get self-attention\n",
    "        _h = h.reshape((-1, self.hidden_dim))\n",
    "        _w = nd.tanh(self.w_1(_h)) # Batch * time_step * r\n",
    "        w = self.w_2(_w) # Batch * time_step * d\n",
    "        _att = w.reshape((-1, time_step, self.r)) # Batch * time_step * r\n",
    "        att = nd.softmax(_att, axis = 1)\n",
    "        x = gemm2(att, h, transpose_a = True)  # h = Batch * time_step * (2 * hidden_dim)\n",
    "        return x, att"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nn.Sequential()\n",
    "classifier.add(nn.Dense(16, activation = 'relu'))\n",
    "classifier.add(nn.Dense(8, activation = 'relu'))\n",
    "classifier.add(nn.Dense(1))\n",
    "classifier.collect_params().initialize(mx.init.Xavier(), ctx = context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment analysis classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:17:35.867152Z",
     "start_time": "2018-07-21T14:17:35.858191Z"
    }
   },
   "outputs": [],
   "source": [
    "class SA_SA_Classifier(nn.Block):\n",
    "    def __init__(self, sen_rep, classifier, context, **kwargs):\n",
    "        super(SA_SA_Classifier, self).__init__(**kwargs)\n",
    "        self.context = context\n",
    "        with self.name_scope():\n",
    "            self.sen_rep = sen_rep\n",
    "            self.classifier = classifier\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # Initial hidden state\n",
    "        hidden = self.sen_rep.bi_rnn.begin_state()\n",
    "\n",
    "        # sentence representation할 때 hidden의 context가 cpu여서 오류 발생. context를 gpu로 전환\n",
    "        lstm_out, att = self.sen_rep(x, hidden)\n",
    "        x = nd.flatten(lstm_out)\n",
    "        res = self.classifier(x)\n",
    "        return res, att         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:17:36.973660Z",
     "start_time": "2018-07-21T14:17:36.970001Z"
    }
   },
   "outputs": [],
   "source": [
    "emb_dim = 100 # Emb dim\n",
    "hidden_dim = 20 # Hidden dim for LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate sentiment classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:17:38.550227Z",
     "start_time": "2018-07-21T14:17:38.536795Z"
    }
   },
   "outputs": [],
   "source": [
    "emb_dim = 50 # Emb dim\n",
    "hidden_dim = 30 # Hidden dim for LSTM\n",
    "\n",
    "param = {'emb_dim': emb_dim, 'hidden_dim': hidden_dim, 'vocab_size': vocab_size, 'd': 10, 'r': 5, 'dropout': .2}\n",
    "sen_rep = Sentence_Representation(**param)\n",
    "sen_rep.collect_params().initialize(mx.init.Xavier(), ctx = context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:17:39.912316Z",
     "start_time": "2018-07-21T14:17:39.906486Z"
    }
   },
   "outputs": [],
   "source": [
    "sa = SA_SA_Classifier(sen_rep, classifier, context)\n",
    "loss = gluon.loss.SigmoidBCELoss()\n",
    "trainer = gluon.Trainer(sa.collect_params(), 'adam', {'learning_rate': 1e-3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:17:41.166470Z",
     "start_time": "2018-07-21T14:17:41.155606Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(net, dataIterator, context):\n",
    "    dataIterator.reset()\n",
    "    loss = gluon.loss.SigmoidBCELoss()\n",
    "    total_L = 0.0\n",
    "    total_sample_num = 0\n",
    "    total_correct_num = 0\n",
    "    start_log_interval_time = time.time()\n",
    "    for i, batch in enumerate(dataIterator):\n",
    "        data =  batch.data[0].as_in_context(context)\n",
    "        label = batch.data[1].as_in_context(context)\n",
    "        output, att = net(data)\n",
    "        L = loss(output, label)\n",
    "        pred = (output > 0.5).reshape((-1,))\n",
    "        total_L += L.sum().asscalar()\n",
    "        total_sample_num += len(label)\n",
    "        total_correct_num += (pred == label).sum().asscalar()\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            print('[Batch {}/{}] elapsed {:.2f} s'.format(\n",
    "                i + 1, dataIterator.num_data//dataIterator.batch_size,\n",
    "                time.time() - start_log_interval_time))\n",
    "            start_log_interval_time = time.time()\n",
    "    avg_L = total_L / float(total_sample_num)\n",
    "    acc = total_correct_num / float(total_sample_num)\n",
    "    return avg_L, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:36:02.670468Z",
     "start_time": "2018-07-21T14:35:18.885713Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0d0f037d0a41ccb448acf6aff2e29b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch', max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 100/354] elapsed 6.95 s,                     avg loss 0.518404, throughput 0.23K wps\n",
      "[Epoch 0 Batch 200/354] elapsed 6.82 s,                     avg loss 0.485311, throughput 0.23K wps\n",
      "[Epoch 0 Batch 300/354] elapsed 6.81 s,                     avg loss 0.474957, throughput 0.23K wps\n",
      "[Batch 100/198] elapsed 3.45 s\n",
      "[Epoch 0] train avg loss 0.488462, valid acc 0.98,         valid avg loss 0.066152, throughput 0.23K wps\n",
      "[Epoch 1 Batch 100/354] elapsed 6.81 s,                     avg loss 0.446654, throughput 0.23K wps\n",
      "[Epoch 1 Batch 200/354] elapsed 6.81 s,                     avg loss 0.434503, throughput 0.24K wps\n",
      "[Epoch 1 Batch 300/354] elapsed 6.81 s,                     avg loss 0.431806, throughput 0.24K wps\n",
      "[Batch 100/198] elapsed 3.45 s\n",
      "[Epoch 1] train avg loss 0.436361, valid acc 0.99,         valid avg loss 0.050618, throughput 0.24K wps\n",
      "[Epoch 2 Batch 100/354] elapsed 6.86 s,                     avg loss 0.428061, throughput 0.23K wps\n",
      "[Epoch 2 Batch 200/354] elapsed 6.81 s,                     avg loss 0.421919, throughput 0.23K wps\n",
      "[Epoch 2 Batch 300/354] elapsed 6.82 s,                     avg loss 0.416950, throughput 0.23K wps\n",
      "[Batch 100/198] elapsed 3.46 s\n",
      "[Epoch 2] train avg loss 0.420874, valid acc 0.99,         valid avg loss 0.054227, throughput 0.23K wps\n",
      "[Epoch 3 Batch 100/354] elapsed 6.81 s,                     avg loss 0.410675, throughput 0.23K wps\n",
      "[Epoch 3 Batch 200/354] elapsed 6.81 s,                     avg loss 0.399664, throughput 0.23K wps\n",
      "[Epoch 3 Batch 300/354] elapsed 6.81 s,                     avg loss 0.399414, throughput 0.24K wps\n",
      "[Batch 100/198] elapsed 3.47 s\n",
      "[Epoch 3] train avg loss 0.402532, valid acc 0.98,         valid avg loss 0.056747, throughput 0.23K wps\n",
      "[Epoch 4 Batch 100/354] elapsed 6.81 s,                     avg loss 0.396557, throughput 0.23K wps\n",
      "[Epoch 4 Batch 200/354] elapsed 6.81 s,                     avg loss 0.390115, throughput 0.23K wps\n",
      "[Epoch 4 Batch 300/354] elapsed 6.81 s,                     avg loss 0.390398, throughput 0.23K wps\n",
      "[Batch 100/198] elapsed 3.46 s\n",
      "[Epoch 4] train avg loss 0.391855, valid acc 0.98,         valid avg loss 0.079009, throughput 0.23K wps\n",
      "[Epoch 5 Batch 100/354] elapsed 6.81 s,                     avg loss 0.388790, throughput 0.23K wps\n",
      "[Epoch 5 Batch 200/354] elapsed 6.81 s,                     avg loss 0.383754, throughput 0.23K wps\n",
      "[Epoch 5 Batch 300/354] elapsed 6.81 s,                     avg loss 0.384970, throughput 0.23K wps\n",
      "[Batch 100/198] elapsed 3.47 s\n",
      "[Epoch 5] train avg loss 0.385661, valid acc 0.98,         valid avg loss 0.083295, throughput 0.23K wps\n",
      "[Epoch 6 Batch 100/354] elapsed 6.82 s,                     avg loss 0.386862, throughput 0.23K wps\n",
      "[Epoch 6 Batch 200/354] elapsed 6.81 s,                     avg loss 0.382918, throughput 0.23K wps\n",
      "[Epoch 6 Batch 300/354] elapsed 6.81 s,                     avg loss 0.383164, throughput 0.23K wps\n",
      "[Batch 100/198] elapsed 3.46 s\n",
      "[Epoch 6] train avg loss 0.384167, valid acc 0.98,         valid avg loss 0.092037, throughput 0.23K wps\n",
      "[Epoch 7 Batch 100/354] elapsed 6.81 s,                     avg loss 0.383855, throughput 0.23K wps\n",
      "[Epoch 7 Batch 200/354] elapsed 6.81 s,                     avg loss 0.382059, throughput 0.24K wps\n",
      "[Epoch 7 Batch 300/354] elapsed 6.81 s,                     avg loss 0.381956, throughput 0.23K wps\n",
      "[Batch 100/198] elapsed 3.47 s\n",
      "[Epoch 7] train avg loss 0.382551, valid acc 0.98,         valid avg loss 0.098744, throughput 0.23K wps\n",
      "[Epoch 8 Batch 100/354] elapsed 6.82 s,                     avg loss 0.382340, throughput 0.23K wps\n",
      "[Epoch 8 Batch 200/354] elapsed 6.81 s,                     avg loss 0.380407, throughput 0.23K wps\n",
      "[Epoch 8 Batch 300/354] elapsed 6.81 s,                     avg loss 0.381592, throughput 0.23K wps\n",
      "[Batch 100/198] elapsed 3.47 s\n",
      "[Epoch 8] train avg loss 0.381616, valid acc 0.98,         valid avg loss 0.098229, throughput 0.23K wps\n",
      "[Epoch 9 Batch 100/354] elapsed 6.81 s,                     avg loss 0.382138, throughput 0.23K wps\n",
      "[Epoch 9 Batch 200/354] elapsed 6.81 s,                     avg loss 0.379807, throughput 0.23K wps\n",
      "[Epoch 9 Batch 300/354] elapsed 6.81 s,                     avg loss 0.381425, throughput 0.23K wps\n",
      "[Batch 100/198] elapsed 3.47 s\n",
      "[Epoch 9] train avg loss 0.381234, valid acc 0.98,         valid avg loss 0.100137, throughput 0.23K wps\n",
      "[Epoch 10 Batch 100/354] elapsed 6.82 s,                     avg loss 0.381527, throughput 0.23K wps\n",
      "[Epoch 10 Batch 200/354] elapsed 6.82 s,                     avg loss 0.379288, throughput 0.23K wps\n",
      "[Epoch 10 Batch 300/354] elapsed 6.81 s,                     avg loss 0.380542, throughput 0.23K wps\n",
      "[Batch 100/198] elapsed 3.47 s\n",
      "[Epoch 10] train avg loss 0.380552, valid acc 0.98,         valid avg loss 0.106500, throughput 0.23K wps\n",
      "[Epoch 11 Batch 100/354] elapsed 6.82 s,                     avg loss 0.381203, throughput 0.23K wps\n",
      "[Epoch 11 Batch 200/354] elapsed 6.82 s,                     avg loss 0.379204, throughput 0.23K wps\n",
      "[Epoch 11 Batch 300/354] elapsed 6.82 s,                     avg loss 0.380410, throughput 0.23K wps\n",
      "[Batch 100/198] elapsed 3.46 s\n",
      "[Epoch 11] train avg loss 0.380309, valid acc 0.98,         valid avg loss 0.103199, throughput 0.23K wps\n",
      "[Epoch 12 Batch 100/354] elapsed 6.82 s,                     avg loss 0.380427, throughput 0.23K wps\n",
      "[Epoch 12 Batch 200/354] elapsed 6.81 s,                     avg loss 0.378984, throughput 0.23K wps\n",
      "[Epoch 12 Batch 300/354] elapsed 6.81 s,                     avg loss 0.379531, throughput 0.23K wps\n",
      "[Batch 100/198] elapsed 3.47 s\n",
      "[Epoch 12] train avg loss 0.379662, valid acc 0.98,         valid avg loss 0.142877, throughput 0.23K wps\n",
      "[Epoch 13 Batch 100/354] elapsed 6.82 s,                     avg loss 0.380083, throughput 0.23K wps\n",
      "[Epoch 13 Batch 200/354] elapsed 6.83 s,                     avg loss 0.380165, throughput 0.23K wps\n",
      "[Epoch 13 Batch 300/354] elapsed 6.82 s,                     avg loss 0.380614, throughput 0.23K wps\n",
      "[Batch 100/198] elapsed 3.47 s\n",
      "[Epoch 13] train avg loss 0.380105, valid acc 0.98,         valid avg loss 0.149325, throughput 0.23K wps\n",
      "[Epoch 14 Batch 100/354] elapsed 6.82 s,                     avg loss 0.379279, throughput 0.23K wps\n",
      "[Epoch 14 Batch 200/354] elapsed 6.82 s,                     avg loss 0.378271, throughput 0.23K wps\n",
      "[Epoch 14 Batch 300/354] elapsed 6.81 s,                     avg loss 0.377249, throughput 0.23K wps\n",
      "[Batch 100/198] elapsed 3.46 s\n",
      "[Epoch 14] train avg loss 0.378160, valid acc 0.98,         valid avg loss 0.127944, throughput 0.23K wps\n",
      "[Epoch 15 Batch 100/354] elapsed 6.83 s,                     avg loss 0.378196, throughput 0.23K wps\n",
      "[Epoch 15 Batch 200/354] elapsed 6.83 s,                     avg loss 0.376922, throughput 0.23K wps\n",
      "[Epoch 15 Batch 300/354] elapsed 6.82 s,                     avg loss 0.376613, throughput 0.23K wps\n",
      "[Batch 100/198] elapsed 3.47 s\n",
      "[Epoch 15] train avg loss 0.377136, valid acc 0.98,         valid avg loss 0.119624, throughput 0.23K wps\n",
      "[Epoch 16 Batch 100/354] elapsed 6.83 s,                     avg loss 0.377465, throughput 0.23K wps\n",
      "[Epoch 16 Batch 200/354] elapsed 6.82 s,                     avg loss 0.375945, throughput 0.23K wps\n",
      "[Epoch 16 Batch 300/354] elapsed 6.82 s,                     avg loss 0.376276, throughput 0.23K wps\n",
      "[Batch 100/198] elapsed 3.47 s\n",
      "[Epoch 16] train avg loss 0.376551, valid acc 0.98,         valid avg loss 0.147686, throughput 0.23K wps\n",
      "[Epoch 17 Batch 100/354] elapsed 6.81 s,                     avg loss 0.377146, throughput 0.23K wps\n",
      "[Epoch 17 Batch 200/354] elapsed 6.82 s,                     avg loss 0.376541, throughput 0.23K wps\n",
      "[Epoch 17 Batch 300/354] elapsed 6.82 s,                     avg loss 0.376291, throughput 0.23K wps\n",
      "[Batch 100/198] elapsed 3.46 s\n",
      "[Epoch 17] train avg loss 0.376514, valid acc 0.98,         valid avg loss 0.115259, throughput 0.23K wps\n",
      "[Epoch 18 Batch 100/354] elapsed 6.81 s,                     avg loss 0.378478, throughput 0.23K wps\n",
      "[Epoch 18 Batch 200/354] elapsed 6.82 s,                     avg loss 0.375920, throughput 0.23K wps\n",
      "[Epoch 18 Batch 300/354] elapsed 6.82 s,                     avg loss 0.375690, throughput 0.23K wps\n",
      "[Batch 100/198] elapsed 3.46 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18] train avg loss 0.376559, valid acc 0.98,         valid avg loss 0.126118, throughput 0.23K wps\n",
      "[Epoch 19 Batch 100/354] elapsed 6.82 s,                     avg loss 0.375903, throughput 0.23K wps\n",
      "[Epoch 19 Batch 200/354] elapsed 6.82 s,                     avg loss 0.375545, throughput 0.23K wps\n",
      "[Epoch 19 Batch 300/354] elapsed 6.82 s,                     avg loss 0.375542, throughput 0.23K wps\n",
      "[Batch 100/198] elapsed 3.46 s\n",
      "[Epoch 19] train avg loss 0.375601, valid acc 0.98,         valid avg loss 0.128972, throughput 0.23K wps\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 20\n",
    "for epoch in tqdm_notebook(range(n_epoch), desc = 'epoch'):\n",
    "    ## Training\n",
    "    train_data.reset()\n",
    "    # Epoch training stats\n",
    "    start_epoch_time = time.time()\n",
    "    epoch_L = 0.0\n",
    "    epoch_sent_num = 0\n",
    "    epoch_wc = 0\n",
    "    # Log interval training stats\n",
    "    start_log_interval_time = time.time()\n",
    "    log_interval_wc = 0\n",
    "    log_interval_sent_num = 0\n",
    "    log_interval_L = 0.0\n",
    "    \n",
    "    for i, batch in enumerate(train_data):\n",
    "        _data = batch.data[0].as_in_context(context)\n",
    "        _label = batch.data[1].as_in_context(context)\n",
    "        L = 0\n",
    "        wc = len(_data)\n",
    "        log_interval_wc += wc\n",
    "        epoch_wc += wc\n",
    "        log_interval_sent_num += _data.shape[1]\n",
    "        epoch_sent_num += _data.shape[1]\n",
    "        with autograd.record():\n",
    "            _out, att = sa(_data)\n",
    "            pen = gemm2(att, att, transpose_b = True)\n",
    "            # Penalty\n",
    "            tmp = nd.dot(att[0], att[0].T) -nd.array(np.identity(att[0].shape[0]), ctx = context)\n",
    "            pen = nd.sum(nd.multiply(nd.abs(tmp), nd.abs(tmp)))\n",
    "            L = L + loss(_out, _label).mean().as_in_context(context) + .5 * pen\n",
    "        L.backward()\n",
    "        trainer.step(_data.shape[0])\n",
    "        log_interval_L += L.asscalar()\n",
    "        epoch_L += L.asscalar()\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            tqdm.write('[Epoch {} Batch {}/{}] elapsed {:.2f} s, \\\n",
    "                    avg loss {:.6f}, throughput {:.2f}K wps'.format(\n",
    "                    epoch, i + 1, train_data.num_data//train_data.batch_size,\n",
    "                    time.time() - start_log_interval_time,\n",
    "                    log_interval_L / log_interval_sent_num,\n",
    "                    log_interval_wc / 1000 / (time.time() - start_log_interval_time)))\n",
    "            # Clear log interval training stats\n",
    "            start_log_interval_time = time.time()\n",
    "            log_interval_wc = 0\n",
    "            log_interval_sent_num = 0\n",
    "            log_interval_L = 0\n",
    "    end_epoch_time = time.time()\n",
    "    test_avg_L, test_acc = evaluate(sa, valid_data, context)\n",
    "    tqdm.write('[Epoch {}] train avg loss {:.6f}, valid acc {:.2f}, \\\n",
    "        valid avg loss {:.6f}, throughput {:.2f}K wps'.format(\n",
    "        epoch, epoch_L / epoch_sent_num,\n",
    "        test_acc, test_avg_L, epoch_wc / 1000 /\n",
    "        (end_epoch_time - start_epoch_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:30:03.714909Z",
     "start_time": "2018-07-21T14:30:03.704239Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_pred(net, iterator):\n",
    "    pred_sa = []\n",
    "    label_sa = []\n",
    "    va_text = []\n",
    "    iterator.reset()\n",
    "    for i, batch in enumerate(iterator):\n",
    "        if i % 100 == 0:\n",
    "            print('i = {}'.format(i))\n",
    "        data =  batch.data[0].as_in_context(context)\n",
    "        label = batch.data[1].as_in_context(context)\n",
    "        output, _ = net(data)\n",
    "        L = loss(output, label)\n",
    "        pred = (nd.sigmoid(output) > 0.5).reshape((-1,))\n",
    "        pred_sa.extend(pred.asnumpy())\n",
    "        label_sa.extend(label.asnumpy())\n",
    "        va_text.extend([' '.join([idx2word[np.int(x)] for x in y.asnumpy() if idx2word[np.int(x)] is not 'PAD']) for y in data])\n",
    "    pred_sa_pd = pd.DataFrame(pred_sa, columns  = ['pred_sa'])\n",
    "    label_pd = pd.DataFrame(label_sa, columns = ['label'])\n",
    "    text_pd = pd.DataFrame(va_text, columns = ['text'])\n",
    "    res = pd.concat([text_pd, pred_sa_pd, label_pd], axis = 1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:30:17.312238Z",
     "start_time": "2018-07-21T14:30:06.907985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n",
      "i = 100\n"
     ]
    }
   ],
   "source": [
    "result = get_pred(sa, valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:30:27.564873Z",
     "start_time": "2018-07-21T14:30:27.555956Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[result.pred_sa != result.label].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3184, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:30:43.777958Z",
     "start_time": "2018-07-21T14:30:43.764692Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pred_sa</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>agree with love da vinci code just think ridic...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>back in melbourne mom and go to see da vinci c...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>love to sing the mission impossible song when ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>sunday before that go and see mission impossib...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>so instead go to see mission impossible which ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>see mission impossible which be rather awesome</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>go to see mission impossible be so awesome i l...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>firstly say that really love the first mission...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>have some awesome time at school in between cl...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>hate though because really like mission imposs...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  pred_sa  label\n",
       "68   agree with love da vinci code just think ridic...      0.0    1.0\n",
       "72   back in melbourne mom and go to see da vinci c...      0.0    1.0\n",
       "466  love to sing the mission impossible song when ...      0.0    1.0\n",
       "479  sunday before that go and see mission impossib...      0.0    1.0\n",
       "481  so instead go to see mission impossible which ...      0.0    1.0\n",
       "502     see mission impossible which be rather awesome      0.0    1.0\n",
       "506  go to see mission impossible be so awesome i l...      0.0    1.0\n",
       "511  firstly say that really love the first mission...      0.0    1.0\n",
       "512  have some awesome time at school in between cl...      0.0    1.0\n",
       "513  hate though because really like mission imposs...      0.0    1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[result.pred_sa != result.label].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erroneous results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sunday before that go and see mission impossible so that be awesome'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[result.pred_sa != result.label]['text'].iloc[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:49:43.020391Z",
     "start_time": "2018-07-21T14:49:43.005596Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_attention(net, n_samples = 10, mean = False):\n",
    "    from matplotlib import pyplot as plt\n",
    "    import seaborn as sns\n",
    "    sns.set()\n",
    "    idx = np.random.choice(np.arange(len(va_x)), size = n_samples, replace = False)\n",
    "    _dat = [va_x[i] for i in idx]\n",
    "    \n",
    "    w_idx = []\n",
    "    word = [[idx2word[x] for x in y] for y in _dat]\n",
    "    original_txt = [va_origin[i] for i in idx]\n",
    "    out, att = net(nd.array(_dat, ctx = context)) \n",
    "    print('attention shape = {}'.format(att.shape))\n",
    "    _a = []\n",
    "    _w = []\n",
    "    for x, y, z in zip(word, att, original_txt):\n",
    "        _idx = [i for i, _x in enumerate(x) if _x is not 'PAD']\n",
    "        _w.append(np.array([x[i] for i in _idx]))\n",
    "        _a.append(np.array([y[i].asnumpy() for i in _idx]))\n",
    "        \n",
    "    _label = [va_y[i] for i in idx]\n",
    "    _pred = (nd.sigmoid(out) > .5).asnumpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(np.int(np.ceil(n_samples / 4)), 4, sharex = False, sharey = True)\n",
    "    plt.subplots_adjust(hspace=1)\n",
    "    if mean == True:\n",
    "        fig.set_size_inches(20, 4)\n",
    "        plt.subplots_adjust(hspace=5)\n",
    "    else:\n",
    "        fig.set_size_inches(20, 20)\n",
    "        plt.subplots_adjust(hspace=1)\n",
    "    cbar_ax = fig.add_axes([.91, .3, .04, .4])\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        if mean == True:\n",
    "            _data = nd.softmax(nd.array(np.mean(_a[i], axis = 1))).asnumpy()\n",
    "            sns.heatmap(pd.DataFrame(_data, index = _w[i]).T, ax = axes.flat[i], cmap = 'RdYlGn', linewidths = .3, cbar_ax = cbar_ax)\n",
    "        else:\n",
    "            sns.heatmap(pd.DataFrame(_a[i], index = _w[i]).T, ax = axes.flat[i], cmap = 'RdYlGn', linewidths = .3, cbar_ax = cbar_ax)\n",
    "        axes.flat[i].set_title('Label: {}, Pred: {}'.format(_label[i], np.int(_pred[i])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/49899823/changing-width-of-heatmap-in-seaborn-to-compensate-for-font-size-reduction\n",
    "def plot_neuron_heatmap(text, values, title, n_limit=80, savename='fig1.png',\n",
    "                        cell_height=0.325, cell_width=0.15, dpi=100):\n",
    "    from matplotlib import pyplot as plt\n",
    "    import seaborn as sns\n",
    "    text = text.replace('\\n', '\\\\n')\n",
    "    text = np.array(list(text + ' ' * (-len(text) % n_limit)))\n",
    "    if len(values) > text.size:\n",
    "        values = np.array(values[:text.size])\n",
    "    else:\n",
    "        t = values\n",
    "        values = np.zeros(text.shape, dtype=np.float32)\n",
    "        values[:len(t)] = t\n",
    "    text = text.reshape(-1, n_limit)\n",
    "    values = values.reshape(-1, n_limit)\n",
    "    plt.figure(figsize=(cell_width * n_limit, cell_height * len(text)))\n",
    "    hmap = sns.heatmap(values, annot=text, fmt='', cmap='RdYlGn', xticklabels=False, yticklabels=False, cbar=False)\n",
    "    plt.subplots_adjust()\n",
    "    plt.title(title)\n",
    "    plt.savefig(savename, dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_sentence(_idx):\n",
    "    # Get data from valid set for _idx\n",
    "    _dat = [va_x[i] for i in _idx]\n",
    "\n",
    "    w_idx = []\n",
    "    word = [[idx2word[x] for x in y] for y in _dat]\n",
    "    original_txt = [va_origin[i] for i in _idx]\n",
    "    out, att = sa(nd.array(_dat, ctx = context)) \n",
    "    _a = []\n",
    "    _w = []\n",
    "    for x, y, z in zip(word, att, original_txt):\n",
    "        _ix = [i for i, _x in enumerate(x) if _x is not 'PAD']\n",
    "        _w.append(np.array([x[i] for i in _ix]))\n",
    "        _a.append(np.array([y[i].asnumpy() for i in _ix]))\n",
    "\n",
    "    _label = [va_y[i] for i in _idx]\n",
    "    _pred = (nd.sigmoid(out) > .5).asnumpy()\n",
    "\n",
    "    for i, _ix in enumerate(_idx):\n",
    "        att_score = [] \n",
    "        _b = nd.softmax(nd.array(np.mean(_a[i], axis = 1))).asnumpy()\n",
    "        for x in original_txt[i].split(' '):\n",
    "            _x_lem = [token.lemma_ for token in nlp(x) if token.is_alpha]\n",
    "            if len(_x_lem) > 0:\n",
    "                x_lemma = [token.lemma_ for token in nlp(x) if token.is_alpha][0]\n",
    "            else:\n",
    "                x_lemma = ''\n",
    "            if x_lemma in _w[i]:\n",
    "                idx = np.argmax(x_lemma == _w[i])\n",
    "                tmp = [_b[idx]] * len(x)\n",
    "            else:\n",
    "                idx = -1\n",
    "                tmp = [1/len(_w[i])] * len(x)\n",
    "            tmp.extend([1/len(_w[i])])\n",
    "            att_score.extend(tmp)\n",
    "        plot_neuron_heatmap(original_txt[i], att_score[:-1] \\\n",
    "                          , 'Label: {}, Pred: {}'.format(_label[i], np.int(_pred[i])) \\\n",
    "                          , n_limit= len(att_score[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_sentence(np.arange(510, 530))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
