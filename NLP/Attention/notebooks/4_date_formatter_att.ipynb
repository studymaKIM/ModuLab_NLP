{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from mxnet import nd, autograd, gluon\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet.ndarray.linalg import gemm2\n",
    "import datetime\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n"
     ]
    }
   ],
   "source": [
    "def padding(chars, maxlen):\n",
    "    if len(chars) < maxlen:\n",
    "        return chars + ' ' * (maxlen - len(chars))\n",
    "    else:\n",
    "        return chars[:maxlen]\n",
    "\n",
    "def gen_date():\n",
    "    rnd = int(np.random.uniform(low = 1000000000, high = 1350000000))\n",
    "    timestamp = datetime.fromtimestamp(rnd)\n",
    "    return str(timestamp.strftime('%Y-%B-%d %a')) # '%Y-%B-%d %H:%M:%S'\n",
    "\n",
    "def format_date(x):\n",
    "    return str(datetime.strptime(x, '%Y-%B-%d %a').strftime('%m/%d/%Y, %A')).lower() #'%H%M%S:%Y%m%d'\n",
    "\n",
    "N = 1000\n",
    "N_train = int(N * .9)\n",
    "N_validation = N - N_train\n",
    "\n",
    "in_seq_len = 32\n",
    "out_seq_len = 32\n",
    "\n",
    "added = set()\n",
    "questions = []\n",
    "answers = []\n",
    "answers_y = []\n",
    "\n",
    "while len(questions) < N:\n",
    "    if len(questions) % 1000 == 0:\n",
    "        print('i = {}'.format(len(questions)))\n",
    "    a = gen_date()\n",
    "    if a in added:\n",
    "        continue\n",
    "    question = '[{}]'.format(a)\n",
    "    answer = '[' + str(format_date(a)) + ']'\n",
    "    answer = padding(answer, out_seq_len)\n",
    "    answer_y = str(format_date(a)) + ']'\n",
    "    answer_y = padding(answer_y, out_seq_len)\n",
    "    \n",
    "    added.add(a)\n",
    "    questions.append(question)\n",
    "    answers.append(answer)\n",
    "    answers_y.append(answer_y)\n",
    "\n",
    "chars = list(set(''.join(questions[:20000])))\n",
    "chars.extend(['[', ']']) # Start and End of Expression\n",
    "chars.extend(list(set(''.join(answers[:20000]))))\n",
    "chars = np.sort(list(set(chars)))\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[2005-April-24 Sun]', '[2007-April-20 Fri]', '[2002-September-11 Wed]', '[2002-December-31 Tue]', '[2011-January-17 Mon]', '[2003-September-29 Mon]', '[2007-September-10 Mon]', '[2010-October-12 Tue]', '[2010-July-29 Thu]', '[2005-October-19 Wed]']\n",
      "['[04/24/2005, sunday]            ', '[04/20/2007, friday]            ', '[09/11/2002, wednesday]         ', '[12/31/2002, tuesday]           ', '[01/17/2011, monday]            ', '[09/29/2003, monday]            ', '[09/10/2007, monday]            ', '[10/12/2010, tuesday]           ', '[07/29/2010, thursday]          ', '[10/19/2005, wednesday]         ']\n"
     ]
    }
   ],
   "source": [
    "print(questions[:10])\n",
    "print(answers[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((len(questions), in_seq_len, len(chars)), dtype=np.integer)\n",
    "Y = np.zeros((len(questions), out_seq_len, len(chars)), dtype=np.integer)\n",
    "Z = np.zeros((len(questions), out_seq_len, len(chars)), dtype=np.integer)\n",
    "\n",
    "for i in range(N):\n",
    "    for t, char in enumerate(questions[i]):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    for t, char in enumerate(answers[i]):\n",
    "        Y[i, t, char_indices[char]] = 1\n",
    "    for t, char in enumerate(answers_y[i]):\n",
    "        Z[i, t, char_indices[char]] = 1\n",
    "    \n",
    "X_train, X_validation, Y_train, Y_validation, Z_train, Z_validation = \\\n",
    "    train_test_split(X, Y, Z, train_size=N_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_test(N):\n",
    "    q = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        question = gen_date()\n",
    "        answer_y = format_date(question)\n",
    "        q.append(question)\n",
    "        y.append(answer_y)\n",
    "    return(q,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['2004-March-11 Thu', '2007-August-19 Sun'],\n",
       " ['03/11/2004, thursday', '08/19/2007, sunday'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_test(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pattern_matcher(gluon.Block):\n",
    "    def __init__(self, n_hidden, in_seq_len, out_seq_len, vocab_size, ctx, **kwargs):\n",
    "        super(pattern_matcher, self).__init__(**kwargs)\n",
    "        self.in_seq_len = in_seq_len\n",
    "        self.out_seq_len = out_seq_len\n",
    "        self.n_hidden = n_hidden\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.encoder = rnn.LSTMCell(hidden_size = n_hidden)\n",
    "            self.decoder = rnn.LSTMCell(hidden_size = n_hidden)\n",
    "            self.attn_weight = nn.Dense(self.in_seq_len, in_units = self.in_seq_len)            \n",
    "            self.batchnorm = nn.BatchNorm(axis = 2)\n",
    "            self.dense = nn.Dense(self.vocab_size, flatten = False)\n",
    "            \n",
    "    def forward(self, inputs, outputs):\n",
    "        self.batch_size = inputs.shape[0]\n",
    "        enout, (next_h, next_c) = self.encoder.unroll(inputs = inputs \\\n",
    "                                                    , length = self.in_seq_len \\\n",
    "                                                    , merge_outputs = True)\n",
    "        # enout: (n_batch * time_step * n_hidden), next_h, next_c: (n_batch * n_hidden)\n",
    "        for i in range(self.out_seq_len):\n",
    "            # For each time step, caclculate context for attention\n",
    "            _n_h = next_h.expand_dims(axis = 2)\n",
    "            ####### Attention part: To get context vector at jth point of output sequence\n",
    "            score_i = gemm2(enout, next_h.expand_dims(axis = 2))  # n_batch * time_step * 1\n",
    "            alpha_i = nd.softmax(self.attn_weight(score_i)) # n_batch * time_step\n",
    "            alpha_expand = alpha_i.expand_dims(2) # (n_batch * 1 * time_step)\n",
    "            alpha_expand = nd.repeat(alpha_expand,repeats= self.n_hidden, axis=2) # n_batch * time_step * n_hidden\n",
    "            context = nd.multiply(alpha_expand, enout) # n_batch * time_step * n_hidden\n",
    "            context = nd.sum(context, axis = 1) # n_batch * n_hidden\n",
    "            _in = nd.concat(outputs[:, i, :], context)\n",
    "            deout, (next_h, next_c) = self.decoder(_in, [next_h, next_c],)\n",
    "            if i == 0:\n",
    "                deouts = deout\n",
    "            else:\n",
    "                deouts = nd.concat(deouts, deout, dim = 1)   \n",
    "        deouts = nd.reshape(deouts, (-1, self.out_seq_len, self.n_hidden))\n",
    "        deouts = self.batchnorm(deouts)\n",
    "        deouts_fc = self.dense(deouts)\n",
    "        return deouts_fc\n",
    "    \n",
    "    def predict(self, input_str, char_indices, indices_char, input_digits = 9, lchars = 14, ctx = mx.gpu()):\n",
    "        # No label when evaluating new example. So try to put the result of the previous time step\n",
    "        alpha = []\n",
    "        input_str = '[' + input_str + ']'\n",
    "        X = nd.zeros((1, input_digits, lchars), ctx = ctx)\n",
    "        for t, char in enumerate(input_str):\n",
    "            X[0, t, char_indices[char]] = 1\n",
    "        Y_init = nd.zeros((1, lchars), ctx = ctx)\n",
    "        Y_init[0, char_indices['[']] = 1\n",
    "        enout, (next_h, next_c) = self.encoder.unroll(inputs = X, length = self.in_seq_len, merge_outputs = True)\n",
    "        deout = Y_init\n",
    "        \n",
    "        for i in range(self.out_seq_len):\n",
    "            ####### Attention part: To get context vector at jth point of output sequence\n",
    "            score_i = gemm2(enout, next_h.expand_dims(axis = 2))\n",
    "            alpha_i = nd.softmax(self.attn_weight(score_i))\n",
    "            alpha_expand = alpha_i.expand_dims(2)\n",
    "            alpha_expand = nd.repeat(alpha_expand,repeats= self.n_hidden, axis=2) # n_batch * time step * n_hidden\n",
    "            context = nd.multiply(alpha_expand, enout)\n",
    "            context = nd.sum(context, axis = 1) # n_batch * n_hidden\n",
    "            _in = nd.concat(deout, context)\n",
    "            deout, (next_h, next_c) = self.decoder(_in, [next_h, next_c],)\n",
    "            deout = nd.expand_dims(deout, axis = 1)\n",
    "            deout = self.batchnorm(deout)\n",
    "            deout = deout[:, 0, :]\n",
    "            deout_sm = self.dense(deout)\n",
    "            deout = nd.one_hot(nd.argmax(nd.softmax(deout_sm, axis = 1), axis = 1), depth = self.vocab_size)\n",
    "            if i == 0:\n",
    "                ret_seq = indices_char[nd.argmax(deout_sm, axis = 1).asnumpy()[0].astype('int')]\n",
    "            else:\n",
    "                ret_seq += indices_char[nd.argmax(deout_sm, axis = 1).asnumpy()[0].astype('int')]\n",
    "            if ret_seq[-1] == ']':\n",
    "                break\n",
    "            alpha.append(alpha_i.asnumpy())\n",
    "        return ret_seq.strip(']').strip(), np.squeeze(np.array(alpha), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_set = gluon.data.ArrayDataset(X_train, Y_train, Z_train)\n",
    "tr_data_iterator = gluon.data.DataLoader(tr_set, batch_size=256, shuffle=True)\n",
    "\n",
    "te_set =gluon.data.ArrayDataset(X_validation, Y_validation, Z_validation)\n",
    "te_data_iterator = gluon.data.DataLoader(te_set, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu()\n",
    "model = pattern_matcher(300, in_seq_len, out_seq_len, len(chars), ctx)\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx = ctx)\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), 'rmsprop')\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss(axis = 2, sparse_label = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pattern_matcher(\n",
      "  (encoder): LSTMCell(None -> 1200)\n",
      "  (decoder): LSTMCell(None -> 1200)\n",
      "  (attn_weight): Dense(32 -> 32, linear)\n",
      "  (batchnorm): BatchNorm(axis=2, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "  (dense): Dense(None -> 47, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(model, data_iter, loss_obj, ctx = ctx):\n",
    "    test_loss = []\n",
    "    for i, (x_data, y_data, z_data) in enumerate(data_iter):\n",
    "        x_data = x_data.as_in_context(ctx).astype('float32')\n",
    "        y_data = y_data.as_in_context(ctx).astype('float32')\n",
    "        z_data = z_data.as_in_context(ctx).astype('float32')\n",
    "        with autograd.predict_mode():\n",
    "            z_output = model(x_data, y_data)\n",
    "            loss_te = loss_obj(z_output, z_data)\n",
    "        curr_loss = nd.mean(loss_te).asscalar()\n",
    "        test_loss.append(curr_loss)\n",
    "    return np.mean(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m☒\u001b[0m 2006-January-06 Fri = (01/06/2006, friday) 0, attention (32, 32)\n",
      "\u001b[91m☒\u001b[0m 2012-May-07 Mon = (05/07/2012, monday) 0, attention (32, 32)\n",
      "\u001b[91m☒\u001b[0m 2012-January-04 Wed = (01/04/2012, wednesday) 0, attention (32, 32)\n",
      "\u001b[91m☒\u001b[0m 2007-November-13 Tue = (11/13/2007, tuesday) 0, attention (32, 32)\n",
      "\u001b[91m☒\u001b[0m 2005-July-12 Tue = (07/12/2005, tuesday) 0, attention (32, 32)\n",
      "\u001b[91m☒\u001b[0m 2005-April-19 Tue = (04/19/2005, tuesday) 0, attention (32, 32)\n",
      "\u001b[91m☒\u001b[0m 2012-April-23 Mon = (04/23/2012, monday) 0, attention (32, 32)\n",
      "\u001b[91m☒\u001b[0m 2006-June-30 Fri = (06/30/2006, friday) 0, attention (32, 32)\n",
      "\u001b[91m☒\u001b[0m 2002-September-02 Mon = (09/02/2002, monday) 0, attention (32, 32)\n",
      "\u001b[91m☒\u001b[0m 2003-June-16 Mon = (06/16/2003, monday) 0, attention (32, 32)\n",
      "Epoch 0. Train Loss: 2.3467214, Test Loss : 3.6852996\n",
      "Epoch 1. Train Loss: 0.7683248, Test Loss : 3.623348\n",
      "Epoch 2. Train Loss: 0.57115424, Test Loss : 3.578885\n",
      "Epoch 3. Train Loss: 0.46894783, Test Loss : 3.5261545\n",
      "Epoch 4. Train Loss: 0.44505703, Test Loss : 3.508314\n",
      "Epoch 5. Train Loss: 0.36288154, Test Loss : 3.4229417\n",
      "Epoch 6. Train Loss: 0.33669823, Test Loss : 3.323375\n",
      "Epoch 7. Train Loss: 0.30514032, Test Loss : 3.219053\n",
      "Epoch 8. Train Loss: 0.28285772, Test Loss : 3.0691793\n",
      "Epoch 9. Train Loss: 0.26948506, Test Loss : 2.9203854\n",
      "\u001b[91m☒\u001b[0m 2008-January-05 Sat = 01/1/20011, saturday(01/05/2008, saturday) 0, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2006-January-31 Tue = 01/11/2011, thursday(01/31/2006, tuesday) 0, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2010-July-20 Tue = 01/1/2011/2011, thursday(07/20/2010, tuesday) 0, attention (24, 32)\n",
      "\u001b[91m☒\u001b[0m 2008-April-14 Mon = 04/11/2014, monday(04/14/2008, monday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2003-October-25 Sat = 01/21/2011, saturday(10/25/2003, saturday) 0, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2004-October-16 Sat = 01/11/2011, saturday(10/16/2004, saturday) 0, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2011-February-28 Mon = 01/211/21211, monday(02/28/2011, monday) 0, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2009-May-02 Sat = 09/11/2009, saturday(05/02/2009, saturday) 0, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2006-September-04 Mon = 06/11/2006, monday(09/04/2006, monday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2005-May-23 Mon = 05/12/2011, monday(05/23/2005, monday) 0, attention (18, 32)\n",
      "Epoch 10. Train Loss: 0.2649361, Test Loss : 2.810527\n",
      "Epoch 11. Train Loss: 0.24813779, Test Loss : 2.5018983\n",
      "Epoch 12. Train Loss: 0.22656356, Test Loss : 2.2104914\n",
      "Epoch 13. Train Loss: 0.20802969, Test Loss : 1.7903638\n",
      "Epoch 14. Train Loss: 0.20230448, Test Loss : 1.4245367\n",
      "Epoch 15. Train Loss: 0.19752336, Test Loss : 1.1341887\n",
      "Epoch 16. Train Loss: 0.20052806, Test Loss : 0.8167533\n",
      "Epoch 17. Train Loss: 0.17337984, Test Loss : 0.5654871\n",
      "Epoch 18. Train Loss: 0.17075908, Test Loss : 0.3862543\n",
      "Epoch 19. Train Loss: 0.16271213, Test Loss : 0.28904924\n",
      "\u001b[91m☒\u001b[0m 2001-October-14 Sun = 11/04/2010, sunday(10/14/2001, sunday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2002-September-13 Fri = 12/09/2003, friday(09/13/2002, friday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2011-May-22 Sun = 05/21/2002, sunday(05/22/2011, sunday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2003-September-04 Thu = 09/03/2000, thursday(09/04/2003, thursday) 0, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2009-July-24 Fri = 04/27/2009, friday(07/24/2009, friday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2005-September-01 Thu = 09/10/2005, thursday(09/01/2005, thursday) 0, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2008-September-28 Sun = 08/29/2008, sunday(09/28/2008, sunday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2004-December-07 Tue = 12/20/2007, tuesday(12/07/2004, tuesday) 0, attention (19, 32)\n",
      "\u001b[91m☒\u001b[0m 2005-January-06 Thu = 00/6/20005, thursday(01/06/2005, thursday) 0, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2010-June-07 Mon = 07/06/2000, monday(06/07/2010, monday) 0, attention (18, 32)\n",
      "Epoch 20. Train Loss: 0.17058855, Test Loss : 0.21487337\n",
      "Epoch 21. Train Loss: 0.15360826, Test Loss : 0.21316542\n",
      "Epoch 22. Train Loss: 0.13758555, Test Loss : 0.18987352\n",
      "Epoch 23. Train Loss: 0.14080627, Test Loss : 0.21286616\n",
      "Epoch 24. Train Loss: 0.138649, Test Loss : 0.27107573\n",
      "Epoch 25. Train Loss: 0.12615277, Test Loss : 0.15401529\n",
      "Epoch 26. Train Loss: 0.11638074, Test Loss : 0.14038108\n",
      "Epoch 27. Train Loss: 0.13744424, Test Loss : 0.15743387\n",
      "Epoch 28. Train Loss: 0.100215994, Test Loss : 0.21348149\n",
      "Epoch 29. Train Loss: 0.12015669, Test Loss : 0.15394911\n",
      "\u001b[91m☒\u001b[0m 2011-January-28 Fri = 01/18/2001, friday(01/28/2011, friday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2008-June-20 Fri = 06/08/2000, friday(06/20/2008, friday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2004-June-04 Fri = 04/06/2004, friday(06/04/2004, friday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2005-July-13 Wed = 07/31/2005, wednesday(07/13/2005, wednesday) 0, attention (21, 32)\n",
      "\u001b[91m☒\u001b[0m 2010-October-10 Sun = 10/10/2001, sunday(10/10/2010, sunday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2010-May-01 Sat = 05/01/2005, saturday(05/01/2010, saturday) 0, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2012-May-06 Sun = 06/05/2006, sunday(05/06/2012, sunday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2011-September-17 Sat = 19/10/207, saturday(09/17/2011, saturday) 0, attention (19, 32)\n",
      "\u001b[91m☒\u001b[0m 2003-March-12 Wed = 03/13/2003, wednesday(03/12/2003, wednesday) 0, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-April-29 Sat = 04/29/2006, saturday(04/29/2006, saturday) 1, attention (20, 32)\n",
      "Epoch 30. Train Loss: 0.104968585, Test Loss : 0.10888071\n",
      "Epoch 31. Train Loss: 0.09455009, Test Loss : 0.10419015\n",
      "Epoch 32. Train Loss: 0.10083762, Test Loss : 0.117110394\n",
      "Epoch 33. Train Loss: 0.113056295, Test Loss : 0.1827235\n",
      "Epoch 34. Train Loss: 0.08200345, Test Loss : 0.096984126\n",
      "Epoch 35. Train Loss: 0.08684878, Test Loss : 0.12718534\n",
      "Epoch 36. Train Loss: 0.080748715, Test Loss : 0.105996884\n",
      "Epoch 37. Train Loss: 0.065870106, Test Loss : 0.09083852\n",
      "Epoch 38. Train Loss: 0.08222527, Test Loss : 0.0822429\n",
      "Epoch 39. Train Loss: 0.08995638, Test Loss : 0.07634541\n",
      "\u001b[91m☒\u001b[0m 2007-August-10 Fri = 08/01/2008, friday(08/10/2007, friday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2008-November-24 Mon = 11/28/2004, monday(11/24/2008, monday) 0, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-April-05 Sat = 04/05/2003, saturday(04/05/2003, saturday) 1, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2011-August-23 Tue = 08/31/2012, tuesday(08/23/2011, tuesday) 0, attention (19, 32)\n",
      "\u001b[91m☒\u001b[0m 2002-January-12 Sat = 01/21/2001, saturday(01/12/2002, saturday) 0, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2002-July-24 Wed = 07/24/2007, wednesday(07/24/2002, wednesday) 0, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-September-23 Wed = 09/23/2009, wednesday(09/23/2009, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2002-September-25 Wed = 09/25/2002, wednesday(09/25/2002, wednesday) 1, attention (21, 32)\n",
      "\u001b[91m☒\u001b[0m 2010-May-31 Mon = 05/13/2001, monday(05/31/2010, monday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2008-December-30 Tue = 12/03/2008, tuesday(12/30/2008, tuesday) 0, attention (19, 32)\n",
      "Epoch 40. Train Loss: 0.066026986, Test Loss : 0.07067486\n",
      "Epoch 41. Train Loss: 0.06877152, Test Loss : 0.170059\n",
      "Epoch 42. Train Loss: 0.07403984, Test Loss : 0.07343204\n",
      "Epoch 43. Train Loss: 0.059986755, Test Loss : 0.07693559\n",
      "Epoch 44. Train Loss: 0.0625267, Test Loss : 0.09019156\n",
      "Epoch 45. Train Loss: 0.069043666, Test Loss : 0.08127445\n",
      "Epoch 46. Train Loss: 0.085921936, Test Loss : 0.09050059\n",
      "Epoch 47. Train Loss: 0.063944854, Test Loss : 0.056186453\n",
      "Epoch 48. Train Loss: 0.043951426, Test Loss : 0.054723836\n",
      "Epoch 49. Train Loss: 0.043552008, Test Loss : 0.05797488\n",
      "\u001b[91m☒\u001b[0m 2004-July-22 Thu = 07/22/2012, thursday(07/22/2004, thursday) 0, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2004-July-06 Tue = 07/06/2012, tuesday(07/06/2004, tuesday) 0, attention (19, 32)\n",
      "\u001b[91m☒\u001b[0m 2008-May-04 Sun = 05/04/2012, sunday(05/04/2008, sunday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2003-September-10 Wed = 09/03/2012, wednesday(09/10/2003, wednesday) 0, attention (21, 32)\n",
      "\u001b[91m☒\u001b[0m 2011-June-23 Thu = 06/23/2111, thursday(06/23/2011, thursday) 0, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2010-March-22 Mon = 03/22/2012, monday(03/22/2010, monday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2008-October-12 Sun = 01/21/2002, sunday(10/12/2008, sunday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2007-November-23 Fri = 11/23/2012, friday(11/23/2007, friday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2003-March-28 Fri = 03/23/2012, friday(03/28/2003, friday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2005-July-16 Sat = 07/16/2012, saturday(07/16/2005, saturday) 0, attention (20, 32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50. Train Loss: 0.06572984, Test Loss : 0.1426088\n",
      "Epoch 51. Train Loss: 0.054214116, Test Loss : 0.05461736\n",
      "Epoch 52. Train Loss: 0.041890066, Test Loss : 0.057963062\n",
      "Epoch 53. Train Loss: 0.0591913, Test Loss : 0.06943564\n",
      "Epoch 54. Train Loss: 0.047537744, Test Loss : 0.058202706\n",
      "Epoch 55. Train Loss: 0.04286334, Test Loss : 0.10177884\n",
      "Epoch 56. Train Loss: 0.06174375, Test Loss : 0.13549936\n",
      "Epoch 57. Train Loss: 0.051056728, Test Loss : 0.038186416\n",
      "Epoch 58. Train Loss: 0.03322888, Test Loss : 0.066749685\n",
      "Epoch 59. Train Loss: 0.04563586, Test Loss : 0.040881585\n",
      "\u001b[92m☑\u001b[0m 2008-February-11 Mon = 02/11/2008, monday(02/11/2008, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-July-29 Thu = 07/29/2004, thursday(07/29/2004, thursday) 1, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2010-January-17 Sun = 01/10/2017, sunday(01/17/2010, sunday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2002-January-23 Wed = 03/21/2002, wednesday(01/23/2002, wednesday) 0, attention (21, 32)\n",
      "\u001b[91m☒\u001b[0m 2006-February-24 Fri = 02/26/2004, friday(02/24/2006, friday) 0, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-January-23 Fri = 01/23/2004, friday(01/23/2004, friday) 1, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2002-September-27 Fri = 09/20/2012, friday(09/27/2002, friday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2003-November-09 Sun = 10/30/2019, sunday(11/09/2003, sunday) 0, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2005-February-24 Thu = 02/24/2005, thursday(02/24/2005, thursday) 1, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2003-March-23 Sun = 03/33/20203, sunday(03/23/2003, sunday) 0, attention (19, 32)\n",
      "Epoch 60. Train Loss: 0.046140812, Test Loss : 0.06522934\n",
      "Epoch 61. Train Loss: 0.04041384, Test Loss : 0.044801593\n",
      "Epoch 62. Train Loss: 0.035556078, Test Loss : 0.14283597\n",
      "Epoch 63. Train Loss: 0.07843227, Test Loss : 0.20976405\n",
      "Epoch 64. Train Loss: 0.07011451, Test Loss : 0.03745325\n",
      "Epoch 65. Train Loss: 0.02587457, Test Loss : 0.03418598\n",
      "Epoch 66. Train Loss: 0.032536, Test Loss : 0.035064384\n",
      "Epoch 67. Train Loss: 0.032615814, Test Loss : 0.067013346\n",
      "Epoch 68. Train Loss: 0.034088902, Test Loss : 0.036423918\n",
      "Epoch 69. Train Loss: 0.034901753, Test Loss : 0.050566398\n",
      "\u001b[92m☑\u001b[0m 2004-November-23 Tue = 11/23/2004, tuesday(11/23/2004, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-July-25 Sat = 07/25/2009, saturday(07/25/2009, saturday) 1, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2010-November-04 Thu = 11/04/2001, thursday(11/04/2010, thursday) 0, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-November-08 Sat = 11/08/2003, saturday(11/08/2003, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-January-18 Fri = 01/18/2008, friday(01/18/2008, friday) 1, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2011-May-30 Mon = 05/03/2011, monday(05/30/2011, monday) 0, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-June-16 Sat = 06/16/2007, saturday(06/16/2007, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-April-06 Thu = 04/06/2006, thursday(04/06/2006, thursday) 1, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2009-April-30 Thu = 04/03/2009, thursday(04/30/2009, thursday) 0, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-September-14 Sun = 09/14/2008, sunday(09/14/2008, sunday) 1, attention (18, 32)\n",
      "Epoch 70. Train Loss: 0.03450309, Test Loss : 0.038078073\n",
      "Epoch 71. Train Loss: 0.046233125, Test Loss : 0.060811047\n",
      "Epoch 72. Train Loss: 0.044479728, Test Loss : 0.051403493\n",
      "Epoch 73. Train Loss: 0.028227514, Test Loss : 0.030520167\n",
      "Epoch 74. Train Loss: 0.027226355, Test Loss : 0.04593066\n",
      "Epoch 75. Train Loss: 0.038200006, Test Loss : 0.09790481\n",
      "Epoch 76. Train Loss: 0.03802954, Test Loss : 0.023645895\n",
      "Epoch 77. Train Loss: 0.019696675, Test Loss : 0.030105244\n",
      "Epoch 78. Train Loss: 0.034828518, Test Loss : 0.064678416\n",
      "Epoch 79. Train Loss: 0.05174471, Test Loss : 0.0699707\n",
      "\u001b[91m☒\u001b[0m 2006-July-22 Sat = 07/22/2016, saturday(07/22/2006, saturday) 0, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2003-April-02 Wed = 04/20/2003, wednesday(04/02/2003, wednesday) 0, attention (21, 32)\n",
      "\u001b[91m☒\u001b[0m 2007-March-05 Mon = 03/05/2017, monday(03/05/2007, monday) 0, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2011-July-23 Sat = 07/23/2011, saturday(07/23/2011, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2005-May-05 Thu = 05/05/2005, thursday(05/05/2005, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-September-27 Mon = 09/27/2010, monday(09/27/2010, monday) 1, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2006-May-18 Thu = 05/18/2016, thursday(05/18/2006, thursday) 0, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2008-October-30 Thu = 10/03/2008, thursday(10/30/2008, thursday) 0, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-March-03 Wed = 03/03/2004, wednesday(03/03/2004, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-July-09 Wed = 07/09/2008, wednesday(07/09/2008, wednesday) 1, attention (21, 32)\n",
      "Epoch 80. Train Loss: 0.04241418, Test Loss : 0.058884785\n",
      "Epoch 81. Train Loss: 0.02347105, Test Loss : 0.022729134\n",
      "Epoch 82. Train Loss: 0.019101448, Test Loss : 0.021379672\n",
      "Epoch 83. Train Loss: 0.01678921, Test Loss : 0.02453974\n",
      "Epoch 84. Train Loss: 0.026919246, Test Loss : 0.072025314\n",
      "Epoch 85. Train Loss: 0.050021708, Test Loss : 0.0435205\n",
      "Epoch 86. Train Loss: 0.021841325, Test Loss : 0.030261585\n",
      "Epoch 87. Train Loss: 0.026518425, Test Loss : 0.032489575\n",
      "Epoch 88. Train Loss: 0.03423491, Test Loss : 0.058174014\n",
      "Epoch 89. Train Loss: 0.02952917, Test Loss : 0.070993826\n",
      "\u001b[92m☑\u001b[0m 2009-November-26 Thu = 11/26/2009, thursday(11/26/2009, thursday) 1, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2011-June-20 Mon = 06/02/201unesdunesdunesdunesdune(06/20/2011, monday) 0, attention (32, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-July-21 Sat = 07/21/2007, saturday(07/21/2007, saturday) 1, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2007-October-03 Wed = 10/30/2007, wednesday(10/03/2007, wednesday) 0, attention (21, 32)\n",
      "\u001b[91m☒\u001b[0m 2005-October-04 Tue = 10/05/2003, tuesday(10/04/2005, tuesday) 0, attention (19, 32)\n",
      "\u001b[91m☒\u001b[0m 2002-November-03 Sun = 10/30/2002, sunday(11/03/2002, sunday) 0, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-October-06 Sat = 10/06/2007, saturday(10/06/2007, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-July-28 Mon = 07/28/2008, monday(07/28/2008, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-February-05 Mon = 02/05/2007, monday(02/05/2007, monday) 1, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2005-October-20 Thu = 01/02/2005, thursday(10/20/2005, thursday) 0, attention (20, 32)\n",
      "Epoch 90. Train Loss: 0.042126752, Test Loss : 0.088437766\n",
      "Epoch 91. Train Loss: 0.029999409, Test Loss : 0.023686256\n",
      "Epoch 92. Train Loss: 0.018173335, Test Loss : 0.023448193\n",
      "Epoch 93. Train Loss: 0.019361408, Test Loss : 0.02745013\n",
      "Epoch 94. Train Loss: 0.017958213, Test Loss : 0.033824444\n",
      "Epoch 95. Train Loss: 0.025231127, Test Loss : 0.06753241\n",
      "Epoch 96. Train Loss: 0.050098486, Test Loss : 0.050444942\n",
      "Epoch 97. Train Loss: 0.031056754, Test Loss : 0.036400575\n",
      "Epoch 98. Train Loss: 0.019061556, Test Loss : 0.021972029\n",
      "Epoch 99. Train Loss: 0.016206564, Test Loss : 0.02445606\n",
      "\u001b[92m☑\u001b[0m 2007-July-31 Tue = 07/31/2007, tuesday(07/31/2007, tuesday) 1, attention (19, 32)\n",
      "\u001b[91m☒\u001b[0m 2001-October-10 Wed = 10/01/2010, wednesday(10/10/2001, wednesday) 0, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-March-26 Wed = 03/26/2008, wednesday(03/26/2008, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2011-August-17 Wed = 08/17/2011, wednesday(08/17/2011, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-August-04 Mon = 08/04/2008, monday(08/04/2008, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-January-13 Tue = 01/13/2009, tuesday(01/13/2009, tuesday) 1, attention (19, 32)\n",
      "\u001b[91m☒\u001b[0m 2010-December-31 Fri = 12/13/2001, friday(12/31/2010, friday) 0, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-March-05 Sun = 03/05/2006, sunday(03/05/2006, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-May-14 Mon = 05/14/2007, monday(05/14/2007, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2011-October-31 Mon = 10/31/2011, monday(10/31/2011, monday) 1, attention (18, 32)\n",
      "Epoch 100. Train Loss: 0.019335054, Test Loss : 0.019500451\n",
      "Epoch 101. Train Loss: 0.014356787, Test Loss : 0.03449865\n",
      "Epoch 102. Train Loss: 0.03043808, Test Loss : 0.04610115\n",
      "Epoch 103. Train Loss: 0.01918217, Test Loss : 0.021830432\n",
      "Epoch 104. Train Loss: 0.014688038, Test Loss : 0.019161964\n",
      "Epoch 105. Train Loss: 0.024144351, Test Loss : 0.04969101\n",
      "Epoch 106. Train Loss: 0.042397927, Test Loss : 0.062756345\n",
      "Epoch 107. Train Loss: 0.053566247, Test Loss : 0.060115818\n",
      "Epoch 108. Train Loss: 0.02000928, Test Loss : 0.018330194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109. Train Loss: 0.01589706, Test Loss : 0.016393324\n",
      "\u001b[91m☒\u001b[0m 2003-January-01 Wed = 01/10/2003, wednesday(01/01/2003, wednesday) 0, attention (21, 32)\n",
      "\u001b[91m☒\u001b[0m 2010-May-31 Mon = 05/13/2010, monday(05/31/2010, monday) 0, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-June-15 Thu = 06/15/2006, thursday(06/15/2006, thursday) 1, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2010-January-23 Sat = 01/23/2001, saturday(01/23/2010, saturday) 0, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2010-December-17 Fri = 12/17/2001, friday(12/17/2010, friday) 0, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-June-11 Wed = 06/11/2008, wednesday(06/11/2008, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2011-March-07 Mon = 03/07/2011, monday(03/07/2011, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-March-01 Sun = 03/01/2009, sunday(03/01/2009, sunday) 1, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2010-January-21 Thu = 01/12/2001, thursday(01/21/2010, thursday) 0, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-May-30 Sat = 05/30/2009, saturday(05/30/2009, saturday) 1, attention (20, 32)\n",
      "Epoch 110. Train Loss: 0.012265882, Test Loss : 0.014589239\n",
      "Epoch 111. Train Loss: 0.011840157, Test Loss : 0.024829278\n",
      "Epoch 112. Train Loss: 0.020873792, Test Loss : 0.022442484\n",
      "Epoch 113. Train Loss: 0.040888615, Test Loss : 0.0781185\n",
      "Epoch 114. Train Loss: 0.06129284, Test Loss : 0.059857007\n",
      "Epoch 115. Train Loss: 0.020002898, Test Loss : 0.031558674\n",
      "Epoch 116. Train Loss: 0.0136878425, Test Loss : 0.02319556\n",
      "Epoch 117. Train Loss: 0.012968545, Test Loss : 0.013717251\n",
      "Epoch 118. Train Loss: 0.010715094, Test Loss : 0.021920295\n",
      "Epoch 119. Train Loss: 0.01660626, Test Loss : 0.02079261\n",
      "\u001b[91m☒\u001b[0m 2001-October-19 Fri = 10/19/2012, friday(10/19/2001, friday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2012-March-20 Tue = 02/02/2012, tuesday(03/20/2012, tuesday) 0, attention (19, 32)\n",
      "\u001b[91m☒\u001b[0m 2006-June-11 Sun = 06/11/206201, sunday(06/11/2006, sunday) 0, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2004-October-22 Fri = 12/20/2024, friday(10/22/2004, friday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2009-June-12 Fri = 06/21/2009, friday(06/12/2009, friday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2003-October-05 Sun = 10/05/2002, sunday(10/05/2003, sunday) 0, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-July-17 Thu = 07/17/2008, thursday(07/17/2008, thursday) 1, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2007-November-10 Sat = 11/10/2702511, saturday(11/10/2007, saturday) 0, attention (23, 32)\n",
      "\u001b[91m☒\u001b[0m 2011-November-25 Fri = 11/25/2511, friday(11/25/2011, friday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2010-April-18 Sun = 04/18/2012, sunday(04/18/2010, sunday) 0, attention (18, 32)\n",
      "Epoch 120. Train Loss: 0.020449579, Test Loss : 0.16315159\n",
      "Epoch 121. Train Loss: 0.05766817, Test Loss : 0.21495247\n",
      "Epoch 122. Train Loss: 0.02455073, Test Loss : 0.023452926\n",
      "Epoch 123. Train Loss: 0.014147736, Test Loss : 0.019054763\n",
      "Epoch 124. Train Loss: 0.011996092, Test Loss : 0.019659156\n",
      "Epoch 125. Train Loss: 0.013249515, Test Loss : 0.028397286\n",
      "Epoch 126. Train Loss: 0.022437282, Test Loss : 0.054550968\n",
      "Epoch 127. Train Loss: 0.029636985, Test Loss : 0.03854428\n",
      "Epoch 128. Train Loss: 0.010826904, Test Loss : 0.018324219\n",
      "Epoch 129. Train Loss: 0.013684861, Test Loss : 0.020728333\n",
      "\u001b[92m☑\u001b[0m 2004-August-25 Wed = 08/25/2004, wednesday(08/25/2004, wednesday) 1, attention (21, 32)\n",
      "\u001b[91m☒\u001b[0m 2012-April-04 Wed = 14/00/2042, wednesday(04/04/2012, wednesday) 0, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2005-February-06 Sun = 02/06/2005, sunday(02/06/2005, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-July-17 Fri = 07/17/2009, friday(07/17/2009, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2002-March-16 Sat = 03/16/2002, saturday(03/16/2002, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-September-05 Sat = 09/05/2009, saturday(09/05/2009, saturday) 1, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2003-May-07 Wed = 05/03/2007, wednesday(05/07/2003, wednesday) 0, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-February-12 Thu = 02/12/2004, thursday(02/12/2004, thursday) 1, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2002-October-17 Thu = 10/10/2007, thursday(10/17/2002, thursday) 0, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2012-March-01 Thu = 03/01/2012, thursday(03/01/2012, thursday) 1, attention (20, 32)\n",
      "Epoch 130. Train Loss: 0.020819627, Test Loss : 0.039959837\n",
      "Epoch 131. Train Loss: 0.03309155, Test Loss : 0.03852271\n",
      "Epoch 132. Train Loss: 0.02746844, Test Loss : 0.046256267\n",
      "Epoch 133. Train Loss: 0.011580534, Test Loss : 0.0126311835\n",
      "Epoch 134. Train Loss: 0.012664395, Test Loss : 0.030327525\n",
      "Epoch 135. Train Loss: 0.018231206, Test Loss : 0.018335892\n",
      "Epoch 136. Train Loss: 0.015248317, Test Loss : 0.02060955\n",
      "Epoch 137. Train Loss: 0.012653671, Test Loss : 0.015773848\n",
      "Epoch 138. Train Loss: 0.01138266, Test Loss : 0.02046734\n",
      "Epoch 139. Train Loss: 0.0118894335, Test Loss : 0.021942947\n",
      "\u001b[91m☒\u001b[0m 2007-July-06 Fri = 07/06/2007, monday(07/06/2007, friday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2006-October-21 Sat = 10/23/2006, sunday(10/21/2006, saturday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2007-November-14 Wed = 11/14/2007, monday(11/14/2007, wednesday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2009-April-11 Sat = 04/11/2009, sunday(04/11/2009, saturday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2003-June-20 Fri = 06/20/2003, monday(06/20/2003, friday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2006-June-03 Sat = 06/30/2006, sunday(06/03/2006, saturday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2007-October-25 Thu = 10/25/2007, tuesday(10/25/2007, thursday) 0, attention (19, 32)\n",
      "\u001b[91m☒\u001b[0m 2005-June-10 Fri = 06/01/2005, monday(06/10/2005, friday) 0, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2002-April-07 Sun = 04/07/2002, sunday(04/07/2002, sunday) 1, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2010-February-05 Fri = 02/05/200unesdunday(02/05/2010, friday) 0, attention (19, 32)\n",
      "Epoch 140. Train Loss: 0.022445941, Test Loss : 0.2910816\n",
      "Epoch 141. Train Loss: 0.039112516, Test Loss : 0.06757442\n",
      "Epoch 142. Train Loss: 0.017191868, Test Loss : 0.023026593\n",
      "Epoch 143. Train Loss: 0.010606759, Test Loss : 0.013512416\n",
      "Epoch 144. Train Loss: 0.008645161, Test Loss : 0.018948497\n",
      "Epoch 145. Train Loss: 0.0147183, Test Loss : 0.024748547\n",
      "Epoch 146. Train Loss: 0.03137566, Test Loss : 0.1299579\n",
      "Epoch 147. Train Loss: 0.032539114, Test Loss : 0.025019169\n",
      "Epoch 148. Train Loss: 0.013381599, Test Loss : 0.021714263\n",
      "Epoch 149. Train Loss: 0.016340727, Test Loss : 0.020884506\n",
      "\u001b[92m☑\u001b[0m 2002-November-11 Mon = 11/11/2002, monday(11/11/2002, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-February-18 Sun = 02/18/2007, sunday(02/18/2007, sunday) 1, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2001-October-13 Sat = 10/31/2001, saturday(10/13/2001, saturday) 0, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-January-14 Mon = 01/14/2008, monday(01/14/2008, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-August-22 Sun = 08/22/2004, sunday(08/22/2004, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-March-22 Mon = 03/22/2010, monday(03/22/2010, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-May-17 Sat = 05/17/2003, saturday(05/17/2003, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-March-09 Tue = 03/09/2004, tuesday(03/09/2004, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-October-07 Wed = 10/07/2009, wednesday(10/07/2009, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-July-04 Wed = 07/04/2007, wednesday(07/04/2007, wednesday) 1, attention (21, 32)\n",
      "Epoch 150. Train Loss: 0.0105079, Test Loss : 0.019586291\n",
      "Epoch 151. Train Loss: 0.01384168, Test Loss : 0.016466144\n",
      "Epoch 152. Train Loss: 0.008821219, Test Loss : 0.016356431\n",
      "Epoch 153. Train Loss: 0.011913018, Test Loss : 0.02183302\n",
      "Epoch 154. Train Loss: 0.037096247, Test Loss : 0.0926904\n",
      "Epoch 155. Train Loss: 0.015099373, Test Loss : 0.012275374\n",
      "Epoch 156. Train Loss: 0.008009788, Test Loss : 0.012763777\n",
      "Epoch 157. Train Loss: 0.010483992, Test Loss : 0.020799281\n",
      "Epoch 158. Train Loss: 0.012980738, Test Loss : 0.013038249\n",
      "Epoch 159. Train Loss: 0.008864041, Test Loss : 0.01323878\n",
      "\u001b[92m☑\u001b[0m 2006-October-03 Tue = 10/03/2006, tuesday(10/03/2006, tuesday) 1, attention (19, 32)\n",
      "\u001b[91m☒\u001b[0m 2011-March-23 Wed = 03/23/2010, wednesday(03/23/2011, wednesday) 0, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2008-March-20 Thu = 03/20/2008, thursday(03/20/2008, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-January-25 Sun = 01/25/2009, sunday(01/25/2009, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-December-07 Tue = 12/07/2004, tuesday(12/07/2004, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-June-18 Thu = 06/18/2009, thursday(06/18/2009, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-December-06 Sat = 12/06/2003, saturday(12/06/2003, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2006-January-18 Wed = 01/18/2006, wednesday(01/18/2006, wednesday) 1, attention (21, 32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m☑\u001b[0m 2009-October-17 Sat = 10/17/2009, saturday(10/17/2009, saturday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-May-23 Sun = 05/23/2004, sunday(05/23/2004, sunday) 1, attention (18, 32)\n",
      "Epoch 160. Train Loss: 0.015395905, Test Loss : 0.039514102\n",
      "Epoch 161. Train Loss: 0.04033074, Test Loss : 0.1548018\n",
      "Epoch 162. Train Loss: 0.050886083, Test Loss : 0.014369287\n",
      "Epoch 163. Train Loss: 0.008513749, Test Loss : 0.012252885\n",
      "Epoch 164. Train Loss: 0.0070127873, Test Loss : 0.011874284\n",
      "Epoch 165. Train Loss: 0.0065892804, Test Loss : 0.012416784\n",
      "Epoch 166. Train Loss: 0.008167538, Test Loss : 0.015625546\n",
      "Epoch 167. Train Loss: 0.0120962, Test Loss : 0.01615056\n",
      "Epoch 168. Train Loss: 0.009951978, Test Loss : 0.012625293\n",
      "Epoch 169. Train Loss: 0.0068333717, Test Loss : 0.0124403285\n",
      "\u001b[92m☑\u001b[0m 2003-July-24 Thu = 07/24/2003, thursday(07/24/2003, thursday) 1, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2011-February-06 Sun = 02/06/2111, sunday(02/06/2011, sunday) 0, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2009-December-12 Sat = 12/19/2002, saturday(12/12/2009, saturday) 0, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2001-November-20 Tue = 11/20/2010, tuesday(11/20/2001, tuesday) 0, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-January-19 Fri = 01/19/2007, friday(01/19/2007, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-October-14 Sun = 10/14/2007, sunday(10/14/2007, sunday) 1, attention (18, 32)\n",
      "\u001b[91m☒\u001b[0m 2011-June-29 Wed = 06/29/2111, wednesday(06/29/2011, wednesday) 0, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-February-04 Sun = 02/04/2007, sunday(02/04/2007, sunday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-December-11 Sat = 12/11/2004, saturday(12/11/2004, saturday) 1, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2011-October-23 Sun = 10/20/2111, sunday(10/23/2011, sunday) 0, attention (18, 32)\n",
      "Epoch 170. Train Loss: 0.010107595, Test Loss : 0.042858668\n",
      "Epoch 171. Train Loss: 0.037389513, Test Loss : 0.14313821\n",
      "Epoch 172. Train Loss: 0.044172075, Test Loss : 0.014310859\n",
      "Epoch 173. Train Loss: 0.007843553, Test Loss : 0.009707938\n",
      "Epoch 174. Train Loss: 0.0069265794, Test Loss : 0.014160666\n",
      "Epoch 175. Train Loss: 0.005642806, Test Loss : 0.010562189\n",
      "Epoch 176. Train Loss: 0.0061965017, Test Loss : 0.014721345\n",
      "Epoch 177. Train Loss: 0.0106529975, Test Loss : 0.025470657\n",
      "Epoch 178. Train Loss: 0.010532935, Test Loss : 0.013567651\n",
      "Epoch 179. Train Loss: 0.012719977, Test Loss : 0.015198834\n",
      "\u001b[92m☑\u001b[0m 2011-May-19 Thu = 05/19/2011, thursday(05/19/2011, thursday) 1, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2004-January-31 Sat = 01/13/2004, saturday(01/31/2004, saturday) 0, attention (20, 32)\n",
      "\u001b[91m☒\u001b[0m 2005-January-31 Mon = 01/13/2005, monday(01/31/2005, monday) 0, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2009-June-10 Wed = 06/10/2009, wednesday(06/10/2009, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2002-March-21 Thu = 03/21/2002, thursday(03/21/2002, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2010-February-18 Thu = 02/18/2010, thursday(02/18/2010, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2005-October-11 Tue = 10/11/2005, tuesday(10/11/2005, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2012-October-10 Wed = 10/10/2012, wednesday(10/10/2012, wednesday) 1, attention (21, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-December-23 Tue = 12/23/2003, tuesday(12/23/2003, tuesday) 1, attention (19, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-September-10 Mon = 09/10/2007, monday(09/10/2007, monday) 1, attention (18, 32)\n",
      "Epoch 180. Train Loss: 0.006964096, Test Loss : 0.012577277\n",
      "Epoch 181. Train Loss: 0.009134761, Test Loss : 0.021717794\n",
      "Epoch 182. Train Loss: 0.011111449, Test Loss : 0.02244063\n",
      "Epoch 183. Train Loss: 0.010742931, Test Loss : 0.057457484\n",
      "Epoch 184. Train Loss: 0.032038607, Test Loss : 0.056589957\n",
      "Epoch 185. Train Loss: 0.008233631, Test Loss : 0.009297665\n",
      "Epoch 186. Train Loss: 0.005940761, Test Loss : 0.0097720735\n",
      "Epoch 187. Train Loss: 0.0056054397, Test Loss : 0.008294303\n",
      "Epoch 188. Train Loss: 0.009445583, Test Loss : 0.044686425\n",
      "Epoch 189. Train Loss: 0.023833545, Test Loss : 0.027295744\n",
      "\u001b[92m☑\u001b[0m 2006-May-26 Fri = 05/26/2006, friday(05/26/2006, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2012-August-30 Thu = 08/30/2012, thursday(08/30/2012, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-February-05 Mon = 02/05/2007, monday(02/05/2007, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2012-February-23 Thu = 02/23/2012, thursday(02/23/2012, thursday) 1, attention (20, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-October-01 Wed = 10/01/2003, wednesday(10/01/2003, wednesday) 1, attention (21, 32)\n",
      "\u001b[91m☒\u001b[0m 2008-February-01 Fri = 02/10/2008, friday(02/01/2008, friday) 0, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2005-December-30 Fri = 12/30/2005, friday(12/30/2005, friday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2004-May-24 Mon = 05/24/2004, monday(05/24/2004, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2003-August-04 Mon = 08/04/2003, monday(08/04/2003, monday) 1, attention (18, 32)\n",
      "\u001b[92m☑\u001b[0m 2007-October-16 Tue = 10/16/2007, tuesday(10/16/2007, tuesday) 1, attention (19, 32)\n",
      "Epoch 190. Train Loss: 0.006239658, Test Loss : 0.010592193\n",
      "Epoch 191. Train Loss: 0.009126812, Test Loss : 0.026178366\n",
      "Epoch 192. Train Loss: 0.01256394, Test Loss : 0.013253365\n",
      "Epoch 193. Train Loss: 0.008379394, Test Loss : 0.023860438\n",
      "Epoch 194. Train Loss: 0.01294947, Test Loss : 0.008869548\n",
      "Epoch 195. Train Loss: 0.0061766594, Test Loss : 0.024769679\n",
      "Epoch 196. Train Loss: 0.011165418, Test Loss : 0.012074659\n",
      "Epoch 197. Train Loss: 0.0049948213, Test Loss : 0.010559895\n",
      "Epoch 198. Train Loss: 0.007966895, Test Loss : 0.011426946\n",
      "Epoch 199. Train Loss: 0.008135061, Test Loss : 0.012202125\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "\n",
    "tot_test_loss = []\n",
    "tot_train_loss = []\n",
    "for e in range(epochs):\n",
    "    train_loss = []\n",
    "    for i, (x_data, y_data, z_data) in enumerate(tr_data_iterator):\n",
    "        x_data = x_data.as_in_context(ctx).astype('float32')\n",
    "        y_data = y_data.as_in_context(ctx).astype('float32')\n",
    "        z_data = z_data.as_in_context(ctx).astype('float32')\n",
    "        \n",
    "        with autograd.record():\n",
    "            z_output = model(x_data, y_data)\n",
    "            loss_ = loss(z_output, z_data)\n",
    "        loss_.backward()\n",
    "        trainer.step(x_data.shape[0])\n",
    "        curr_loss = nd.mean(loss_).asscalar()\n",
    "        train_loss.append(curr_loss)\n",
    "        \n",
    "    if e % 10 == 0:\n",
    "        q, y = gen_test(10)\n",
    "        for i in range(10):\n",
    "            with autograd.predict_mode():\n",
    "                p, attn = model.predict(q[i], char_indices, indices_char, input_digits = in_seq_len, lchars = len(chars))\n",
    "                p = p.strip()\n",
    "                iscorr = 1 if p == y[i] else 0\n",
    "                if iscorr == 1:\n",
    "                    print(colors.ok + '☑' + colors.close, end=' ')\n",
    "                else:\n",
    "                    print(colors.fail + '☒' + colors.close, end=' ')\n",
    "                print(\"{} = {}({}) {}, attention {}\".format(q[i], p, y[i], str(iscorr), attn.shape))\n",
    "    #caculate test loss\n",
    "    test_loss = calculate_loss(model, te_data_iterator, loss_obj = loss, ctx=ctx) \n",
    "\n",
    "    print(\"Epoch %s. Train Loss: %s, Test Loss : %s\" % (e, np.mean(train_loss), test_loss))    \n",
    "    tot_test_loss.append(test_loss)\n",
    "    tot_train_loss.append(np.mean(train_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(net, data, ctx = mx.cpu()):\n",
    "    from matplotlib import pyplot as plt\n",
    "    import seaborn as sns\n",
    "    sns.set()\n",
    "    p =[]\n",
    "    attn = []\n",
    "    for i, d  in enumerate(data):\n",
    "        _p, _attn = net.predict(d, char_indices, indices_char, input_digits = in_seq_len, lchars = len(chars))\n",
    "        p.append(_p.strip())\n",
    "        attn.append(_attn)\n",
    "\n",
    "    fig, axes = plt.subplots(np.int(np.ceil(len(data) / 1)), 1, sharex = False, sharey = False)\n",
    "    plt.subplots_adjust(hspace=1)\n",
    "\n",
    "    if len(data) > 1:\n",
    "        fig.set_size_inches(5, 40)\n",
    "    else:\n",
    "        fig.set_size_inches(10, 10)\n",
    "    plt.subplots_adjust(hspace=1)\n",
    "    \n",
    "    for i, (d, p, a) in enumerate(zip(data, p, attn)):\n",
    "        _col = list(d)\n",
    "        _idx = list(p)\n",
    "        _val = a[:len(p), :len(d)]\n",
    "        print('input: {}, length: {}'.format(d,len(d)))\n",
    "        print('prediction: {}, length:{}'.format(p,len(p)))\n",
    "        print('attention shape= {}'.format(a.shape))\n",
    "        print('check attn = {}'.format(np.sum(a, axis = 1)))\n",
    "        print('val shape= {}'.format(_val.shape))\n",
    "        if len(data) > 1:\n",
    "            sns.heatmap(pd.DataFrame(_val, index = _idx, columns = _col), ax = axes.flat[i], cmap = 'RdYlGn', linewidths = .3)\n",
    "        else:\n",
    "            sns.heatmap(pd.DataFrame(_val, index = _idx, columns = _col), cmap = 'RdYlGn', linewidths = .3)\n",
    "        #axes.flat[i].set_title('Label: {}, Pred: {}'.format(_label[i], np.int(_pred[i])))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 2003-March-13 Thu, length: 17\n",
      "prediction: 03/13/2003, thursday, length:20\n",
      "attention shape= (20, 32)\n",
      "check attn = [1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.99999994 0.99999994 1.         1.\n",
      " 0.99999994 1.         1.         0.9999999  1.         1.\n",
      " 1.         0.99999994]\n",
      "val shape= (20, 17)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAJHCAYAAAB4n+mOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xt0VPW9/vFnT7gIhRCCJCSAIByBaEGgCiJIgHBrDSRBLpbCQS1Qy61qrWARQjBWgl0elyBVUQT9aauIiAkcykW8gEC9RmuA40EgILkoIQTkFjL794fLHGNk2IF8N2Tv9+usWSuZzMzzzRhWP+fZ373Hsm3bFgAAgI8ELvYCAAAA3MYABAAAfIcBCAAA+A4DEAAA8B0GIAAA4DsMQAAAwHcYgAAAgO8wAAEAAN9hAAIAAL7DAAQAAHyHAQgAAPgOAxAAAPCdWm6Gfdm7k/GMNu98KknKS+luPCtm5XZ9O62/8ZyfPb5B+xI6G89ptfETvWS1N54z2t4lSXqtnvmsYSd26dW65nOGn9rl2u/zcVwH4zldduyUJB1M6mY8K3bVv2QfWmo8x2pym6zf32A8x/7bNnVYOtx4zs7bXpUk7Tu6yHhWq4aTtPPwI8ZzOjT+k17aNdl4zuj2Tyj45jTjOYF+jxvP+DE3/sa/Z/9tm2tZJtAAAQAA33G1AQIAAOZYAetiL6HGoAECAAC+QwMEAIBH0AA5RwMEAAB8hwEIAAD4DofAAADwCA6BOUcDBAAAfIcGCAAAj6ABco4GCAAA+I6jBujw4cPKz8+XJDVr1kyNGzc2uigAAFB1lkUD5FTIASg3N1ezZs1STk6OoqKiJEmFhYW6+uqrlZaWptatW7uxRgAAgGoVcgC67777NHr0aD333HMKBL47WhYMBpWZmanp06fr5ZdfdmWRAADg3NgD5FzIPUDFxcUaOnRo+fAjSYFAQElJSTpy5IjxxQEAAJgQcgCKiIhQVlaWbNsuv8+2bb3xxhsKDw83vjgAAOCcFbBcu9V0IQ+BzZs3T6mpqZo7d66io6MlSQUFBerQoYPmzZvnygIBAACqW8gBqHXr1lq2bJmKioqUl5cnSYqJiVFkZKQriwMAAM55oZlxi6PT4CMjIxl6AACAZ3AlaAAAPIIGyDmuBA0AAHyHBggAAI+gAXKOBggAAPgOAxAAAPAdDoEBAOARHAJzjgYIAAD4jmX/8HMuAABAjdVwTn/Xso7O2eBalgk0QAAAwHdc3QOUO6CL8Ywr1n8sScpL6W48K2bldh2+I954TuMlb+vbuwcYz/nZf63Xhqj2xnP6F+6SJG2K6WA8q2/eTr3ZzPzv1C9/l2u/z/9cf7XxnHbv50iSCkf2MJ4V9cpW2fsfM55jtbxLt6//rfGc5wY8q4fe/53xnJnXP/XdF/Ym41my+urAsaeNx7RoMFHv5aUaz7kxJk32x7OM51hdHjSeUSnTYg+QUzRAAADAdzgLDAAAj+AsMOdogAAAgO/QAAEA4BE0QM7RAAEAAN+hAQIAwCNogJyjAQIAAL5DAwQAgEfQADlHAwQAAHynyg3QkSNH1KhRIxNrAQAAF4AGyLmQDdDOnTs1bNgwDR8+XLt379bEiRPVu3dvxcfHa8eOHW6tEQAAoFqFHIDS09M1efJkjRkzRuPHj1diYqKys7OVmpqqjIwMt9YIAABQrUIOQN9++60SEhKUnJwsSRo6dKgkqV+/fiouLja/OgAA4JgVsFy71XQhByDbtsu/7tmzZ4WfBYNBMysCAAAwLOQm6ObNm+vYsWNq0KCB0tPTy+/Pz89XvXr1jC8OAAA454Vmxi0hB6AnnnjiJ+8PDw/XokWLjCwIAADAtPO6EGL9+vVVv3796l4LAAC4ADRAznEhRAAA4Dt8FAYAAB5hWTRATtEAAQAA36EBAgDAI9gD5BwNEAAA8B0aIAAAPIIGyDkaIAAA4DuW/cPPuwAAADVW7JMprmUdvHOla1km0AABAADfcXUP0MGkbsYzYlf9S5JUOLKH8ayoV7bq8B3xxnMaL3lbpxeONJ5TZ8or2t62g/Gc7rt3SpLev8p81vVf7PRczp6+1xrPuXJTtiTp0NhexrOavLBZ9p75xnOsK+/TkpxJxnPuuHqR3vpqpvGcPs0fkiTZxf/PeJYVMUbfnDSfc/llY7T36ELjOa0bTpH9+VzjOdY1s41n/FiAWsMx3ioAAOA7DEAAAMB3OA0eAACPCOOjMByjAQIAAL5DAwQAgEeEcSFEx2iAAACA79AAAQDgEewBco4GCAAA+A4NEAAAHhFGreHYWd+q+fPn66OPPnJzLQAAAK44awPUoUMHLV26VNOnT1e3bt2UkJCgnj17qm7dum6uDwAAOMQeIOfOOgANHTpUQ4cO1enTp7V161Zt3LhRf/nLX9S+fXslJCSoT58+ioyMdHOtAAAA1eKce4Dq1Kmj+Ph4xcfHy7ZtZWdna8OGDVqyZImysrLcWCMAAHCABsi5Km2CtixLnTt3VufOnXXvvfeaWhMAAIBRnAUGAIBHcCVo5zhhDgAA+A4NEAAAHhFGAeQYDRAAAPAdBiAAAOA7HAIDAMAj2ATtHA0QAADwHRogAAA8ggshOkcDBAAAfIcGCAAAj7hU9wDt2bNHM2bMUHFxsSIiIpSRkaHWrVtXeExZWZnS09P17rvvyrIsTZw4USNGjJAk3Xfffdq1a1f5Y3ft2qUnnnhCCQkJWrBggV566SVFRUVJkrp27arU1NRzrokBCAAAGJWamqrRo0crKSlJq1at0uzZs/X8889XeExmZqZyc3O1bt06FRcXKzk5WT169FCLFi00f/788sft3LlT48aN00033VR+X3JysqZPn16lNVm2bdsX9msBAIBLQZ/lo13LemvES44ed+jQIQ0aNEjbt29XWFiYysrK1L17d61bt06RkZHlj5s4caKGDRumwYMHS5Lmzp2r2NhYjR8/vsLrpaenS5IeeOABSdKCBQt0/PjxKg9ArjZAhSN7GM+IemWrq1lf33qj8Zym/3hPwdduN54TGPacdnTuYDwn7pOdkqTPO5nPuubTnfrs5+ZzOv57p2u/z/7BXY3ntFz7kSTp0NhexrOavLBZ9sezjOdYXR7UvwrmGs/pFj1bXxx51HjOVY3ukSTZeYuMZ1kxk1RyeoXxnPA6t+ibk//PeM7ll42R/cXDxnOsq+43nnExlZSUqKSkpNL94eHhCg8PL/8+Ly9P0dHRCgsLkySFhYUpKipKeXl5FQagvLw8xcbGln8fExOj/Pz8Cq99+vRpZWZmaunSpRXuX716tTZv3qymTZtq6tSp6tKlyznXzyEwAAA8ws09QMuWLdPChQsr3T9lyhRNnTrVSOaGDRsUGxuruLi48vtuvfVW3Xnnnapdu7a2bNmiSZMmac2aNWrcuHHI12IAAgAAVTZu3DilpKRUuv+H7Y/0XZNTUFCgsrKy8kNghYWFiomJqfS4gwcPqlOnTpIqN0KStGLFCt1yyy0V7mvatGn51z179lRMTIy++OILdevWLeT6OQ0eAACPCLMs127h4eFq0aJFpduPB6AmTZooLi5OWVlZkqSsrCzFxcVVOPwlSYMHD9by5csVDAZVVFSkDRs2aNCgQeU/z8/P14cffqghQ4ZUeF5BQUH51zt27NBXX32lK6+88pzvFQ0QAAAwas6cOZoxY4YWLVqk8PBwZWRkSJImTJigadOmqWPHjkpKSlJ2drYGDhwoSZo8ebJatmxZ/horV65U37591ahRowqv/eijj+rzzz9XIBBQ7dq1NX/+/Aqt0NkwAAEA4BGX6pWg27Ztq+XLl1e6f/HixeVfh4WFKS0t7ayv8fvf//4n7/9+mKoqDoEBAADfoQECAMAjwqg1HOOtAgAAvsMABAAAfIdDYAAAeMSlugn6UkQDBAAAfOe8B6AfX4gIAABcXGEBy7VbTRfyENj//u//nvVnhw8frvbFAAAAuCHkAJSYmKjmzZvLtu1KPysuLja2KAAAUHXsAXIu5ADUvHlzvfTSS4qOjq70s/j4eGOLAgAAMCnkADRw4EB99dVXPzkADRgwwNiiAABA1XEhROdCDkDTp08/688eeOCBal8MAACAG7gOEAAAHsEeIOcoywAAgO/QAAEA4BFeuD6PW2iAAACA79AAAQDgEewBco4GCAAA+A4NEAAAHsF1gJzjrQIAAL5j2T/1QV8AAKDG+d2b413LeqrfM65lmeDqIbCvb73ReEbTf7znatY3v+lpPOfyF7fI3j7DeI7VfZ6+7N3JeE6bdz6VJO3u1dF4VtvNn3ku52BSN+M5sav+JUkquu0m41mRS9+V/fEs4zlWlwe17+gi4zmtGk7St6WrjOf8rHaSJMnekW48y4p7QEF7o/GcgJWgM8H1xnNqBQbIzn3UeI51xT3GM3D+OAQGAAB8h03QAAB4RBhnwTtGAwQAAHyHBggAAI8IcCFEx2iAAACA79AAAQDgEewBco4GCAAA+A4NEAAAHhGgAXKMBggAAPgODRAAAB7BHiDnqtwAHTlyxMQ6AAAAXBNyANq5c6eGDRum4cOHa/fu3Zo4caJ69+6t+Ph47dixw601AgAABwIBy7VbTRdyAEpPT9fkyZM1ZswYjR8/XomJicrOzlZqaqoyMjLcWiMAAEC1CjkAffvtt0pISFBycrIkaejQoZKkfv36qbi42PzqAACAY2GWe7eaLuQAZNt2+dc9e/as8LNgMGhmRQAAAIaFPAusefPmOnbsmBo0aKD09PTy+/Pz81WvXj3jiwMAAM55YGuOa0IOQE888cRP3h8eHq5FixYZWRAAAIBp53UdoPr166t+/frVvRYAAABXcCFEAAA8wgubk93CR2EAAADfoQECAMAjAhYVkFM0QAAAwHdogAAA8Aj2ADlHAwQAAHyHBggAAI/gQojOWfYPP+8CAADUWA+9/zvXsmZe/5RrWSa42gB9feuNxjOa/uM9SVLhyB7Gs6Je2epajr0j/dwPvEBW3APaP7ir8ZyWaz+SJO1L6Gw8q9XGT5Q7oIvxnCvWf6w9fa81nnPlpmzlpXQ3nhOzcrskqei2m4xnRS59V/b2GcZzrO7zVHxqufGciLojpOBG4zkKJEiS7A8fMB5l/SLdvd+pbL35nLABsr963HiM1Xya8YwfC+MsMMfYAwQAAHyHPUAAAHgEe4CcowECAAC+QwMEAIBHcB0g52iAAACA79AAAQDgEQFqDcd4qwAAgO8wAAEAAN/hEBgAAB7BhRCdO2sDNH/+fH300UdurgUAAMAVZx2AOnTooKVLl2rAgAGaOXOm3nzzTZ06dcrNtQEAgCoIWO7darqzHgIbOnSohg4dqtOnT2vr1q3auHGj/vKXv6h9+/ZKSEhQnz59FBkZ6eZaAQAAqsU59wDVqVNH8fHxio+Pl23bys7O1oYNG7RkyRJlZWW5sUYAAOAAF0J0rkqboC3LUufOndW5c2fde++9ptYEAABgFGeBAQDgEV7Ym+MWrgMEAAB8hwYIAACP4DpAztEAAQAA36EBAgDAI9gD5BwNEAAA8B0aIAAAPILrADlHAwQAAHyHBggAAI8IcBaYYzRAAADAdxiAAACA71i2bdsXexEAAODCvbRrsmtZo9s/4VqWCa7uAfr61huNZzT9x3uSpPzhNxjPavbqNtdy7C8eNp5jXXW/DiZ1M54Tu+pfkqTcAV2MZ12x/mPP5eSldDeeE7NyuyTpm9/0NJ51+YtbFHzvT8ZzAjc+oqOlK43nNKydIlubjOdY6itJst//s/ms6/8ila03nqOwAdKp1eZz6t4sO2+R8RgrZpLxDJw/NkEDAOARbIJ2jj1AAADAd2iAAADwCBog52iAAACA79AAAQDgETRAztEAAQAA36EBAgDAIwIWvYZTvFMAAMB3aIAAAPAI9gA5RwMEAAB8hwYIAACPoAFyLmQDdPjwYc2cOVN33HGHXnzxxQo/mzp1qtGFAQAAmBJyAEpNTVWjRo106623asOGDZoyZYrOnDkjSdq/f78rCwQAAM4ELMu1W00XcgDau3ev7rvvPg0cOFBLlixR06ZN9bvf/U6nTp1ya30AAADVLuQAVFpaWv61ZVlKTU1Vu3btNHHiRIYgAABQY4UcgFq2bKn333+/wn3Tp0/Xtddeq71795pcFwAAqKKAi/9X04U8C2z+/PmyfuI43z333KOhQ4caWxQAAIBJIQegiIiIs/7sP/7jP6p9MQAA4Px5YXOyW2p+hwUAAFBFXAgRAACPoAFyjgYIAAD4Dg0QAAAeEbDoNZzinQIAAL5DAwQAgEewB8g5GiAAAOA7lm3b9sVeBAAAuHBvfTXTtaw+zR9yLcsEGiAAAOA7ru4BKhzZw3hG1CtbJUl5Kd2NZ8Ws3O7a72Tvf8x4jtXyLuUPv8F4TrNXt0mSvhpyvfGs5pnv68CvrjOe02LNB67luPnf6Jvf9DSedfmLWxTcNt14TuCGDH1busp4zs9qJ0ln/mk8R7UGSZKC7/3JeFTgxkekk5nGc3TZEOnYCvM5DW6RXfi08RgraqLxjB9jD5BzNEAAAMB3OAsMAACP4DpAzvFOAQAAo/bs2aNRo0Zp0KBBGjVqlPbu3VvpMWVlZUpLS1P//v01YMAALV++vMLP16xZoyFDhigxMVFDhgzRN9984+h5Z0MDBAAAjEpNTdXo0aOVlJSkVatWafbs2Xr++ecrPCYzM1O5ublat26diouLlZycrB49eqhFixb67LPPtHDhQi1btkxNmzbV0aNHVadOnXM+LxQaIAAAPCIgy7VbSUmJDhw4UOlWUlJSYU2HDh1STk6OEhMTJUmJiYnKyclRUVFRhcetWbNGI0aMUCAQUGRkpPr376+1a9dKkpYuXao77rhDTZs2lSQ1bNhQdevWPefzQqEBAgAAVbZs2TItXLiw0v1TpkzR1KlTy7/Py8tTdHS0wsLCJElhYWGKiopSXl6eIiMjKzwuNja2/PuYmBjl5+dLknbv3q0WLVroN7/5jY4fP64BAwbo97//vSzLCvm8UBiAAADwCDdPgx83bpxSUlIq3R8eHl7tWWVlZdq1a5eee+45nT59WuPHj1dsbKySk5PP+zU5BAYAAKosPDxcLVq0qHT78QAUExOjgoIClZWVSfpumCksLFRMTEylxx08eLD8+7y8PDVr1kySFBsbq8GDB6tOnTpq0KCBEhIS9Omnn57zeaEwAAEA4BEBK+DazakmTZooLi5OWVlZkqSsrCzFxcVVOPwlSYMHD9by5csVDAZVVFSkDRs2aNCg7y74mZiYqM2bN8u2bZWWlmrbtm3q0KHDOZ8XiqNDYIcPHy4/ntasWTM1btzY8S8OAAD8bc6cOZoxY4YWLVqk8PBwZWRkSJImTJigadOmqWPHjkpKSlJ2drYGDhwoSZo8ebJatmwpSbr55pv173//W7/61a8UCATUq1cvDR8+XJJCPi+UkANQbm6uZs2apZycHEVFRUmSCgsLdfXVVystLU2tW7c+v3cCAABUu0v1ozDatm37k9fnWbx4cfnXYWFhSktL+8nnBwIB3X///br//vsr/SzU80IJOQDdd999Gj16tJ577jkFAt/VXcFgUJmZmZo+fbpefvnlKgcCAABcbCEP4hUXF2vo0KHlw4/03RSWlJSkI0eOGF8cAABwLmBZrt1qupADUEREhLKysmTbdvl9tm3rjTfeMHKaGwAAgBtCHgKbN2+eUlNTNXfuXEVHR0uSCgoK1KFDB82bN8+VBQIAAGf4MFTnQg5ArVu31rJly1RUVKS8vDxJ351v/+NT1wAAAGoSR6fBR0ZGMvQAAHCJ88LeHLfQlQEAAN/hs8AAAPCIgGiAnKIBAgAAvsMABAAAfIdDYAAAeASboJ2jAQIAAL5DAwQAgEdwIUTnLPuHn3MBAABqrJ2HH3Etq0PjP7mWZQINEAAAHsEeIOdcHYAOJnUznhG76l+SpLyU7sazYlZu1ze/6Wk85/IXt8g+uNB4jhU7RV/feqPxnKb/eE+StH9wV+NZLdd+5NrfwoFfXWc8p8WaD1Q4sofxnKhXtkqSDt8Rbzyr8ZK3ZX82x3iO1XGOjpauNJ7TsHaKdGKV8RzVS5IkBd+cZjwq0O9xqeRl4zkKHyUd+bv5nEa/lp3/pPEYq9mdxjNw/miAAADwCIs9QI7xTgEAAN+hAQIAwCMC9BqO8U4BAADfoQECAMAj2APkHO8UAADwHRogAAA8gitBO8c7BQAAfIcGCAAAj7DoNRxzNAAdPnxY+fn5kqRmzZqpcePGRhcFAABgUsgBKDc3V7NmzVJOTo6ioqIkSYWFhbr66quVlpam1q1bu7FGAACAahVyALrvvvs0evRoPffccwoEvqvVgsGgMjMzNX36dL38sgufDQMAABxhE7RzId+p4uJiDR06tHz4kaRAIKCkpCQdOXLE+OIAAABMCDkARUREKCsrS7Ztl99n27beeOMNhYeHG18cAABwzlLAtVtNF/IQ2Lx585Samqq5c+cqOjpaklRQUKAOHTpo3rx5riwQAACguoUcgFq3bq1ly5apqKhIeXl5kqSYmBhFRka6sjgAAOAce4Ccc3QafGRkJEMPAADwDC6ECACAR/BhqM7xTgEAAN+hAQIAwCMC9BqO8U4BAADfoQECAMAj2APkHO8UAADwHRogAAA8gusAOcc7BQAAfMeyf/hBXwAAoMY6dPLvrmU1uezXrmWZ4OohsLyU7sYzYlZulyTlD7/BeFazV7fp0NhexnOavLBZ9q6HjOdY7WfqqyHXG89pnvm+JOnL3p2MZ7V551PtH9zVeE7LtR9pX0Jn4zmtNn7i6r+jw3fEG89qvORt2TvSjedYcQ/owLGnjee0aDBRCm40nqNAgiTJ3j7DeJTVfZ50YpXxHNVLkk5mms+5bIjsgwuNx1ixU4xn4PxxCAwAAPgOm6ABAPAINkE7xzsFAAB8hwYIAACPsOg1HOOdAgAAvkMDBACAR7AHyDneKQAA4Ds0QAAAeAQfhuoc7xQAAPCdKjdAR44cUaNGjUysBQAAXIAAvYZjId+pnTt3atiwYRo+fLh2796tiRMnqnfv3oqPj9eOHTvcWiMAAEC1CjkApaena/LkyRozZozGjx+vxMREZWdnKzU1VRkZGW6tEQAAOGBZAdduNV3I3+Dbb79VQkKCkpOTJUlDhw6VJPXr10/FxcXmVwcAAGBAyD1Atm2Xf92zZ88KPwsGg2ZWBAAAzgvXAXIu5DvVvHlzHTt2TNJ3h8O+l5+fr3r16pldGQAAgCEhG6AnnnjiJ+8PDw/XokWLjCwIAACcHz4LzLnzuhBi/fr1Vb9+/epeCwAAgCsYFQEAgO/wURgAAHgEm6Cd450CAAC+QwMEAIBHsAnaOd4pAADgOzRAAAB4BHuAnOOdAgAAvkMDBACAR3jhQ0rdYtk//MAvAABQY9na5FqWpb6uZZngagOUP/wG4xnNXt3matbhO+KN5zRe8rbsj2cZz7G6PKh9CZ2N57Ta+IkkaXevjsaz2m7+zLXf6cvenYzntHnnU+WldDeeE7NyuySpeHwf41kRz7wl+4uHjedYV92v/yn+q/GcdhH36kxwvfGcWoEBkiT7wweMZ1m/SJdK/9t4jmr/0rUcO/dR4zHWFfcYz6iU6WalYbmYZQBdGQAA8B32AAEA4BV20L0sGiAAAICahQYIAACvcLMBquFogAAAgO/QAAEA4BU0QI7RAAEAAN+hAQIAwCtogByjAQIAAL7DAAQAAHznvA6BbdiwQTExMbrmmmuqez0AAOB8BTkE5tR5DUDr16/X559/rujoaD377LPVvSYAAACjzmsAysjIkCQVFxdX62IAAMAFYBO0Yxe0BygiIqK61gEAAOAaToMHAMAraIAc4ywwAADgOzRAAAB4BQ2QYzRAAADAd2iAAADwCq4D5BgNEAAA8B0aIAAAvII9QI7RAAEAAN+hAQIAwCtogByjAQIAAL5DAwQAgFfQADlm2bZtX+xFAACAanDk7+5lNfq1e1kGuNoA5Q+/wXhGs1e3uZr1zW96Gs+5/MUtCr73J+M5gRsf0Ze9OxnPafPOp5Kk/+3xc+NZ/7H1367l7O7V0XhO282f6WBSN+M5sav+JUkquu0m41mRS9+V/dkc4zlWxznaefgR4zkdGv9Jh0+9bDyncd1RkiR710PGs6z2M2Vrk/kc9XUvJ/dR8zlX3GM8A+ePQ2AAAHiEbZe5lmW5lmQGm6ABAIDv0AABAOAVfBSGYzRAAADAd2iAAADwCk6Dd4wGCAAA+A4NEAAAXkED5BgNEAAA8B0GIAAAvMIOunergj179mjUqFEaNGiQRo0apb1791Z6TFlZmdLS0tS/f38NGDBAy5cvr/SYL7/8Utdee60yMjLK75sxY4Z69+6tpKQkJSUl6W9/+5ujNXEIDAAAGJWamqrRo0crKSlJq1at0uzZs/X8889XeExmZqZyc3O1bt06FRcXKzk5WT169FCLFi0kfTcgpaamqn///pVef+LEiRozZkyV1kQDBACAV1yCDdChQ4eUk5OjxMRESVJiYqJycnJUVFRU4XFr1qzRiBEjFAgEFBkZqf79+2vt2rXlP3/66afVp08ftW7dulreKgYgAABQZSUlJTpw4EClW0lJSYXH5eXlKTo6WmFhYZKksLAwRUVFKS8vr9LjYmNjy7+PiYlRfn6+JGnnzp3avHmzbrvttp9cy3PPPachQ4Zo0qRJ2r17t6P1cwgMAACvcPFK0MuWLdPChQsr3T9lyhRNnTq12nJKS0s1a9YsPfzww+VD1A/dfffdatq0qQKBgF5//XWNHz9eGzZs+MnH/hADEAAAqLJx48YpJSWl0v3h4eEVvo+JiVFBQYHKysoUFhamsrIyFRYWKiYmptLjDh48qE6dOkn6v0bo66+/Vm5uriZOnCjpu+bJtm0dO3ZMDz74oKKjo8tfIzk5WQ8//LDy8/PVvHnzkOtnAAIAwCtcvA5QeHh4pWHnpzRp0kRxcXHKyspSUlKSsrKyFBcXp8jIyAqPGzx4sJYvX66BAwequLhYGzZs0IsvvqjY2Fht3769/HELFizQ8ePHNX36dElSQUFB+RD07rvvKhAIVBiKzoYBCAAAGDVnzhzNmDFDixYtUnh4ePnXiDZtAAAbK0lEQVRp7BMmTNC0adPUsWNHJSUlKTs7WwMHDpQkTZ48WS1btjzna0+fPl2HDh2SZVlq0KCB/va3v6lWrXOPNwxAAADAqLZt2/7kdX0WL15c/nVYWJjS0tLO+Vo/3l+0dOnS81oTAxAAAF7BR2E4xmnwAADAd2iAAADwChogx2iAAACA79AAAQDgFS5eCLGmowECAAC+QwMEAIBXsAfIMRogAADgO5Zt2/bFXgQAALhw9r6/upZltbrXtSwTaIAAAIDvuLoHqHBkD+MZUa9slSTlD7/BeFazV7fp0NhexnOavLBZdnaq8Rzr2jTlDuhiPOeK9R9Lkr7s3cl4Vpt3PtW+hM7Gc1pt/ER7+l5rPOfKTdmu/W1L0uE74o1nNV7ytuzP5xrPsa6Zrb1HFxrPad1wik6V/bfxnLphv5Qk2R8+YDzL+kW6FNxoPEeBBOnUavM5dW+Wvf8x4zFWy7uMZ1TCWWCO0QABAADf4SwwAAC8Isi2XqdogAAAgO/QAAEA4BXsAXKMBggAAPgOAxAAAPAdDoEBAOAVHAJzjAYIAAD4zjkboGPHjqlBgwbnvA8AAFxknAbv2DkboLFjxzq6DwAAoKY4awN05swZlZaWKhgM6uTJk/r+M1OPHj2qEydOuLZAAADgEHuAHDvrAPTkk09q4cKFsixLnTv/32cpNWjQQLfffrsriwMAADDhrAPQlClTNGXKFM2dO1ezZ892c00AAOB80AA5ds49QAw/AADAa7gOEAAAXsFZYI5xHSAAAOA7NEAAAHgFe4AcowECAAC+QwMEAIBXsAfIMRogAADgOzRAAAB4BXuAHKMBAgAAvsMABAAAfIdDYAAAeAWHwByz7O8/5h0AANRo9vYZrmVZ3ee5lmUCDRAAAB7hZqdhuZZkhqsDUOHIHsYzol7Z6mrWobG9jOc0eWGz7OxU4znWtWnal9DZeE6rjZ9Ikr7s3cl4Vpt3PnXtd9rT91rjOVduylb+8BuM5zR7dZsk6fAd8cazGi95W/aOdOM5VtwD2nt0ofGc1g2n6ExwvfGcWoEBkiT7wweMZ1m/SJfKzP9OChsglf63+Zzav5S9/zHjMVbLu4xn4PzRAAEA4BXsAXKMs8AAAIDv0AABAOAVNECO0QABAADfoQECAMAr+DBUx2iAAACA79AAAQDgFewBcowGCAAA+A4NEAAAXkED5BgNEAAA8J0qNUCnT59WWVlZ+ff16tWr9gUBAIDzxFlgjjkagNavX68HH3xQX3/9taTvPmzNsizt2LHD6OIAAABMcDQAzZ8/X4899pg6d+6sQICjZgAAoGZzNAA1atRIXbt2Nb0WAABwIdgE7VjIOufEiRM6ceKEBgwYoJdeeknFxcXl9504ccKtNQIAAFSrkA1Qly5dZFmWbPu7TVVz584t/549QAAAXGJogBwLOQDt3LnTrXUAAAC4hgshAgDgFZwG7xindAEAAN+hAQIAwCvYA+QYDRAAAPAdGiAAALyCBsgxGiAAAOA7NEAAAHgFZ4E5RgMEAAB8x7K/v8wzAACo0YKv3e5aVmDYc65lmeDqIbDCkT2MZ0S9stXVrENjexnPafLCZtkfPmA8x/pFur7s3cl4Tpt3PpUk7e7V0XhW282fufY7uZWTl9LdeE7Myu2SpMN3xBvParzkbdk70o3nWHEP6H+K/2o8p13Evfq2dJXxnJ/VTpIk2dmpxrOsa9Mke5PxHFl9pTP/NJ9Ta5Ds3EeNx1hX3GM8A+ePPUAAAHiEXcZBHafYAwQAAHyHAQgAAPgOh8AAAPAKToN3jAYIAAD4Dg0QAABewSZox2iAAACA79AAAQDgETZ7gByjAQIAAL5DAwQAgFewB8gxRwPQDTfcIMuyKt2/devWal8QAACAaY4GoBUrVpR/ferUKWVmZqpWLcojAAAuKWXBi72CGsPRHqDmzZuX39q0aaM//OEPevvtt02vDQAAwIjzqnH279+vQ4cOVfdaAADABeAsMOeqvAcoGAzqzJkzmjlzptGFAQAAmFLlPUC1atXS5ZdfrrCwMGOLAgAA54GzwBxzNAA1b97c9DoAAABcw6lcAAB4BXuAHONK0AAAwHcYgAAAgO9wCAwAAI+w2QTtGA0QAADwHRogAAC8IshHYThFAwQAAHyHBggAAK9gD5Bjlm3bvFsAAHjA6f+6xbWsOnevOPeDLmGuNkD5w28wntHs1W2SpMKRPYxnRb2yVd/8pqfxnMtf3CL787nGc6xrZit3QBfjOVes/1iS9GXvTsaz2rzzqfYldDae02rjJ9rT91rjOVduynb139HhO+KNZzVe8rbsz+YYz7E6ztH+Y08az2nZ4E6VBv9pPKd2YJAkyX7/z8azrOv/IpWtN56jsAHSqdXmc+reLHv/Y8ZjrJZ3Gc/4MT4M1Tn2AAEAAN9hDxAAAF7BHiDHaIAAAIDv0AABAOAVNECO0QABAADfoQECAMAjOAvMORogAADgOzRAAAB4RRmfBeYUDRAAAPAdBiAAAOA75xyAysrK9Pjjj7uxFgAAcAHsoO3araY75wAUFhamd955x421AAAAuMLRIbA+ffro2Wef1aFDh3TixInyGwAAuISU2e7dajhHZ4EtXLhQkvTII4/IsizZti3LsrRjxw6jiwMAADDB0QC0c+dO0+sAAAAX6hLdm7Nnzx7NmDFDxcXFioiIUEZGhlq3bl3hMWVlZUpPT9e7774ry7I0ceJEjRgxQpK0YsUKLV26VIFAQMFgUCNGjNB//ud/nvN5oXAdIAAAYFRqaqpGjx6tpKQkrVq1SrNnz9bzzz9f4TGZmZnKzc3VunXrVFxcrOTkZPXo0UMtWrTQoEGDNGzYMFmWpWPHjmnIkCHq1q2bOnToEPJ5oXAaPAAAHmGX2a7dSkpKdODAgUq3kpKSCms6dOiQcnJylJiYKElKTExUTk6OioqKKjxuzZo1GjFihAKBgCIjI9W/f3+tXbtWktSgQQNZliVJOnnypEpLS8u/D/W8UGiAAABAlS1btqx8j/APTZkyRVOnTi3/Pi8vT9HR0QoLC5P03dnlUVFRysvLU2RkZIXHxcbGln8fExOj/Pz88u83btyoRx99VLm5ufrjH/+o9u3bO3re2TAAAQDgFS7uARo3bpxSUlIq3R8eHm4kLyEhQQkJCTp48KAmT56s3r17q02bNuf9egxAAACgysLDwx0NOzExMSooKFBZWZnCwsJUVlamwsJCxcTEVHrcwYMH1alTJ0mVm53vxcbGqmPHjnrrrbfUpk0bx8/7MfYAAQDgFWVB924ONWnSRHFxccrKypIkZWVlKS4ursLhL0kaPHiwli9frmAwqKKiIm3YsEGDBg2SJO3evbv8cUVFRdq+fbvatWt3zueFQgMEAACMmjNnjmbMmKFFixYpPDxcGRkZkqQJEyZo2rRp6tixo5KSkpSdna2BAwdKkiZPnqyWLVtKkl5++WVt2bJFtWrVkm3bGjNmjHr16iVJIZ8XCgMQAAAecal+Rlfbtm21fPnySvcvXry4/OuwsDClpaX95PP//Oc/n/W1Qz0vFA6BAQAA36EBAgDAKzzwGV1usWzb5t0CAMADvp3W37Wsnz2+wbUsE1xtgA4mdTOeEbvqX5Kk/OE3GM9q9uo2HRrby3hOkxc2y/7iYeM51lX366sh1xvPaZ75viTpy96djGe1eedT7R/c1XhOy7UfaU/fa43nXLkpW3kp3Y3nxKzcLkkqHt/HeFbEM2/J3pFuPMeKe0AHjj1tPKdFg4mSvcl4jqy+kqTge38yHhW48RHpZKbxHF02RDq12nxO3Ztlf/W48Rir+TTjGTh/HAIDAMAjLtVN0JciNkEDAADfoQECAMAjbDZBO0YDBAAAfIcGCAAAj2APkHM0QAAAwHdogAAA8Igge4AcowECAAC+QwMEAIBHsAfIuSo3QKdPn9bXX39tYi0AAACucDQA3X333Tp69KhOnjypIUOG6Oabb9azzz5rem0AAKAK7GDQtVtN52gA2rNnjxo2bKi33npL3bt319tvv63XX3/d9NoAAACMcLQH6MyZM5Kk999/X/Hx8apXr54CAfZPAwBwKeFK0M45mmLatm2r8ePHa9OmTerRo4dOnjxpel0AAADGOGqAMjIytHnzZrVv317169dXQUGB/vjHP5peGwAAqALOAnPO0QB02WWXqX///uXfR0dHKzo62tiiAAAATOI6QAAAeAR7gJxjJzMAAPAdBiAAAOA7HAIDAMAj2ATtHA0QAADwHRogAAA8IkgD5BgNEAAA8B0aIAAAPILT4J2jAQIAAL5j2bbNuAgAgAccTOrmWlbsqn+5lmWCq4fA3PgP8/1/kPzhNxjPavbqNh0a28t4TpMXNsv+4mHjOdZV9+vAr64zntNizQeSpC97dzKe1eadT7V/cFfjOS3XfqQ9fa81nnPlpmzX/rYlqeR3fY1nhT+1Sfauh4znWO1nKu/4EuM5MfXvkIIbjecokCBJCm6bbj7qhgzpxCrjOaqXJJ1abT6n7s2yv3rceIzVfJrxDJw/9gABAOARXAfIOfYAAQAA36EBAgDAIzgLzDkaIAAA4Ds0QAAAeIQdDF7sJdQYNEAAAMB3aIAAAPAI9gA5RwMEAAB8hwEIAAD4DofAAADwCC6E6FzIAejEiRMhn1yvXr1qXQwAAIAbQg5AXbp0kWVZZ/35jh07qn1BAADg/ARpgBwLOQDt3LlTkrRo0SLVqVNHo0aNkm3bWr58uUpLS11ZIAAAQHVztAl6/fr1Gj9+vBo2bKjw8HD99re/1bp160yvDQAAVIFdZrt2q+kcDUAnT57Uvn37yr/Pzc095/4gAACAS5Wjs8DuvvtujRw5Uj//+c8lSTk5OXrwwQeNLgwAAFQNZ4E552gAGjhwoH7xi18oOztbktS5c2dFRkYaXRgAAIApjq8D1KRJE/Xr18/kWgAAwAXwwt4ct3AlaAAA4DtcCRoAAI9gD5BzNEAAAMB3aIAAAPAIGiDnaIAAAIDv0AABAOARnAXmHA0QAADwHcu2bcZFAAA8YFfXONey2n+0w7UsEzgEBgCARwTZBO2YqwNQXkp34xkxK7dLkgpH9jCeFfXKVh0a28t4TpMXNsvekW48x4p7QPsHdzWe03LtR5KkL3t3Mp7V5p1PlTugi/GcK9Z/7Nrvkz/8BuM5zV7dJkkqHt/HeFbEM2/J3vWQ8Ryr/UwdOPa08ZwWDSZK9ibjObL6SpLs7TPMR3WfJ53MNJ6jy4ZIpf9tPqf2L2V/9bjxGKv5NOMZOH80QAAAeEQweLFXUHOwCRoAAPgODRAAAB5BA+QcDRAAAPAdGiAAADyCBsg5GiAAAOA7NEAAAHgElwFyjgYIAAD4Dg0QAAAewR4g52iAAACA7zhqgI4eParFixdrx44dOnXqVPn9zz//vLGFAQCAqqEBcs5RA/TnP/9ZgUBAe/fu1ciRIxUWFqZOncx/7hEAAIAJjgagffv26a677tJll12mxMREPfXUU/rggw9Mrw0AAFRBMOjeraZzNADVqVNHklS7dm0VFxerdu3aKioqMrowAAAAUxztAWrdurWKi4s1ZMgQjRo1Sg0bNtQ111xjem0AAABGOBqA/vrXv0qSbr/9dnXs2FFHjx7VTTfdZHRhAACgarxwaMotVb4O0HXXXWdiHQAAAK7hQogAAHgEDZBzXAgRAAD4Dg0QAAAeQQPkHA0QAADwHRogAAA8ggbIORogAADgOzRAAAB4BA2Qc5Zt2/bFXgQAALhwm2I6uJbVN2+na1km0AABAOARdBrOuToA5aV0N54Rs3K7JKlwZA/jWVGvbNWhsb2M5zR5YbPsHenGc6y4B7R/cFfjOS3XfiRJ+rJ3J+NZbd75VPsSOhvPabXxE+3pe63xnCs3ZSt/+A3Gc5q9uk2SVDy+j/GsiGfecu/v+9iTxnNaNrhTQXuj8ZyAlSBJst//s/Es6/q/SKdWG89R3Ztdy7G/etx4jNV8mvEMnD8aIAAAPII9QM5xFhgAAPAdGiAAADyCBsg5GiAAAOA7DEAAAMB3OAQGAIBHcAjMORogAADgOzRAAAB4BA2QczRAAADAdxw1QKdOnVLdunVNrwUAAFwAGiDnHDVA/fr107x585Sbm2t6PQAAAMY5GoDeeOMNhYeHa9y4cRo/frw2bdpkel0AAKCKgkH3bjWdowGoSZMmmjRpkjZs2KCRI0cqLS1N/fr105IlS3Tq1CnTawQAAKhWjjdBnzhxQsuXL9fChQt1xRVX6O6779aXX36pCRMmmFwfAABwiAbIOUeboOfOnat169apX79++utf/6p27dpJkoYMGaLBgwcbXSAAAEB1czQANW/eXKtXr1ajRo0q/ez555+v9kUBAICqC9oXewU1h6MB6Le//e1ZfxYVFVVtiwEAAHADV4IGAMAjvLA3xy1cCRoAAPgOAxAAAB5xqZ4FtmfPHo0aNUqDBg3SqFGjtHfv3kqPKSsrU1pamvr3768BAwZo+fLl5T/bvHmzhg0bpp///OfKyMio8LwFCxaoR48eSkpKUlJSktLS0hytiUNgAADAqNTUVI0ePVpJSUlatWqVZs+eXekkqszMTOXm5mrdunUqLi5WcnKyevTooRYtWqhly5Z66KGHtHbtWp0+fbrS6ycnJ2v69OlVWhMNEAAAMObQoUPKyclRYmKiJCkxMVE5OTkqKiqq8Lg1a9ZoxIgRCgQCioyMVP/+/bV27VpJUqtWrRQXF6dataqvt6EBAgDAI9zcBF1SUqKSkpJK94eHhys8PLz8+7y8PEVHRyssLEySFBYWpqioKOXl5SkyMrLC42JjY8u/j4mJUX5+vqO1rF69Wps3b1bTpk01depUdenS5ZzPYQACAABVtmzZMi1cuLDS/VOmTNHUqVNdW8ett96qO++8U7Vr19aWLVs0adIkrVmzRo0bNw75PAYgAAA8ws0GaNy4cUpJSal0/w/bH+m7JqegoEBlZWUKCwtTWVmZCgsLFRMTU+lxBw8eVKdOnSRVboTOpmnTpuVf9+zZUzExMfriiy/UrVu3kM+zbNvmupEAAMCYsWPHavjw4eWboF999VW98MILFR7z2muvafXq1Vq8eHH5JugXX3xRLVu2LH/MggULdPz48QobngsKChQdHS1J2rFjh2677TZlZWVVGIx+CgMQAAAwavfu3ZoxY4ZKSkoUHh6ujIwMtWnTRhMmTNC0adPUsWNHlZWVae7cudqyZYskacKECRo1apQk6YMPPtA999yjY8eOybZtNWzYUA899JBuuukmTZ8+XZ9//rkCgYBq166tadOmKT4+/pxrYgACAAC+w2nwAADAdxiAAACA7zAAAQAA32EAAgAAvsMABAAAfIcBCAAA+A4DEAAA8J1L8qMwDh8+rPvuu0+5ubmqU6eOWrVqpblz51b40LTqsmfPHs2YMUPFxcWKiIhQRkaGWrduXe05bmZNmjRJBw4cUCAQUP369TVr1izFxcVVe45p/fr10+nTp/X222+Xf4jea6+9pvvvv1+zZs3SmDFjLvIK8b327dvro48+0s9+9rOLvZRqkZGRoX/+85/66quvlJmZqXbt2hnL8sq/V0kaMWKETp8+rdLSUu3du1dXXXWVJOnqq6/Www8/XK1ZXvubw0VgX4IOHz5sb9u2rfz7efPm2ffff7+RrLFjx9qvv/66bdu2/frrr9tjx441kuNmVklJSfnX69evt5OTk43kmNa3b187JSXFfuutt8rvGzNmjJ2SkmK/8MILF3Fl1au0tPRiL+GCtWvXzj527NjFXka1ef/99+2DBw/affv2tXft2mU0yyv/Xn9o//79drdu3YxmeO1vDu67JA+BRUREqHv37uXfd+7cWQcPHqz2nEOHDiknJ0eJiYmSpMTEROXk5KioqKhGZzVs2LD862PHjsmyrGrPcEtKSopee+01SdL+/ft1/Phxo//f+B//+EcNGzZMQ4YM0eTJk3XkyBEjOe3bt9eCBQt0yy23/OSnKVeXjz/+WL/+9a81dOhQDR06VJs3bzaW9cILL+iWW25RQkKC/vnPfxrLccN1111X6YMaTfHSv1e3ufE3d+DAgQr/e/Tj71FzXZKHwH4oGAzq73//u/r161ftr52Xl6fo6OjywythYWGKiopSXl5etR9uczNLkmbOnKktW7bItm0988wz1f76bunWrZteeuklHTlyRCtXrlRycrI+//xzY3kzZ84s/+/xX//1X1q8eLHuvfdeI1l169bVihUrjLy2JBUXF2vKlClasGCBunbtqrKyMh07dsxYXoMGDbRixQp9+OGHuuuuuzRo0CBjWV7jlX+vbuNvDhfikh+AHnzwQdWvX5/9HlX00EMPSZJef/11zZ8/X4sXL77IKzo/lmXpl7/8pVavXq3Vq1frH//4h9EBaNWqVcrMzFRpaamOHz9ubD+Y9F27ZdInn3yitm3bqmvXrpK+G7obNWpkLO9Xv/qVpO8a28LCQp06dUp169Y1luclXvn36jb+5nAhLslDYN/LyMjQvn379NhjjykQqP6lxsTEqKCgQGVlZZKksrIyFRYWGqm+3cz6oeTkZG3fvl2HDx+u9tdesWKFkpKSlJSUpDfeeKPaX/97KSkpevzxx9WuXTs1btzYWM4HH3ygv//973rmmWeUmZmpu+66S6dPnzaWV79+fWOvfTF8/z8837ecZ86cMZLj1t/dxWDy36sXufE3V6tWLdk/+MzwU6dOVXsGLo5LtgF69NFH9e9//1tPP/206tSpYySjSZMmiouLU1ZWlpKSkpSVlaW4uDgjh6Tcyvr2229VUlJSPli9+eabatSokSIiIqo1R5JuueUW3XLLLdX+uj/WsmVL3X333erUqZPRnJKSEjVo0EARERE6ffq00cNTbujcubN2796tjz/+WF26dCk/BGayBXKDW393bnDz3yvOz+WXX67S0lLt27dPrVq1UlZW1sVeEqrJJTkAffHFF3rqqafUunVr3XrrrZKkFi1a6Iknnqj2rDlz5mjGjBlatGiRwsPDlZGRUe0ZbmadOHFCf/jDH3TixAkFAgE1atRITz75ZI3fWDlq1CjjGTfddJPeeOMNDRo0SI0bN9Z1112nzz77zHiuKREREVqwYIHmzZun48ePKxAIaPr06brxxhsv9tIueenp6Vq3bp2++eYb3X777YqIiNDq1aurPcer/169pFatWpo5c6Zuv/12RUZGqk+fPhd7Sagmlv3Dbg8AAMAHLuk9QAAAACYwAAEAAN9hAAIAAL7DAAQAAHyHAQgAAPgOAxAAAPAdBiAAAOA7DEAAAMB3/j8Ky5nw8X/lawAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example = [gen_date() for _ in range(1)]\n",
    "plot_attention(model, example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
